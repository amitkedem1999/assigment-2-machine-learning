{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1786c0c9",
      "metadata": {},
      "source": [
        "# This is formatted as code\n",
        "\n",
        "Student 1: name: Amit Shlomo Kedem, i.d.: 315216663, github: https://github.com/amitkedem1999/assigment-2-machine-learning\n",
        "\n",
        "\n",
        "Student 2: name: Ido Abodi Amarteli, i.d.: 209306323, github: https://github.com/Idoamarteli20/Machine-learning-task-2\n",
        "\n",
        "\n",
        "Student 3: name: Daniel Cohen, i.d.: 318974391, github: https://github.com/danielcohen0121/assignment-2-machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q3S5loqBCqgM",
      "metadata": {
        "id": "Q3S5loqBCqgM"
      },
      "source": [
        "Libraries (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5f1ba5ec",
      "metadata": {
        "id": "5f1ba5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: pip\n"
          ]
        }
      ],
      "source": [
        "!pip -q install torchinfo\n",
        "!pip -q install mlflow\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Callable, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchinfo import summary\n",
        "import mlflow\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ykrfYnc6g1Bs",
      "metadata": {
        "id": "ykrfYnc6g1Bs"
      },
      "source": [
        "Device (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "XJIYaWIggSCp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJIYaWIggSCp",
        "outputId": "c1ca0bcd-5037-4d74-8334-7bd89e22ba29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Mixed precision (AMP): False\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Mixed precision (AMP): {use_amp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71ffe61",
      "metadata": {
        "id": "d71ffe61"
      },
      "source": [
        "Data (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2cb0bf0c",
      "metadata": {
        "id": "2cb0bf0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataManager:\n",
        "    def __init__(self, dataset_class, root: str = \"./data\", val_fraction: float = 0.1,\n",
        "                 batch_size: int = 32, seed: int = 42):\n",
        "        self.dataset_class = dataset_class\n",
        "        self.root = root\n",
        "        self.val_fraction = val_fraction\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1918,), (0.3483,))\n",
        "        ])\n",
        "\n",
        "    def get_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "        full_train = self.dataset_class(root=self.root, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "        test_ds = self.dataset_class(root=self.root, train=False,\n",
        "                                     download=True, transform=self.transform)\n",
        "\n",
        "        val_size = int(len(full_train) * self.val_fraction)\n",
        "        train_size = len(full_train) - val_size\n",
        "\n",
        "        generator = torch.Generator().manual_seed(self.seed)\n",
        "        train_ds, val_ds = random_split(full_train, [train_size, val_size], generator=generator)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=self.batch_size,\n",
        "                                  shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader   = DataLoader(val_ds,   batch_size=self.batch_size,\n",
        "                                  shuffle=False, num_workers=2, pin_memory=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=self.batch_size,\n",
        "                                  shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
        "        return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O0SlvxE36HG0",
      "metadata": {
        "id": "O0SlvxE36HG0"
      },
      "source": [
        "Configurations (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "FwDGf9J66bXL",
      "metadata": {
        "id": "FwDGf9J66bXL"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class LayerSpec:\n",
        "    out_dim: int\n",
        "    activation: Callable[[torch.Tensor], torch.Tensor] = F.relu\n",
        "    dropout: float = 0.0\n",
        "    batch_norm: bool = True\n",
        "    weight_decay: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    input_dim: Tuple[int, int, int] = (1, 28, 28)\n",
        "    num_classes: int = 10\n",
        "    layers: List[LayerSpec] = None\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 100\n",
        "    lr: float = 1e-4\n",
        "    patience: int = 15\n",
        "    min_delta: float = 1e-4\n",
        "    val_fraction: float = 0.1\n",
        "    seed: int = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53221445",
      "metadata": {
        "id": "53221445"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b5843387",
      "metadata": {
        "id": "b5843387"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPFromConfig(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        flat_dim = config.input_dim[0] * config.input_dim[1] * config.input_dim[2]\n",
        "        self.layers_specs = config.layers\n",
        "        layers = []\n",
        "        prev_dim = flat_dim\n",
        "\n",
        "        for i, spec in enumerate(config.layers):\n",
        "            linear = nn.Linear(prev_dim, spec.out_dim)\n",
        "\n",
        "            layers.append(linear)\n",
        "            if spec.batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(spec.out_dim))\n",
        "            if spec.dropout > 0:\n",
        "                layers.append(nn.Dropout(spec.dropout))\n",
        "            layers.append(spec.activation())\n",
        "            prev_dim = spec.out_dim\n",
        "\n",
        "        # Final classifier layer\n",
        "        self.final_linear = nn.Linear(prev_dim, config.num_classes)\n",
        "        layers.append(self.final_linear)\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "    def get_layer_params(self):\n",
        "        param_groups = []\n",
        "        for i, spec in enumerate(self.layers_specs):\n",
        "            linear_layer = self.net[i * (4 if spec.batch_norm or spec.dropout > 0 else 3)]\n",
        "            pass\n",
        "        return self.layers_specs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eND74vML5XYh",
      "metadata": {
        "id": "eND74vML5XYh"
      },
      "source": [
        "Early Stopping (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KgImYRwI5hAr",
      "metadata": {
        "id": "KgImYRwI5hAr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 10, min_delta: float = 1e-4):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        return self.should_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b11a04f",
      "metadata": {
        "id": "9b11a04f"
      },
      "source": [
        "Trainer (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e474713f",
      "metadata": {
        "id": "e474713f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module, config: TrainConfig):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.config = config\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = self._build_optimizer()\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "        self.early_stopping = EarlyStopping(patience=config.patience,\n",
        "                                            min_delta=config.min_delta)\n",
        "\n",
        "        self.history = {\"train_loss\": [], \"train_acc\": [],\n",
        "                        \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    def _build_optimizer(self):\n",
        "\n",
        "        # Collect all Linear layers in the order they appear\n",
        "        linear_layers = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                linear_layers.append((name, module))\n",
        "\n",
        "        param_groups = []\n",
        "\n",
        "        for i, spec in enumerate(self.model.layers_specs):\n",
        "            name, layer = linear_layers[i]\n",
        "            param_groups.append({\n",
        "                'params': layer.parameters(),\n",
        "                'weight_decay': spec.weight_decay\n",
        "            })\n",
        "\n",
        "        final_name, final_layer = linear_layers[-1]\n",
        "        param_groups.append({\n",
        "            'params': final_layer.parameters(),\n",
        "            'weight_decay': 0.0\n",
        "        })\n",
        "\n",
        "        return torch.optim.SGD(param_groups, momentum=0.9, nesterov=True, lr=self.config.lr)\n",
        "\n",
        "    def _train_epoch(self, loader: DataLoader):\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            correct += (output.argmax(1) == target).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "        return total_loss / total, correct / total\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _eval_epoch(self, loader: DataLoader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            correct += (output.argmax(1) == target).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "        return total_loss / total, correct / total\n",
        "\n",
        "    def fit(self, train_loader: DataLoader, val_loader: DataLoader):\n",
        "        print(\"ðŸš€ Starting training...\\n\")\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "            train_loss, train_acc = self._train_epoch(train_loader)\n",
        "            val_loss, val_acc     = self._eval_epoch(val_loader)\n",
        "\n",
        "            self.history[\"train_loss\"].append(train_loss)\n",
        "            self.history[\"train_acc\"].append(train_acc)\n",
        "            self.history[\"val_loss\"].append(val_loss)\n",
        "            self.history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "            print(f\"Epoch {epoch:3d} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "            if self.early_stopping(val_loss):\n",
        "                print(f\"\\nðŸ›‘ Early stopping triggered at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(\"\\nâœ… Training complete!\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, loader: DataLoader):\n",
        "        return self._eval_epoch(loader)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_all(self, loader: DataLoader):\n",
        "        self.model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            logits = self.model(x)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(y.numpy())\n",
        "        return np.concatenate(all_preds), np.concatenate(all_targets)\n",
        "\n",
        "\n",
        "    def save(self, path: str = \"mlp_best.pt\"):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"ðŸ’¾ Model saved to {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c7d02d",
      "metadata": {
        "id": "55c7d02d"
      },
      "source": [
        "\n",
        "Run (do change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d133bb1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d133bb1a",
        "outputId": "38c40b6f-58a9-4269-ccad-6ef818d21fb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.2M/18.2M [00:28<00:00, 647kB/s] \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 94.1kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.04M/3.04M [00:06<00:00, 451kB/s] \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.12k/5.12k [00:00<00:00, 15.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 54000 | Val: 6000 | Test: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/01/14 13:44:52 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2026/01/14 13:44:52 INFO mlflow.store.db.utils: Updating database tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -> 1bd49d398cd23, add secrets tables\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/14 13:44:52 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2026/01/14 13:44:52 INFO mlflow.tracking.fluent: Experiment with name 'KMNIST_Assigment' does not exist. Creating a new experiment.\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2906 Acc: 0.1059 | Val Loss: 2.2546 Acc: 0.1265\n",
            "Epoch   2 | Train Loss: 2.2338 Acc: 0.1474 | Val Loss: 2.2036 Acc: 0.1915\n",
            "Epoch   3 | Train Loss: 2.1851 Acc: 0.2101 | Val Loss: 2.1554 Acc: 0.2598\n",
            "Epoch   4 | Train Loss: 2.1376 Acc: 0.2672 | Val Loss: 2.1067 Acc: 0.3242\n",
            "Epoch   5 | Train Loss: 2.0890 Acc: 0.3241 | Val Loss: 2.0573 Acc: 0.3868\n",
            "Epoch   6 | Train Loss: 2.0384 Acc: 0.3733 | Val Loss: 2.0023 Acc: 0.4388\n",
            "Epoch   7 | Train Loss: 1.9843 Acc: 0.4139 | Val Loss: 1.9420 Acc: 0.4830\n",
            "Epoch   8 | Train Loss: 1.9284 Acc: 0.4438 | Val Loss: 1.8863 Acc: 0.5042\n",
            "Epoch   9 | Train Loss: 1.8704 Acc: 0.4614 | Val Loss: 1.8214 Acc: 0.5160\n",
            "Epoch  10 | Train Loss: 1.8101 Acc: 0.4739 | Val Loss: 1.7545 Acc: 0.5265\n",
            "Epoch  11 | Train Loss: 1.7527 Acc: 0.4807 | Val Loss: 1.6921 Acc: 0.5328\n",
            "Epoch  12 | Train Loss: 1.6953 Acc: 0.4883 | Val Loss: 1.6321 Acc: 0.5365\n",
            "Epoch  13 | Train Loss: 1.6430 Acc: 0.4952 | Val Loss: 1.5743 Acc: 0.5452\n",
            "Epoch  14 | Train Loss: 1.5899 Acc: 0.5022 | Val Loss: 1.5289 Acc: 0.5507\n",
            "Epoch  15 | Train Loss: 1.5447 Acc: 0.5094 | Val Loss: 1.4773 Acc: 0.5573\n",
            "Epoch  16 | Train Loss: 1.5052 Acc: 0.5145 | Val Loss: 1.4319 Acc: 0.5670\n",
            "Epoch  17 | Train Loss: 1.4655 Acc: 0.5204 | Val Loss: 1.3916 Acc: 0.5728\n",
            "Epoch  18 | Train Loss: 1.4317 Acc: 0.5256 | Val Loss: 1.3563 Acc: 0.5847\n",
            "Epoch  19 | Train Loss: 1.4004 Acc: 0.5330 | Val Loss: 1.3262 Acc: 0.5923\n",
            "Epoch  20 | Train Loss: 1.3698 Acc: 0.5400 | Val Loss: 1.2887 Acc: 0.6008\n",
            "Epoch  21 | Train Loss: 1.3427 Acc: 0.5479 | Val Loss: 1.2607 Acc: 0.6067\n",
            "Epoch  22 | Train Loss: 1.3186 Acc: 0.5556 | Val Loss: 1.2326 Acc: 0.6145\n",
            "Epoch  23 | Train Loss: 1.2943 Acc: 0.5654 | Val Loss: 1.2067 Acc: 0.6232\n",
            "Epoch  24 | Train Loss: 1.2744 Acc: 0.5718 | Val Loss: 1.1867 Acc: 0.6362\n",
            "Epoch  25 | Train Loss: 1.2515 Acc: 0.5824 | Val Loss: 1.1628 Acc: 0.6447\n",
            "Epoch  26 | Train Loss: 1.2301 Acc: 0.5915 | Val Loss: 1.1417 Acc: 0.6538\n",
            "Epoch  27 | Train Loss: 1.2112 Acc: 0.5997 | Val Loss: 1.1181 Acc: 0.6613\n",
            "Epoch  28 | Train Loss: 1.1930 Acc: 0.6094 | Val Loss: 1.1052 Acc: 0.6722\n",
            "Epoch  29 | Train Loss: 1.1803 Acc: 0.6162 | Val Loss: 1.0867 Acc: 0.6813\n",
            "Epoch  30 | Train Loss: 1.1626 Acc: 0.6260 | Val Loss: 1.0672 Acc: 0.6935\n",
            "Epoch  31 | Train Loss: 1.1451 Acc: 0.6359 | Val Loss: 1.0488 Acc: 0.6992\n",
            "Epoch  32 | Train Loss: 1.1293 Acc: 0.6442 | Val Loss: 1.0311 Acc: 0.7080\n",
            "Epoch  33 | Train Loss: 1.1112 Acc: 0.6529 | Val Loss: 1.0149 Acc: 0.7142\n",
            "Epoch  34 | Train Loss: 1.1000 Acc: 0.6586 | Val Loss: 1.0023 Acc: 0.7242\n",
            "Epoch  35 | Train Loss: 1.0801 Acc: 0.6713 | Val Loss: 0.9842 Acc: 0.7327\n",
            "Epoch  36 | Train Loss: 1.0684 Acc: 0.6756 | Val Loss: 0.9676 Acc: 0.7413\n",
            "Epoch  37 | Train Loss: 1.0552 Acc: 0.6840 | Val Loss: 0.9564 Acc: 0.7513\n",
            "Epoch  38 | Train Loss: 1.0424 Acc: 0.6886 | Val Loss: 0.9345 Acc: 0.7553\n",
            "Epoch  39 | Train Loss: 1.0297 Acc: 0.6941 | Val Loss: 0.9238 Acc: 0.7630\n",
            "Epoch  40 | Train Loss: 1.0135 Acc: 0.7018 | Val Loss: 0.9091 Acc: 0.7692\n",
            "Epoch  41 | Train Loss: 1.0015 Acc: 0.7074 | Val Loss: 0.8961 Acc: 0.7758\n",
            "Epoch  42 | Train Loss: 0.9874 Acc: 0.7121 | Val Loss: 0.8776 Acc: 0.7828\n",
            "Epoch  43 | Train Loss: 0.9759 Acc: 0.7154 | Val Loss: 0.8660 Acc: 0.7872\n",
            "Epoch  44 | Train Loss: 0.9600 Acc: 0.7204 | Val Loss: 0.8525 Acc: 0.7912\n",
            "Epoch  45 | Train Loss: 0.9483 Acc: 0.7254 | Val Loss: 0.8390 Acc: 0.7972\n",
            "Epoch  46 | Train Loss: 0.9331 Acc: 0.7274 | Val Loss: 0.8259 Acc: 0.7983\n",
            "Epoch  47 | Train Loss: 0.9243 Acc: 0.7312 | Val Loss: 0.8126 Acc: 0.8040\n",
            "Epoch  48 | Train Loss: 0.9144 Acc: 0.7338 | Val Loss: 0.8036 Acc: 0.8050\n",
            "Epoch  49 | Train Loss: 0.8994 Acc: 0.7390 | Val Loss: 0.7860 Acc: 0.8090\n",
            "Epoch  50 | Train Loss: 0.8910 Acc: 0.7413 | Val Loss: 0.7738 Acc: 0.8120\n",
            "Epoch  51 | Train Loss: 0.8755 Acc: 0.7444 | Val Loss: 0.7637 Acc: 0.8138\n",
            "Epoch  52 | Train Loss: 0.8709 Acc: 0.7431 | Val Loss: 0.7514 Acc: 0.8160\n",
            "Epoch  53 | Train Loss: 0.8559 Acc: 0.7477 | Val Loss: 0.7377 Acc: 0.8190\n",
            "Epoch  54 | Train Loss: 0.8485 Acc: 0.7499 | Val Loss: 0.7280 Acc: 0.8203\n",
            "Epoch  55 | Train Loss: 0.8412 Acc: 0.7509 | Val Loss: 0.7173 Acc: 0.8228\n",
            "Epoch  56 | Train Loss: 0.8326 Acc: 0.7518 | Val Loss: 0.7119 Acc: 0.8262\n",
            "Epoch  57 | Train Loss: 0.8203 Acc: 0.7561 | Val Loss: 0.6988 Acc: 0.8293\n",
            "Epoch  58 | Train Loss: 0.8163 Acc: 0.7574 | Val Loss: 0.6911 Acc: 0.8305\n",
            "Epoch  59 | Train Loss: 0.8028 Acc: 0.7600 | Val Loss: 0.6823 Acc: 0.8328\n",
            "Epoch  60 | Train Loss: 0.7935 Acc: 0.7648 | Val Loss: 0.6740 Acc: 0.8343\n",
            "Epoch  61 | Train Loss: 0.7859 Acc: 0.7641 | Val Loss: 0.6640 Acc: 0.8373\n",
            "Epoch  62 | Train Loss: 0.7823 Acc: 0.7663 | Val Loss: 0.6571 Acc: 0.8382\n",
            "Epoch  63 | Train Loss: 0.7744 Acc: 0.7696 | Val Loss: 0.6505 Acc: 0.8393\n",
            "Epoch  64 | Train Loss: 0.7655 Acc: 0.7708 | Val Loss: 0.6438 Acc: 0.8400\n",
            "Epoch  65 | Train Loss: 0.7599 Acc: 0.7712 | Val Loss: 0.6388 Acc: 0.8423\n",
            "Epoch  66 | Train Loss: 0.7525 Acc: 0.7744 | Val Loss: 0.6282 Acc: 0.8422\n",
            "Epoch  67 | Train Loss: 0.7485 Acc: 0.7749 | Val Loss: 0.6242 Acc: 0.8442\n",
            "Epoch  68 | Train Loss: 0.7424 Acc: 0.7762 | Val Loss: 0.6170 Acc: 0.8458\n",
            "Epoch  69 | Train Loss: 0.7344 Acc: 0.7785 | Val Loss: 0.6131 Acc: 0.8477\n",
            "Epoch  70 | Train Loss: 0.7264 Acc: 0.7809 | Val Loss: 0.6053 Acc: 0.8492\n",
            "Epoch  71 | Train Loss: 0.7246 Acc: 0.7821 | Val Loss: 0.6008 Acc: 0.8508\n",
            "Epoch  72 | Train Loss: 0.7221 Acc: 0.7811 | Val Loss: 0.5973 Acc: 0.8497\n",
            "Epoch  73 | Train Loss: 0.7165 Acc: 0.7821 | Val Loss: 0.5908 Acc: 0.8493\n",
            "Epoch  74 | Train Loss: 0.7145 Acc: 0.7837 | Val Loss: 0.5855 Acc: 0.8538\n",
            "Epoch  75 | Train Loss: 0.7109 Acc: 0.7830 | Val Loss: 0.5814 Acc: 0.8557\n",
            "Epoch  76 | Train Loss: 0.7026 Acc: 0.7878 | Val Loss: 0.5753 Acc: 0.8558\n",
            "Epoch  77 | Train Loss: 0.6989 Acc: 0.7880 | Val Loss: 0.5747 Acc: 0.8537\n",
            "Epoch  78 | Train Loss: 0.6968 Acc: 0.7892 | Val Loss: 0.5691 Acc: 0.8563\n",
            "Epoch  79 | Train Loss: 0.6884 Acc: 0.7902 | Val Loss: 0.5653 Acc: 0.8562\n",
            "Epoch  80 | Train Loss: 0.6825 Acc: 0.7936 | Val Loss: 0.5614 Acc: 0.8585\n",
            "Epoch  81 | Train Loss: 0.6802 Acc: 0.7939 | Val Loss: 0.5573 Acc: 0.8573\n",
            "Epoch  82 | Train Loss: 0.6800 Acc: 0.7943 | Val Loss: 0.5542 Acc: 0.8587\n",
            "Epoch  83 | Train Loss: 0.6695 Acc: 0.7959 | Val Loss: 0.5491 Acc: 0.8602\n",
            "Epoch  84 | Train Loss: 0.6719 Acc: 0.7964 | Val Loss: 0.5459 Acc: 0.8598\n",
            "Epoch  85 | Train Loss: 0.6664 Acc: 0.7976 | Val Loss: 0.5435 Acc: 0.8608\n",
            "Epoch  86 | Train Loss: 0.6640 Acc: 0.7989 | Val Loss: 0.5401 Acc: 0.8613\n",
            "Epoch  87 | Train Loss: 0.6598 Acc: 0.8002 | Val Loss: 0.5374 Acc: 0.8625\n",
            "Epoch  88 | Train Loss: 0.6567 Acc: 0.7996 | Val Loss: 0.5354 Acc: 0.8630\n",
            "Epoch  89 | Train Loss: 0.6539 Acc: 0.8011 | Val Loss: 0.5329 Acc: 0.8618\n",
            "Epoch  90 | Train Loss: 0.6459 Acc: 0.8027 | Val Loss: 0.5269 Acc: 0.8645\n",
            "Epoch  91 | Train Loss: 0.6422 Acc: 0.8051 | Val Loss: 0.5248 Acc: 0.8648\n",
            "Epoch  92 | Train Loss: 0.6428 Acc: 0.8023 | Val Loss: 0.5203 Acc: 0.8662\n",
            "Epoch  93 | Train Loss: 0.6385 Acc: 0.8058 | Val Loss: 0.5199 Acc: 0.8667\n",
            "Epoch  94 | Train Loss: 0.6389 Acc: 0.8046 | Val Loss: 0.5145 Acc: 0.8647\n",
            "Epoch  95 | Train Loss: 0.6295 Acc: 0.8092 | Val Loss: 0.5141 Acc: 0.8665\n",
            "Epoch  96 | Train Loss: 0.6327 Acc: 0.8039 | Val Loss: 0.5093 Acc: 0.8668\n",
            "Epoch  97 | Train Loss: 0.6294 Acc: 0.8076 | Val Loss: 0.5102 Acc: 0.8663\n",
            "Epoch  98 | Train Loss: 0.6283 Acc: 0.8073 | Val Loss: 0.5047 Acc: 0.8678\n",
            "Epoch  99 | Train Loss: 0.6192 Acc: 0.8101 | Val Loss: 0.5027 Acc: 0.8667\n",
            "Epoch 100 | Train Loss: 0.6173 Acc: 0.8104 | Val Loss: 0.5012 Acc: 0.8683\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_0.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2950 Acc: 0.1092 | Val Loss: 2.2431 Acc: 0.1297\n",
            "Epoch   2 | Train Loss: 2.2083 Acc: 0.1912 | Val Loss: 2.1594 Acc: 0.2577\n",
            "Epoch   3 | Train Loss: 2.1284 Acc: 0.2908 | Val Loss: 2.0739 Acc: 0.3557\n",
            "Epoch   4 | Train Loss: 2.0448 Acc: 0.3401 | Val Loss: 1.9834 Acc: 0.3947\n",
            "Epoch   5 | Train Loss: 1.9607 Acc: 0.3764 | Val Loss: 1.8901 Acc: 0.4573\n",
            "Epoch   6 | Train Loss: 1.8731 Acc: 0.4139 | Val Loss: 1.8039 Acc: 0.4747\n",
            "Epoch   7 | Train Loss: 1.7924 Acc: 0.4218 | Val Loss: 1.7132 Acc: 0.4848\n",
            "Epoch   8 | Train Loss: 1.7149 Acc: 0.4326 | Val Loss: 1.6350 Acc: 0.4953\n",
            "Epoch   9 | Train Loss: 1.6428 Acc: 0.4486 | Val Loss: 1.5623 Acc: 0.5122\n",
            "Epoch  10 | Train Loss: 1.5793 Acc: 0.4648 | Val Loss: 1.4970 Acc: 0.5338\n",
            "Epoch  11 | Train Loss: 1.5230 Acc: 0.4819 | Val Loss: 1.4309 Acc: 0.5567\n",
            "Epoch  12 | Train Loss: 1.4654 Acc: 0.5049 | Val Loss: 1.3769 Acc: 0.5750\n",
            "Epoch  13 | Train Loss: 1.4137 Acc: 0.5259 | Val Loss: 1.3156 Acc: 0.6037\n",
            "Epoch  14 | Train Loss: 1.3614 Acc: 0.5519 | Val Loss: 1.2641 Acc: 0.6273\n",
            "Epoch  15 | Train Loss: 1.3132 Acc: 0.5785 | Val Loss: 1.2164 Acc: 0.6608\n",
            "Epoch  16 | Train Loss: 1.2722 Acc: 0.6080 | Val Loss: 1.1629 Acc: 0.6918\n",
            "Epoch  17 | Train Loss: 1.2241 Acc: 0.6338 | Val Loss: 1.1167 Acc: 0.7112\n",
            "Epoch  18 | Train Loss: 1.1848 Acc: 0.6501 | Val Loss: 1.0724 Acc: 0.7243\n",
            "Epoch  19 | Train Loss: 1.1472 Acc: 0.6646 | Val Loss: 1.0328 Acc: 0.7380\n",
            "Epoch  20 | Train Loss: 1.1087 Acc: 0.6782 | Val Loss: 0.9943 Acc: 0.7463\n",
            "Epoch  21 | Train Loss: 1.0746 Acc: 0.6893 | Val Loss: 0.9661 Acc: 0.7528\n",
            "Epoch  22 | Train Loss: 1.0473 Acc: 0.6959 | Val Loss: 0.9266 Acc: 0.7600\n",
            "Epoch  23 | Train Loss: 1.0158 Acc: 0.7054 | Val Loss: 0.8978 Acc: 0.7667\n",
            "Epoch  24 | Train Loss: 0.9945 Acc: 0.7104 | Val Loss: 0.8757 Acc: 0.7737\n",
            "Epoch  25 | Train Loss: 0.9684 Acc: 0.7158 | Val Loss: 0.8520 Acc: 0.7800\n",
            "Epoch  26 | Train Loss: 0.9503 Acc: 0.7201 | Val Loss: 0.8258 Acc: 0.7838\n",
            "Epoch  27 | Train Loss: 0.9246 Acc: 0.7285 | Val Loss: 0.8048 Acc: 0.7913\n",
            "Epoch  28 | Train Loss: 0.9073 Acc: 0.7324 | Val Loss: 0.7866 Acc: 0.7940\n",
            "Epoch  29 | Train Loss: 0.8881 Acc: 0.7376 | Val Loss: 0.7641 Acc: 0.7990\n",
            "Epoch  30 | Train Loss: 0.8752 Acc: 0.7404 | Val Loss: 0.7498 Acc: 0.8027\n",
            "Epoch  31 | Train Loss: 0.8539 Acc: 0.7485 | Val Loss: 0.7300 Acc: 0.8083\n",
            "Epoch  32 | Train Loss: 0.8412 Acc: 0.7504 | Val Loss: 0.7132 Acc: 0.8093\n",
            "Epoch  33 | Train Loss: 0.8284 Acc: 0.7561 | Val Loss: 0.7018 Acc: 0.8135\n",
            "Epoch  34 | Train Loss: 0.8142 Acc: 0.7579 | Val Loss: 0.6897 Acc: 0.8193\n",
            "Epoch  35 | Train Loss: 0.7999 Acc: 0.7620 | Val Loss: 0.6733 Acc: 0.8212\n",
            "Epoch  36 | Train Loss: 0.7896 Acc: 0.7672 | Val Loss: 0.6641 Acc: 0.8237\n",
            "Epoch  37 | Train Loss: 0.7798 Acc: 0.7677 | Val Loss: 0.6516 Acc: 0.8290\n",
            "Epoch  38 | Train Loss: 0.7633 Acc: 0.7726 | Val Loss: 0.6423 Acc: 0.8298\n",
            "Epoch  39 | Train Loss: 0.7535 Acc: 0.7765 | Val Loss: 0.6292 Acc: 0.8318\n",
            "Epoch  40 | Train Loss: 0.7481 Acc: 0.7747 | Val Loss: 0.6228 Acc: 0.8360\n",
            "Epoch  41 | Train Loss: 0.7383 Acc: 0.7802 | Val Loss: 0.6102 Acc: 0.8377\n",
            "Epoch  42 | Train Loss: 0.7296 Acc: 0.7820 | Val Loss: 0.6032 Acc: 0.8407\n",
            "Epoch  43 | Train Loss: 0.7159 Acc: 0.7875 | Val Loss: 0.5927 Acc: 0.8428\n",
            "Epoch  44 | Train Loss: 0.7101 Acc: 0.7874 | Val Loss: 0.5860 Acc: 0.8460\n",
            "Epoch  45 | Train Loss: 0.7004 Acc: 0.7905 | Val Loss: 0.5758 Acc: 0.8483\n",
            "Epoch  46 | Train Loss: 0.6957 Acc: 0.7914 | Val Loss: 0.5677 Acc: 0.8490\n",
            "Epoch  47 | Train Loss: 0.6859 Acc: 0.7966 | Val Loss: 0.5621 Acc: 0.8518\n",
            "Epoch  48 | Train Loss: 0.6768 Acc: 0.7957 | Val Loss: 0.5565 Acc: 0.8543\n",
            "Epoch  49 | Train Loss: 0.6729 Acc: 0.7990 | Val Loss: 0.5490 Acc: 0.8560\n",
            "Epoch  50 | Train Loss: 0.6649 Acc: 0.8007 | Val Loss: 0.5404 Acc: 0.8588\n",
            "Epoch  51 | Train Loss: 0.6552 Acc: 0.8034 | Val Loss: 0.5365 Acc: 0.8593\n",
            "Epoch  52 | Train Loss: 0.6525 Acc: 0.8035 | Val Loss: 0.5304 Acc: 0.8615\n",
            "Epoch  53 | Train Loss: 0.6435 Acc: 0.8066 | Val Loss: 0.5240 Acc: 0.8612\n",
            "Epoch  54 | Train Loss: 0.6404 Acc: 0.8089 | Val Loss: 0.5172 Acc: 0.8635\n",
            "Epoch  55 | Train Loss: 0.6379 Acc: 0.8070 | Val Loss: 0.5124 Acc: 0.8658\n",
            "Epoch  56 | Train Loss: 0.6302 Acc: 0.8103 | Val Loss: 0.5062 Acc: 0.8673\n",
            "Epoch  57 | Train Loss: 0.6234 Acc: 0.8123 | Val Loss: 0.5024 Acc: 0.8682\n",
            "Epoch  58 | Train Loss: 0.6166 Acc: 0.8141 | Val Loss: 0.4965 Acc: 0.8695\n",
            "Epoch  59 | Train Loss: 0.6135 Acc: 0.8138 | Val Loss: 0.4924 Acc: 0.8712\n",
            "Epoch  60 | Train Loss: 0.6058 Acc: 0.8166 | Val Loss: 0.4858 Acc: 0.8720\n",
            "Epoch  61 | Train Loss: 0.5995 Acc: 0.8190 | Val Loss: 0.4846 Acc: 0.8730\n",
            "Epoch  62 | Train Loss: 0.5983 Acc: 0.8184 | Val Loss: 0.4791 Acc: 0.8748\n",
            "Epoch  63 | Train Loss: 0.5911 Acc: 0.8220 | Val Loss: 0.4739 Acc: 0.8760\n",
            "Epoch  64 | Train Loss: 0.5872 Acc: 0.8218 | Val Loss: 0.4711 Acc: 0.8748\n",
            "Epoch  65 | Train Loss: 0.5847 Acc: 0.8224 | Val Loss: 0.4634 Acc: 0.8785\n",
            "Epoch  66 | Train Loss: 0.5813 Acc: 0.8227 | Val Loss: 0.4628 Acc: 0.8783\n",
            "Epoch  67 | Train Loss: 0.5742 Acc: 0.8255 | Val Loss: 0.4580 Acc: 0.8782\n",
            "Epoch  68 | Train Loss: 0.5688 Acc: 0.8281 | Val Loss: 0.4521 Acc: 0.8790\n",
            "Epoch  69 | Train Loss: 0.5641 Acc: 0.8277 | Val Loss: 0.4484 Acc: 0.8803\n",
            "Epoch  70 | Train Loss: 0.5625 Acc: 0.8287 | Val Loss: 0.4462 Acc: 0.8818\n",
            "Epoch  71 | Train Loss: 0.5567 Acc: 0.8307 | Val Loss: 0.4422 Acc: 0.8823\n",
            "Epoch  72 | Train Loss: 0.5507 Acc: 0.8335 | Val Loss: 0.4403 Acc: 0.8842\n",
            "Epoch  73 | Train Loss: 0.5499 Acc: 0.8338 | Val Loss: 0.4381 Acc: 0.8842\n",
            "Epoch  74 | Train Loss: 0.5492 Acc: 0.8309 | Val Loss: 0.4333 Acc: 0.8827\n",
            "Epoch  75 | Train Loss: 0.5414 Acc: 0.8360 | Val Loss: 0.4324 Acc: 0.8852\n",
            "Epoch  76 | Train Loss: 0.5399 Acc: 0.8348 | Val Loss: 0.4265 Acc: 0.8855\n",
            "Epoch  77 | Train Loss: 0.5332 Acc: 0.8375 | Val Loss: 0.4252 Acc: 0.8873\n",
            "Epoch  78 | Train Loss: 0.5359 Acc: 0.8360 | Val Loss: 0.4209 Acc: 0.8880\n",
            "Epoch  79 | Train Loss: 0.5297 Acc: 0.8383 | Val Loss: 0.4192 Acc: 0.8878\n",
            "Epoch  80 | Train Loss: 0.5251 Acc: 0.8389 | Val Loss: 0.4169 Acc: 0.8887\n",
            "Epoch  81 | Train Loss: 0.5201 Acc: 0.8403 | Val Loss: 0.4140 Acc: 0.8887\n",
            "Epoch  82 | Train Loss: 0.5197 Acc: 0.8408 | Val Loss: 0.4089 Acc: 0.8898\n",
            "Epoch  83 | Train Loss: 0.5137 Acc: 0.8415 | Val Loss: 0.4075 Acc: 0.8898\n",
            "Epoch  84 | Train Loss: 0.5100 Acc: 0.8438 | Val Loss: 0.4038 Acc: 0.8935\n",
            "Epoch  85 | Train Loss: 0.5068 Acc: 0.8454 | Val Loss: 0.4024 Acc: 0.8953\n",
            "Epoch  86 | Train Loss: 0.5030 Acc: 0.8461 | Val Loss: 0.3985 Acc: 0.8945\n",
            "Epoch  87 | Train Loss: 0.4999 Acc: 0.8461 | Val Loss: 0.3969 Acc: 0.8952\n",
            "Epoch  88 | Train Loss: 0.4979 Acc: 0.8476 | Val Loss: 0.3943 Acc: 0.8958\n",
            "Epoch  89 | Train Loss: 0.4986 Acc: 0.8475 | Val Loss: 0.3933 Acc: 0.8968\n",
            "Epoch  90 | Train Loss: 0.4948 Acc: 0.8476 | Val Loss: 0.3893 Acc: 0.8978\n",
            "Epoch  91 | Train Loss: 0.4901 Acc: 0.8481 | Val Loss: 0.3880 Acc: 0.8982\n",
            "Epoch  92 | Train Loss: 0.4870 Acc: 0.8512 | Val Loss: 0.3878 Acc: 0.8952\n",
            "Epoch  93 | Train Loss: 0.4839 Acc: 0.8509 | Val Loss: 0.3840 Acc: 0.8977\n",
            "Epoch  94 | Train Loss: 0.4836 Acc: 0.8503 | Val Loss: 0.3794 Acc: 0.8982\n",
            "Epoch  95 | Train Loss: 0.4794 Acc: 0.8526 | Val Loss: 0.3799 Acc: 0.8970\n",
            "Epoch  96 | Train Loss: 0.4788 Acc: 0.8534 | Val Loss: 0.3779 Acc: 0.8975\n",
            "Epoch  97 | Train Loss: 0.4770 Acc: 0.8533 | Val Loss: 0.3760 Acc: 0.9002\n",
            "Epoch  98 | Train Loss: 0.4719 Acc: 0.8553 | Val Loss: 0.3738 Acc: 0.8998\n",
            "Epoch  99 | Train Loss: 0.4706 Acc: 0.8559 | Val Loss: 0.3726 Acc: 0.8988\n",
            "Epoch 100 | Train Loss: 0.4673 Acc: 0.8556 | Val Loss: 0.3702 Acc: 0.9010\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_1.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2529 Acc: 0.1768 | Val Loss: 2.1717 Acc: 0.2353\n",
            "Epoch   2 | Train Loss: 2.1276 Acc: 0.2589 | Val Loss: 2.0522 Acc: 0.3307\n",
            "Epoch   3 | Train Loss: 2.0136 Acc: 0.3451 | Val Loss: 1.9352 Acc: 0.4410\n",
            "Epoch   4 | Train Loss: 1.9017 Acc: 0.4188 | Val Loss: 1.8152 Acc: 0.5145\n",
            "Epoch   5 | Train Loss: 1.7898 Acc: 0.4751 | Val Loss: 1.6993 Acc: 0.5578\n",
            "Epoch   6 | Train Loss: 1.6859 Acc: 0.5077 | Val Loss: 1.5958 Acc: 0.5803\n",
            "Epoch   7 | Train Loss: 1.5922 Acc: 0.5242 | Val Loss: 1.4923 Acc: 0.5980\n",
            "Epoch   8 | Train Loss: 1.5014 Acc: 0.5536 | Val Loss: 1.3958 Acc: 0.6342\n",
            "Epoch   9 | Train Loss: 1.4197 Acc: 0.5836 | Val Loss: 1.3117 Acc: 0.6663\n",
            "Epoch  10 | Train Loss: 1.3442 Acc: 0.6074 | Val Loss: 1.2270 Acc: 0.6848\n",
            "Epoch  11 | Train Loss: 1.2781 Acc: 0.6218 | Val Loss: 1.1585 Acc: 0.7002\n",
            "Epoch  12 | Train Loss: 1.2186 Acc: 0.6358 | Val Loss: 1.0944 Acc: 0.7138\n",
            "Epoch  13 | Train Loss: 1.1678 Acc: 0.6455 | Val Loss: 1.0409 Acc: 0.7232\n",
            "Epoch  14 | Train Loss: 1.1217 Acc: 0.6569 | Val Loss: 0.9950 Acc: 0.7328\n",
            "Epoch  15 | Train Loss: 1.0837 Acc: 0.6657 | Val Loss: 0.9519 Acc: 0.7412\n",
            "Epoch  16 | Train Loss: 1.0471 Acc: 0.6769 | Val Loss: 0.9158 Acc: 0.7502\n",
            "Epoch  17 | Train Loss: 1.0195 Acc: 0.6839 | Val Loss: 0.8822 Acc: 0.7567\n",
            "Epoch  18 | Train Loss: 0.9870 Acc: 0.6920 | Val Loss: 0.8542 Acc: 0.7637\n",
            "Epoch  19 | Train Loss: 0.9667 Acc: 0.6997 | Val Loss: 0.8267 Acc: 0.7732\n",
            "Epoch  20 | Train Loss: 0.9380 Acc: 0.7075 | Val Loss: 0.8029 Acc: 0.7780\n",
            "Epoch  21 | Train Loss: 0.9205 Acc: 0.7101 | Val Loss: 0.7829 Acc: 0.7848\n",
            "Epoch  22 | Train Loss: 0.8968 Acc: 0.7198 | Val Loss: 0.7577 Acc: 0.7918\n",
            "Epoch  23 | Train Loss: 0.8817 Acc: 0.7228 | Val Loss: 0.7389 Acc: 0.7952\n",
            "Epoch  24 | Train Loss: 0.8638 Acc: 0.7287 | Val Loss: 0.7243 Acc: 0.7982\n",
            "Epoch  25 | Train Loss: 0.8468 Acc: 0.7332 | Val Loss: 0.7071 Acc: 0.8042\n",
            "Epoch  26 | Train Loss: 0.8327 Acc: 0.7364 | Val Loss: 0.6919 Acc: 0.8090\n",
            "Epoch  27 | Train Loss: 0.8217 Acc: 0.7388 | Val Loss: 0.6783 Acc: 0.8145\n",
            "Epoch  28 | Train Loss: 0.8081 Acc: 0.7453 | Val Loss: 0.6680 Acc: 0.8167\n",
            "Epoch  29 | Train Loss: 0.7936 Acc: 0.7492 | Val Loss: 0.6532 Acc: 0.8222\n",
            "Epoch  30 | Train Loss: 0.7845 Acc: 0.7521 | Val Loss: 0.6420 Acc: 0.8263\n",
            "Epoch  31 | Train Loss: 0.7707 Acc: 0.7574 | Val Loss: 0.6296 Acc: 0.8278\n",
            "Epoch  32 | Train Loss: 0.7619 Acc: 0.7580 | Val Loss: 0.6205 Acc: 0.8320\n",
            "Epoch  33 | Train Loss: 0.7515 Acc: 0.7618 | Val Loss: 0.6116 Acc: 0.8352\n",
            "Epoch  34 | Train Loss: 0.7385 Acc: 0.7667 | Val Loss: 0.6019 Acc: 0.8393\n",
            "Epoch  35 | Train Loss: 0.7264 Acc: 0.7714 | Val Loss: 0.5909 Acc: 0.8392\n",
            "Epoch  36 | Train Loss: 0.7188 Acc: 0.7737 | Val Loss: 0.5821 Acc: 0.8410\n",
            "Epoch  37 | Train Loss: 0.7102 Acc: 0.7756 | Val Loss: 0.5741 Acc: 0.8440\n",
            "Epoch  38 | Train Loss: 0.7019 Acc: 0.7794 | Val Loss: 0.5631 Acc: 0.8473\n",
            "Epoch  39 | Train Loss: 0.6936 Acc: 0.7815 | Val Loss: 0.5551 Acc: 0.8500\n",
            "Epoch  40 | Train Loss: 0.6837 Acc: 0.7864 | Val Loss: 0.5487 Acc: 0.8512\n",
            "Epoch  41 | Train Loss: 0.6775 Acc: 0.7856 | Val Loss: 0.5421 Acc: 0.8528\n",
            "Epoch  42 | Train Loss: 0.6683 Acc: 0.7898 | Val Loss: 0.5335 Acc: 0.8547\n",
            "Epoch  43 | Train Loss: 0.6617 Acc: 0.7927 | Val Loss: 0.5265 Acc: 0.8568\n",
            "Epoch  44 | Train Loss: 0.6539 Acc: 0.7955 | Val Loss: 0.5199 Acc: 0.8595\n",
            "Epoch  45 | Train Loss: 0.6442 Acc: 0.7976 | Val Loss: 0.5120 Acc: 0.8618\n",
            "Epoch  46 | Train Loss: 0.6445 Acc: 0.7970 | Val Loss: 0.5073 Acc: 0.8620\n",
            "Epoch  47 | Train Loss: 0.6356 Acc: 0.8017 | Val Loss: 0.5014 Acc: 0.8643\n",
            "Epoch  48 | Train Loss: 0.6268 Acc: 0.8045 | Val Loss: 0.4956 Acc: 0.8663\n",
            "Epoch  49 | Train Loss: 0.6219 Acc: 0.8046 | Val Loss: 0.4859 Acc: 0.8683\n",
            "Epoch  50 | Train Loss: 0.6107 Acc: 0.8083 | Val Loss: 0.4826 Acc: 0.8697\n",
            "Epoch  51 | Train Loss: 0.6100 Acc: 0.8096 | Val Loss: 0.4780 Acc: 0.8710\n",
            "Epoch  52 | Train Loss: 0.5983 Acc: 0.8141 | Val Loss: 0.4720 Acc: 0.8725\n",
            "Epoch  53 | Train Loss: 0.5947 Acc: 0.8143 | Val Loss: 0.4651 Acc: 0.8735\n",
            "Epoch  54 | Train Loss: 0.5919 Acc: 0.8149 | Val Loss: 0.4629 Acc: 0.8758\n",
            "Epoch  55 | Train Loss: 0.5831 Acc: 0.8178 | Val Loss: 0.4574 Acc: 0.8775\n",
            "Epoch  56 | Train Loss: 0.5771 Acc: 0.8193 | Val Loss: 0.4525 Acc: 0.8780\n",
            "Epoch  57 | Train Loss: 0.5736 Acc: 0.8209 | Val Loss: 0.4479 Acc: 0.8793\n",
            "Epoch  58 | Train Loss: 0.5697 Acc: 0.8202 | Val Loss: 0.4438 Acc: 0.8798\n",
            "Epoch  59 | Train Loss: 0.5625 Acc: 0.8239 | Val Loss: 0.4397 Acc: 0.8798\n",
            "Epoch  60 | Train Loss: 0.5560 Acc: 0.8265 | Val Loss: 0.4351 Acc: 0.8812\n",
            "Epoch  61 | Train Loss: 0.5497 Acc: 0.8274 | Val Loss: 0.4309 Acc: 0.8850\n",
            "Epoch  62 | Train Loss: 0.5480 Acc: 0.8294 | Val Loss: 0.4288 Acc: 0.8853\n",
            "Epoch  63 | Train Loss: 0.5406 Acc: 0.8314 | Val Loss: 0.4236 Acc: 0.8862\n",
            "Epoch  64 | Train Loss: 0.5405 Acc: 0.8309 | Val Loss: 0.4219 Acc: 0.8855\n",
            "Epoch  65 | Train Loss: 0.5341 Acc: 0.8334 | Val Loss: 0.4159 Acc: 0.8857\n",
            "Epoch  66 | Train Loss: 0.5259 Acc: 0.8377 | Val Loss: 0.4115 Acc: 0.8887\n",
            "Epoch  67 | Train Loss: 0.5253 Acc: 0.8362 | Val Loss: 0.4086 Acc: 0.8888\n",
            "Epoch  68 | Train Loss: 0.5215 Acc: 0.8376 | Val Loss: 0.4060 Acc: 0.8893\n",
            "Epoch  69 | Train Loss: 0.5159 Acc: 0.8378 | Val Loss: 0.4019 Acc: 0.8908\n",
            "Epoch  70 | Train Loss: 0.5128 Acc: 0.8387 | Val Loss: 0.3974 Acc: 0.8923\n",
            "Epoch  71 | Train Loss: 0.5106 Acc: 0.8399 | Val Loss: 0.3965 Acc: 0.8915\n",
            "Epoch  72 | Train Loss: 0.5053 Acc: 0.8426 | Val Loss: 0.3915 Acc: 0.8923\n",
            "Epoch  73 | Train Loss: 0.5014 Acc: 0.8440 | Val Loss: 0.3894 Acc: 0.8938\n",
            "Epoch  74 | Train Loss: 0.4990 Acc: 0.8447 | Val Loss: 0.3860 Acc: 0.8952\n",
            "Epoch  75 | Train Loss: 0.4906 Acc: 0.8477 | Val Loss: 0.3852 Acc: 0.8957\n",
            "Epoch  76 | Train Loss: 0.4867 Acc: 0.8477 | Val Loss: 0.3816 Acc: 0.8973\n",
            "Epoch  77 | Train Loss: 0.4874 Acc: 0.8468 | Val Loss: 0.3780 Acc: 0.8985\n",
            "Epoch  78 | Train Loss: 0.4837 Acc: 0.8506 | Val Loss: 0.3751 Acc: 0.8987\n",
            "Epoch  79 | Train Loss: 0.4776 Acc: 0.8518 | Val Loss: 0.3731 Acc: 0.8988\n",
            "Epoch  80 | Train Loss: 0.4736 Acc: 0.8533 | Val Loss: 0.3706 Acc: 0.8988\n",
            "Epoch  81 | Train Loss: 0.4685 Acc: 0.8547 | Val Loss: 0.3666 Acc: 0.9025\n",
            "Epoch  82 | Train Loss: 0.4672 Acc: 0.8538 | Val Loss: 0.3652 Acc: 0.9027\n",
            "Epoch  83 | Train Loss: 0.4660 Acc: 0.8539 | Val Loss: 0.3626 Acc: 0.9022\n",
            "Epoch  84 | Train Loss: 0.4657 Acc: 0.8543 | Val Loss: 0.3604 Acc: 0.9040\n",
            "Epoch  85 | Train Loss: 0.4568 Acc: 0.8585 | Val Loss: 0.3570 Acc: 0.9050\n",
            "Epoch  86 | Train Loss: 0.4572 Acc: 0.8573 | Val Loss: 0.3550 Acc: 0.9062\n",
            "Epoch  87 | Train Loss: 0.4550 Acc: 0.8590 | Val Loss: 0.3525 Acc: 0.9058\n",
            "Epoch  88 | Train Loss: 0.4517 Acc: 0.8583 | Val Loss: 0.3505 Acc: 0.9067\n",
            "Epoch  89 | Train Loss: 0.4454 Acc: 0.8610 | Val Loss: 0.3480 Acc: 0.9077\n",
            "Epoch  90 | Train Loss: 0.4443 Acc: 0.8617 | Val Loss: 0.3473 Acc: 0.9068\n",
            "Epoch  91 | Train Loss: 0.4389 Acc: 0.8645 | Val Loss: 0.3452 Acc: 0.9088\n",
            "Epoch  92 | Train Loss: 0.4383 Acc: 0.8621 | Val Loss: 0.3428 Acc: 0.9093\n",
            "Epoch  93 | Train Loss: 0.4322 Acc: 0.8652 | Val Loss: 0.3413 Acc: 0.9087\n",
            "Epoch  94 | Train Loss: 0.4359 Acc: 0.8631 | Val Loss: 0.3393 Acc: 0.9093\n",
            "Epoch  95 | Train Loss: 0.4286 Acc: 0.8665 | Val Loss: 0.3363 Acc: 0.9088\n",
            "Epoch  96 | Train Loss: 0.4268 Acc: 0.8666 | Val Loss: 0.3339 Acc: 0.9107\n",
            "Epoch  97 | Train Loss: 0.4264 Acc: 0.8669 | Val Loss: 0.3325 Acc: 0.9095\n",
            "Epoch  98 | Train Loss: 0.4221 Acc: 0.8676 | Val Loss: 0.3306 Acc: 0.9117\n",
            "Epoch  99 | Train Loss: 0.4200 Acc: 0.8672 | Val Loss: 0.3290 Acc: 0.9115\n",
            "Epoch 100 | Train Loss: 0.4161 Acc: 0.8688 | Val Loss: 0.3290 Acc: 0.9110\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_2.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2503 Acc: 0.1954 | Val Loss: 2.1693 Acc: 0.3000\n",
            "Epoch   2 | Train Loss: 2.0863 Acc: 0.3404 | Val Loss: 1.9813 Acc: 0.4185\n",
            "Epoch   3 | Train Loss: 1.9133 Acc: 0.4260 | Val Loss: 1.8046 Acc: 0.5167\n",
            "Epoch   4 | Train Loss: 1.7536 Acc: 0.5155 | Val Loss: 1.6382 Acc: 0.6163\n",
            "Epoch   5 | Train Loss: 1.6042 Acc: 0.5786 | Val Loss: 1.4804 Acc: 0.6713\n",
            "Epoch   6 | Train Loss: 1.4665 Acc: 0.6180 | Val Loss: 1.3401 Acc: 0.7020\n",
            "Epoch   7 | Train Loss: 1.3506 Acc: 0.6424 | Val Loss: 1.2248 Acc: 0.7237\n",
            "Epoch   8 | Train Loss: 1.2539 Acc: 0.6606 | Val Loss: 1.1294 Acc: 0.7388\n",
            "Epoch   9 | Train Loss: 1.1775 Acc: 0.6749 | Val Loss: 1.0404 Acc: 0.7573\n",
            "Epoch  10 | Train Loss: 1.1082 Acc: 0.6892 | Val Loss: 0.9747 Acc: 0.7672\n",
            "Epoch  11 | Train Loss: 1.0561 Acc: 0.6980 | Val Loss: 0.9176 Acc: 0.7728\n",
            "Epoch  12 | Train Loss: 1.0082 Acc: 0.7068 | Val Loss: 0.8703 Acc: 0.7833\n",
            "Epoch  13 | Train Loss: 0.9699 Acc: 0.7150 | Val Loss: 0.8269 Acc: 0.7878\n",
            "Epoch  14 | Train Loss: 0.9337 Acc: 0.7218 | Val Loss: 0.7927 Acc: 0.7928\n",
            "Epoch  15 | Train Loss: 0.9041 Acc: 0.7285 | Val Loss: 0.7605 Acc: 0.8010\n",
            "Epoch  16 | Train Loss: 0.8765 Acc: 0.7335 | Val Loss: 0.7331 Acc: 0.8062\n",
            "Epoch  17 | Train Loss: 0.8538 Acc: 0.7376 | Val Loss: 0.7085 Acc: 0.8102\n",
            "Epoch  18 | Train Loss: 0.8290 Acc: 0.7458 | Val Loss: 0.6883 Acc: 0.8160\n",
            "Epoch  19 | Train Loss: 0.8146 Acc: 0.7502 | Val Loss: 0.6682 Acc: 0.8197\n",
            "Epoch  20 | Train Loss: 0.7935 Acc: 0.7557 | Val Loss: 0.6539 Acc: 0.8242\n",
            "Epoch  21 | Train Loss: 0.7711 Acc: 0.7618 | Val Loss: 0.6342 Acc: 0.8283\n",
            "Epoch  22 | Train Loss: 0.7590 Acc: 0.7651 | Val Loss: 0.6192 Acc: 0.8313\n",
            "Epoch  23 | Train Loss: 0.7497 Acc: 0.7661 | Val Loss: 0.6037 Acc: 0.8348\n",
            "Epoch  24 | Train Loss: 0.7355 Acc: 0.7724 | Val Loss: 0.5917 Acc: 0.8372\n",
            "Epoch  25 | Train Loss: 0.7172 Acc: 0.7780 | Val Loss: 0.5796 Acc: 0.8378\n",
            "Epoch  26 | Train Loss: 0.7142 Acc: 0.7780 | Val Loss: 0.5684 Acc: 0.8433\n",
            "Epoch  27 | Train Loss: 0.6966 Acc: 0.7829 | Val Loss: 0.5588 Acc: 0.8460\n",
            "Epoch  28 | Train Loss: 0.6868 Acc: 0.7881 | Val Loss: 0.5498 Acc: 0.8487\n",
            "Epoch  29 | Train Loss: 0.6813 Acc: 0.7882 | Val Loss: 0.5398 Acc: 0.8513\n",
            "Epoch  30 | Train Loss: 0.6671 Acc: 0.7934 | Val Loss: 0.5314 Acc: 0.8522\n",
            "Epoch  31 | Train Loss: 0.6614 Acc: 0.7954 | Val Loss: 0.5254 Acc: 0.8537\n",
            "Epoch  32 | Train Loss: 0.6516 Acc: 0.7992 | Val Loss: 0.5139 Acc: 0.8568\n",
            "Epoch  33 | Train Loss: 0.6441 Acc: 0.7998 | Val Loss: 0.5078 Acc: 0.8603\n",
            "Epoch  34 | Train Loss: 0.6371 Acc: 0.8014 | Val Loss: 0.5007 Acc: 0.8618\n",
            "Epoch  35 | Train Loss: 0.6280 Acc: 0.8053 | Val Loss: 0.4929 Acc: 0.8635\n",
            "Epoch  36 | Train Loss: 0.6193 Acc: 0.8088 | Val Loss: 0.4860 Acc: 0.8658\n",
            "Epoch  37 | Train Loss: 0.6118 Acc: 0.8120 | Val Loss: 0.4779 Acc: 0.8695\n",
            "Epoch  38 | Train Loss: 0.6063 Acc: 0.8121 | Val Loss: 0.4744 Acc: 0.8707\n",
            "Epoch  39 | Train Loss: 0.5974 Acc: 0.8156 | Val Loss: 0.4678 Acc: 0.8712\n",
            "Epoch  40 | Train Loss: 0.5929 Acc: 0.8173 | Val Loss: 0.4638 Acc: 0.8727\n",
            "Epoch  41 | Train Loss: 0.5858 Acc: 0.8175 | Val Loss: 0.4567 Acc: 0.8775\n",
            "Epoch  42 | Train Loss: 0.5747 Acc: 0.8234 | Val Loss: 0.4508 Acc: 0.8777\n",
            "Epoch  43 | Train Loss: 0.5746 Acc: 0.8225 | Val Loss: 0.4464 Acc: 0.8792\n",
            "Epoch  44 | Train Loss: 0.5653 Acc: 0.8265 | Val Loss: 0.4412 Acc: 0.8790\n",
            "Epoch  45 | Train Loss: 0.5610 Acc: 0.8255 | Val Loss: 0.4349 Acc: 0.8822\n",
            "Epoch  46 | Train Loss: 0.5555 Acc: 0.8298 | Val Loss: 0.4312 Acc: 0.8833\n",
            "Epoch  47 | Train Loss: 0.5487 Acc: 0.8315 | Val Loss: 0.4256 Acc: 0.8830\n",
            "Epoch  48 | Train Loss: 0.5438 Acc: 0.8323 | Val Loss: 0.4219 Acc: 0.8855\n",
            "Epoch  49 | Train Loss: 0.5432 Acc: 0.8326 | Val Loss: 0.4168 Acc: 0.8882\n",
            "Epoch  50 | Train Loss: 0.5328 Acc: 0.8361 | Val Loss: 0.4136 Acc: 0.8883\n",
            "Epoch  51 | Train Loss: 0.5279 Acc: 0.8385 | Val Loss: 0.4103 Acc: 0.8895\n",
            "Epoch  52 | Train Loss: 0.5232 Acc: 0.8386 | Val Loss: 0.4050 Acc: 0.8910\n",
            "Epoch  53 | Train Loss: 0.5161 Acc: 0.8418 | Val Loss: 0.3993 Acc: 0.8918\n",
            "Epoch  54 | Train Loss: 0.5167 Acc: 0.8410 | Val Loss: 0.3958 Acc: 0.8927\n",
            "Epoch  55 | Train Loss: 0.5112 Acc: 0.8429 | Val Loss: 0.3932 Acc: 0.8932\n",
            "Epoch  56 | Train Loss: 0.5045 Acc: 0.8454 | Val Loss: 0.3885 Acc: 0.8940\n",
            "Epoch  57 | Train Loss: 0.4991 Acc: 0.8477 | Val Loss: 0.3856 Acc: 0.8942\n",
            "Epoch  58 | Train Loss: 0.4917 Acc: 0.8506 | Val Loss: 0.3829 Acc: 0.8965\n",
            "Epoch  59 | Train Loss: 0.4907 Acc: 0.8494 | Val Loss: 0.3784 Acc: 0.8977\n",
            "Epoch  60 | Train Loss: 0.4875 Acc: 0.8507 | Val Loss: 0.3767 Acc: 0.8982\n",
            "Epoch  61 | Train Loss: 0.4905 Acc: 0.8503 | Val Loss: 0.3723 Acc: 0.8983\n",
            "Epoch  62 | Train Loss: 0.4770 Acc: 0.8540 | Val Loss: 0.3697 Acc: 0.9007\n",
            "Epoch  63 | Train Loss: 0.4757 Acc: 0.8543 | Val Loss: 0.3656 Acc: 0.9005\n",
            "Epoch  64 | Train Loss: 0.4714 Acc: 0.8555 | Val Loss: 0.3628 Acc: 0.9027\n",
            "Epoch  65 | Train Loss: 0.4670 Acc: 0.8588 | Val Loss: 0.3605 Acc: 0.9038\n",
            "Epoch  66 | Train Loss: 0.4636 Acc: 0.8583 | Val Loss: 0.3560 Acc: 0.9023\n",
            "Epoch  67 | Train Loss: 0.4630 Acc: 0.8575 | Val Loss: 0.3542 Acc: 0.9022\n",
            "Epoch  68 | Train Loss: 0.4600 Acc: 0.8594 | Val Loss: 0.3523 Acc: 0.9033\n",
            "Epoch  69 | Train Loss: 0.4541 Acc: 0.8614 | Val Loss: 0.3481 Acc: 0.9050\n",
            "Epoch  70 | Train Loss: 0.4486 Acc: 0.8636 | Val Loss: 0.3473 Acc: 0.9047\n",
            "Epoch  71 | Train Loss: 0.4494 Acc: 0.8623 | Val Loss: 0.3431 Acc: 0.9080\n",
            "Epoch  72 | Train Loss: 0.4440 Acc: 0.8656 | Val Loss: 0.3404 Acc: 0.9085\n",
            "Epoch  73 | Train Loss: 0.4394 Acc: 0.8661 | Val Loss: 0.3372 Acc: 0.9090\n",
            "Epoch  74 | Train Loss: 0.4374 Acc: 0.8664 | Val Loss: 0.3356 Acc: 0.9093\n",
            "Epoch  75 | Train Loss: 0.4331 Acc: 0.8676 | Val Loss: 0.3338 Acc: 0.9115\n",
            "Epoch  76 | Train Loss: 0.4276 Acc: 0.8716 | Val Loss: 0.3326 Acc: 0.9105\n",
            "Epoch  77 | Train Loss: 0.4256 Acc: 0.8716 | Val Loss: 0.3291 Acc: 0.9122\n",
            "Epoch  78 | Train Loss: 0.4267 Acc: 0.8700 | Val Loss: 0.3259 Acc: 0.9115\n",
            "Epoch  79 | Train Loss: 0.4182 Acc: 0.8728 | Val Loss: 0.3251 Acc: 0.9128\n",
            "Epoch  80 | Train Loss: 0.4196 Acc: 0.8722 | Val Loss: 0.3223 Acc: 0.9130\n",
            "Epoch  81 | Train Loss: 0.4171 Acc: 0.8731 | Val Loss: 0.3194 Acc: 0.9150\n",
            "Epoch  82 | Train Loss: 0.4114 Acc: 0.8766 | Val Loss: 0.3179 Acc: 0.9150\n",
            "Epoch  83 | Train Loss: 0.4071 Acc: 0.8773 | Val Loss: 0.3151 Acc: 0.9167\n",
            "Epoch  84 | Train Loss: 0.4024 Acc: 0.8777 | Val Loss: 0.3128 Acc: 0.9182\n",
            "Epoch  85 | Train Loss: 0.4060 Acc: 0.8771 | Val Loss: 0.3111 Acc: 0.9158\n",
            "Epoch  86 | Train Loss: 0.3998 Acc: 0.8798 | Val Loss: 0.3108 Acc: 0.9168\n",
            "Epoch  87 | Train Loss: 0.3985 Acc: 0.8793 | Val Loss: 0.3092 Acc: 0.9187\n",
            "Epoch  88 | Train Loss: 0.3969 Acc: 0.8812 | Val Loss: 0.3068 Acc: 0.9178\n",
            "Epoch  89 | Train Loss: 0.3967 Acc: 0.8799 | Val Loss: 0.3038 Acc: 0.9192\n",
            "Epoch  90 | Train Loss: 0.3917 Acc: 0.8810 | Val Loss: 0.3038 Acc: 0.9165\n",
            "Epoch  91 | Train Loss: 0.3872 Acc: 0.8840 | Val Loss: 0.3015 Acc: 0.9205\n",
            "Epoch  92 | Train Loss: 0.3858 Acc: 0.8826 | Val Loss: 0.2986 Acc: 0.9193\n",
            "Epoch  93 | Train Loss: 0.3811 Acc: 0.8857 | Val Loss: 0.2979 Acc: 0.9208\n",
            "Epoch  94 | Train Loss: 0.3787 Acc: 0.8866 | Val Loss: 0.2965 Acc: 0.9195\n",
            "Epoch  95 | Train Loss: 0.3768 Acc: 0.8880 | Val Loss: 0.2928 Acc: 0.9210\n",
            "Epoch  96 | Train Loss: 0.3748 Acc: 0.8873 | Val Loss: 0.2919 Acc: 0.9230\n",
            "Epoch  97 | Train Loss: 0.3716 Acc: 0.8886 | Val Loss: 0.2905 Acc: 0.9220\n",
            "Epoch  98 | Train Loss: 0.3714 Acc: 0.8870 | Val Loss: 0.2891 Acc: 0.9230\n",
            "Epoch  99 | Train Loss: 0.3644 Acc: 0.8927 | Val Loss: 0.2885 Acc: 0.9225\n",
            "Epoch 100 | Train Loss: 0.3652 Acc: 0.8918 | Val Loss: 0.2849 Acc: 0.9233\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_3.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2725 Acc: 0.1265 | Val Loss: 2.2105 Acc: 0.1800\n",
            "Epoch   2 | Train Loss: 2.1716 Acc: 0.2291 | Val Loss: 2.1112 Acc: 0.2915\n",
            "Epoch   3 | Train Loss: 2.0800 Acc: 0.3002 | Val Loss: 2.0179 Acc: 0.3495\n",
            "Epoch   4 | Train Loss: 1.9942 Acc: 0.3487 | Val Loss: 1.9282 Acc: 0.4147\n",
            "Epoch   5 | Train Loss: 1.9090 Acc: 0.4009 | Val Loss: 1.8382 Acc: 0.4785\n",
            "Epoch   6 | Train Loss: 1.8272 Acc: 0.4542 | Val Loss: 1.7472 Acc: 0.5383\n",
            "Epoch   7 | Train Loss: 1.7441 Acc: 0.4986 | Val Loss: 1.6536 Acc: 0.5810\n",
            "Epoch   8 | Train Loss: 1.6616 Acc: 0.5380 | Val Loss: 1.5641 Acc: 0.6245\n",
            "Epoch   9 | Train Loss: 1.5848 Acc: 0.5688 | Val Loss: 1.4910 Acc: 0.6580\n",
            "Epoch  10 | Train Loss: 1.5108 Acc: 0.5934 | Val Loss: 1.4132 Acc: 0.6787\n",
            "Epoch  11 | Train Loss: 1.4431 Acc: 0.6133 | Val Loss: 1.3379 Acc: 0.6985\n",
            "Epoch  12 | Train Loss: 1.3794 Acc: 0.6297 | Val Loss: 1.2788 Acc: 0.7165\n",
            "Epoch  13 | Train Loss: 1.3238 Acc: 0.6417 | Val Loss: 1.2132 Acc: 0.7225\n",
            "Epoch  14 | Train Loss: 1.2771 Acc: 0.6498 | Val Loss: 1.1602 Acc: 0.7318\n",
            "Epoch  15 | Train Loss: 1.2307 Acc: 0.6595 | Val Loss: 1.1074 Acc: 0.7428\n",
            "Epoch  16 | Train Loss: 1.1906 Acc: 0.6674 | Val Loss: 1.0640 Acc: 0.7533\n",
            "Epoch  17 | Train Loss: 1.1506 Acc: 0.6783 | Val Loss: 1.0226 Acc: 0.7608\n",
            "Epoch  18 | Train Loss: 1.1182 Acc: 0.6838 | Val Loss: 0.9836 Acc: 0.7673\n",
            "Epoch  19 | Train Loss: 1.0845 Acc: 0.6901 | Val Loss: 0.9528 Acc: 0.7722\n",
            "Epoch  20 | Train Loss: 1.0547 Acc: 0.6968 | Val Loss: 0.9244 Acc: 0.7760\n",
            "Epoch  21 | Train Loss: 1.0313 Acc: 0.7010 | Val Loss: 0.8951 Acc: 0.7815\n",
            "Epoch  22 | Train Loss: 1.0079 Acc: 0.7067 | Val Loss: 0.8667 Acc: 0.7858\n",
            "Epoch  23 | Train Loss: 0.9876 Acc: 0.7103 | Val Loss: 0.8456 Acc: 0.7908\n",
            "Epoch  24 | Train Loss: 0.9663 Acc: 0.7123 | Val Loss: 0.8207 Acc: 0.7960\n",
            "Epoch  25 | Train Loss: 0.9480 Acc: 0.7187 | Val Loss: 0.7993 Acc: 0.7992\n",
            "Epoch  26 | Train Loss: 0.9290 Acc: 0.7238 | Val Loss: 0.7788 Acc: 0.8025\n",
            "Epoch  27 | Train Loss: 0.9164 Acc: 0.7271 | Val Loss: 0.7643 Acc: 0.8043\n",
            "Epoch  28 | Train Loss: 0.8999 Acc: 0.7277 | Val Loss: 0.7467 Acc: 0.8093\n",
            "Epoch  29 | Train Loss: 0.8840 Acc: 0.7336 | Val Loss: 0.7315 Acc: 0.8123\n",
            "Epoch  30 | Train Loss: 0.8714 Acc: 0.7376 | Val Loss: 0.7195 Acc: 0.8150\n",
            "Epoch  31 | Train Loss: 0.8569 Acc: 0.7417 | Val Loss: 0.7022 Acc: 0.8180\n",
            "Epoch  32 | Train Loss: 0.8473 Acc: 0.7419 | Val Loss: 0.6910 Acc: 0.8218\n",
            "Epoch  33 | Train Loss: 0.8399 Acc: 0.7422 | Val Loss: 0.6797 Acc: 0.8247\n",
            "Epoch  34 | Train Loss: 0.8259 Acc: 0.7478 | Val Loss: 0.6680 Acc: 0.8267\n",
            "Epoch  35 | Train Loss: 0.8175 Acc: 0.7508 | Val Loss: 0.6563 Acc: 0.8305\n",
            "Epoch  36 | Train Loss: 0.8054 Acc: 0.7548 | Val Loss: 0.6466 Acc: 0.8318\n",
            "Epoch  37 | Train Loss: 0.8006 Acc: 0.7554 | Val Loss: 0.6389 Acc: 0.8332\n",
            "Epoch  38 | Train Loss: 0.7903 Acc: 0.7560 | Val Loss: 0.6288 Acc: 0.8355\n",
            "Epoch  39 | Train Loss: 0.7822 Acc: 0.7587 | Val Loss: 0.6217 Acc: 0.8370\n",
            "Epoch  40 | Train Loss: 0.7722 Acc: 0.7632 | Val Loss: 0.6108 Acc: 0.8407\n",
            "Epoch  41 | Train Loss: 0.7684 Acc: 0.7632 | Val Loss: 0.6028 Acc: 0.8412\n",
            "Epoch  42 | Train Loss: 0.7560 Acc: 0.7670 | Val Loss: 0.5950 Acc: 0.8425\n",
            "Epoch  43 | Train Loss: 0.7513 Acc: 0.7684 | Val Loss: 0.5916 Acc: 0.8433\n",
            "Epoch  44 | Train Loss: 0.7446 Acc: 0.7697 | Val Loss: 0.5836 Acc: 0.8445\n",
            "Epoch  45 | Train Loss: 0.7394 Acc: 0.7716 | Val Loss: 0.5773 Acc: 0.8458\n",
            "Epoch  46 | Train Loss: 0.7328 Acc: 0.7728 | Val Loss: 0.5716 Acc: 0.8465\n",
            "Epoch  47 | Train Loss: 0.7248 Acc: 0.7766 | Val Loss: 0.5624 Acc: 0.8500\n",
            "Epoch  48 | Train Loss: 0.7203 Acc: 0.7770 | Val Loss: 0.5582 Acc: 0.8497\n",
            "Epoch  49 | Train Loss: 0.7112 Acc: 0.7790 | Val Loss: 0.5531 Acc: 0.8510\n",
            "Epoch  50 | Train Loss: 0.7098 Acc: 0.7797 | Val Loss: 0.5455 Acc: 0.8513\n",
            "Epoch  51 | Train Loss: 0.7024 Acc: 0.7826 | Val Loss: 0.5415 Acc: 0.8543\n",
            "Epoch  52 | Train Loss: 0.7009 Acc: 0.7847 | Val Loss: 0.5381 Acc: 0.8562\n",
            "Epoch  53 | Train Loss: 0.6904 Acc: 0.7849 | Val Loss: 0.5339 Acc: 0.8543\n",
            "Epoch  54 | Train Loss: 0.6891 Acc: 0.7856 | Val Loss: 0.5269 Acc: 0.8570\n",
            "Epoch  55 | Train Loss: 0.6828 Acc: 0.7893 | Val Loss: 0.5227 Acc: 0.8588\n",
            "Epoch  56 | Train Loss: 0.6796 Acc: 0.7899 | Val Loss: 0.5173 Acc: 0.8583\n",
            "Epoch  57 | Train Loss: 0.6751 Acc: 0.7913 | Val Loss: 0.5135 Acc: 0.8615\n",
            "Epoch  58 | Train Loss: 0.6696 Acc: 0.7936 | Val Loss: 0.5084 Acc: 0.8615\n",
            "Epoch  59 | Train Loss: 0.6660 Acc: 0.7937 | Val Loss: 0.5057 Acc: 0.8643\n",
            "Epoch  60 | Train Loss: 0.6633 Acc: 0.7947 | Val Loss: 0.5018 Acc: 0.8635\n",
            "Epoch  61 | Train Loss: 0.6580 Acc: 0.7943 | Val Loss: 0.4998 Acc: 0.8653\n",
            "Epoch  62 | Train Loss: 0.6555 Acc: 0.7957 | Val Loss: 0.4967 Acc: 0.8657\n",
            "Epoch  63 | Train Loss: 0.6526 Acc: 0.7973 | Val Loss: 0.4891 Acc: 0.8673\n",
            "Epoch  64 | Train Loss: 0.6476 Acc: 0.7988 | Val Loss: 0.4869 Acc: 0.8683\n",
            "Epoch  65 | Train Loss: 0.6416 Acc: 0.8015 | Val Loss: 0.4856 Acc: 0.8678\n",
            "Epoch  66 | Train Loss: 0.6384 Acc: 0.8041 | Val Loss: 0.4787 Acc: 0.8697\n",
            "Epoch  67 | Train Loss: 0.6383 Acc: 0.8032 | Val Loss: 0.4764 Acc: 0.8705\n",
            "Epoch  68 | Train Loss: 0.6335 Acc: 0.8045 | Val Loss: 0.4740 Acc: 0.8722\n",
            "Epoch  69 | Train Loss: 0.6292 Acc: 0.8041 | Val Loss: 0.4734 Acc: 0.8710\n",
            "Epoch  70 | Train Loss: 0.6225 Acc: 0.8079 | Val Loss: 0.4666 Acc: 0.8733\n",
            "Epoch  71 | Train Loss: 0.6202 Acc: 0.8080 | Val Loss: 0.4643 Acc: 0.8735\n",
            "Epoch  72 | Train Loss: 0.6184 Acc: 0.8073 | Val Loss: 0.4616 Acc: 0.8740\n",
            "Epoch  73 | Train Loss: 0.6130 Acc: 0.8104 | Val Loss: 0.4577 Acc: 0.8745\n",
            "Epoch  74 | Train Loss: 0.6130 Acc: 0.8117 | Val Loss: 0.4563 Acc: 0.8745\n",
            "Epoch  75 | Train Loss: 0.6106 Acc: 0.8111 | Val Loss: 0.4534 Acc: 0.8758\n",
            "Epoch  76 | Train Loss: 0.6078 Acc: 0.8113 | Val Loss: 0.4509 Acc: 0.8760\n",
            "Epoch  77 | Train Loss: 0.6041 Acc: 0.8127 | Val Loss: 0.4493 Acc: 0.8767\n",
            "Epoch  78 | Train Loss: 0.6024 Acc: 0.8130 | Val Loss: 0.4454 Acc: 0.8775\n",
            "Epoch  79 | Train Loss: 0.5974 Acc: 0.8142 | Val Loss: 0.4419 Acc: 0.8780\n",
            "Epoch  80 | Train Loss: 0.5949 Acc: 0.8155 | Val Loss: 0.4406 Acc: 0.8785\n",
            "Epoch  81 | Train Loss: 0.5902 Acc: 0.8173 | Val Loss: 0.4412 Acc: 0.8777\n",
            "Epoch  82 | Train Loss: 0.5898 Acc: 0.8186 | Val Loss: 0.4358 Acc: 0.8793\n",
            "Epoch  83 | Train Loss: 0.5879 Acc: 0.8184 | Val Loss: 0.4353 Acc: 0.8785\n",
            "Epoch  84 | Train Loss: 0.5801 Acc: 0.8204 | Val Loss: 0.4321 Acc: 0.8803\n",
            "Epoch  85 | Train Loss: 0.5861 Acc: 0.8183 | Val Loss: 0.4297 Acc: 0.8793\n",
            "Epoch  86 | Train Loss: 0.5774 Acc: 0.8213 | Val Loss: 0.4263 Acc: 0.8818\n",
            "Epoch  87 | Train Loss: 0.5823 Acc: 0.8190 | Val Loss: 0.4262 Acc: 0.8833\n",
            "Epoch  88 | Train Loss: 0.5750 Acc: 0.8218 | Val Loss: 0.4246 Acc: 0.8837\n",
            "Epoch  89 | Train Loss: 0.5745 Acc: 0.8222 | Val Loss: 0.4201 Acc: 0.8835\n",
            "Epoch  90 | Train Loss: 0.5669 Acc: 0.8241 | Val Loss: 0.4171 Acc: 0.8832\n",
            "Epoch  91 | Train Loss: 0.5675 Acc: 0.8247 | Val Loss: 0.4168 Acc: 0.8847\n",
            "Epoch  92 | Train Loss: 0.5661 Acc: 0.8246 | Val Loss: 0.4141 Acc: 0.8843\n",
            "Epoch  93 | Train Loss: 0.5632 Acc: 0.8248 | Val Loss: 0.4117 Acc: 0.8852\n",
            "Epoch  94 | Train Loss: 0.5607 Acc: 0.8255 | Val Loss: 0.4107 Acc: 0.8848\n",
            "Epoch  95 | Train Loss: 0.5556 Acc: 0.8289 | Val Loss: 0.4085 Acc: 0.8870\n",
            "Epoch  96 | Train Loss: 0.5560 Acc: 0.8285 | Val Loss: 0.4067 Acc: 0.8890\n",
            "Epoch  97 | Train Loss: 0.5528 Acc: 0.8290 | Val Loss: 0.4060 Acc: 0.8862\n",
            "Epoch  98 | Train Loss: 0.5537 Acc: 0.8295 | Val Loss: 0.4038 Acc: 0.8875\n",
            "Epoch  99 | Train Loss: 0.5477 Acc: 0.8301 | Val Loss: 0.4041 Acc: 0.8865\n",
            "Epoch 100 | Train Loss: 0.5486 Acc: 0.8296 | Val Loss: 0.4019 Acc: 0.8872\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_4.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2595 Acc: 0.1287 | Val Loss: 2.1856 Acc: 0.2090\n",
            "Epoch   2 | Train Loss: 2.1385 Acc: 0.2539 | Val Loss: 2.0672 Acc: 0.3473\n",
            "Epoch   3 | Train Loss: 2.0280 Acc: 0.3515 | Val Loss: 1.9557 Acc: 0.4333\n",
            "Epoch   4 | Train Loss: 1.9253 Acc: 0.4078 | Val Loss: 1.8491 Acc: 0.4767\n",
            "Epoch   5 | Train Loss: 1.8279 Acc: 0.4478 | Val Loss: 1.7399 Acc: 0.5202\n",
            "Epoch   6 | Train Loss: 1.7387 Acc: 0.4943 | Val Loss: 1.6504 Acc: 0.5670\n",
            "Epoch   7 | Train Loss: 1.6514 Acc: 0.5285 | Val Loss: 1.5503 Acc: 0.6060\n",
            "Epoch   8 | Train Loss: 1.5725 Acc: 0.5568 | Val Loss: 1.4702 Acc: 0.6335\n",
            "Epoch   9 | Train Loss: 1.4992 Acc: 0.5789 | Val Loss: 1.3956 Acc: 0.6557\n",
            "Epoch  10 | Train Loss: 1.4315 Acc: 0.5935 | Val Loss: 1.3196 Acc: 0.6703\n",
            "Epoch  11 | Train Loss: 1.3723 Acc: 0.6064 | Val Loss: 1.2583 Acc: 0.6827\n",
            "Epoch  12 | Train Loss: 1.3194 Acc: 0.6173 | Val Loss: 1.1991 Acc: 0.6970\n",
            "Epoch  13 | Train Loss: 1.2687 Acc: 0.6287 | Val Loss: 1.1437 Acc: 0.7057\n",
            "Epoch  14 | Train Loss: 1.2253 Acc: 0.6402 | Val Loss: 1.1004 Acc: 0.7162\n",
            "Epoch  15 | Train Loss: 1.1837 Acc: 0.6507 | Val Loss: 1.0534 Acc: 0.7273\n",
            "Epoch  16 | Train Loss: 1.1569 Acc: 0.6537 | Val Loss: 1.0143 Acc: 0.7367\n",
            "Epoch  17 | Train Loss: 1.1228 Acc: 0.6619 | Val Loss: 0.9824 Acc: 0.7445\n",
            "Epoch  18 | Train Loss: 1.0942 Acc: 0.6715 | Val Loss: 0.9521 Acc: 0.7548\n",
            "Epoch  19 | Train Loss: 1.0708 Acc: 0.6756 | Val Loss: 0.9249 Acc: 0.7605\n",
            "Epoch  20 | Train Loss: 1.0500 Acc: 0.6814 | Val Loss: 0.8986 Acc: 0.7672\n",
            "Epoch  21 | Train Loss: 1.0281 Acc: 0.6863 | Val Loss: 0.8757 Acc: 0.7737\n",
            "Epoch  22 | Train Loss: 1.0118 Acc: 0.6932 | Val Loss: 0.8556 Acc: 0.7773\n",
            "Epoch  23 | Train Loss: 0.9922 Acc: 0.6988 | Val Loss: 0.8358 Acc: 0.7807\n",
            "Epoch  24 | Train Loss: 0.9794 Acc: 0.6997 | Val Loss: 0.8172 Acc: 0.7838\n",
            "Epoch  25 | Train Loss: 0.9658 Acc: 0.7045 | Val Loss: 0.8027 Acc: 0.7885\n",
            "Epoch  26 | Train Loss: 0.9531 Acc: 0.7060 | Val Loss: 0.7850 Acc: 0.7897\n",
            "Epoch  27 | Train Loss: 0.9362 Acc: 0.7126 | Val Loss: 0.7716 Acc: 0.7938\n",
            "Epoch  28 | Train Loss: 0.9282 Acc: 0.7137 | Val Loss: 0.7589 Acc: 0.7958\n",
            "Epoch  29 | Train Loss: 0.9130 Acc: 0.7180 | Val Loss: 0.7457 Acc: 0.7992\n",
            "Epoch  30 | Train Loss: 0.9090 Acc: 0.7179 | Val Loss: 0.7308 Acc: 0.8013\n",
            "Epoch  31 | Train Loss: 0.8936 Acc: 0.7236 | Val Loss: 0.7233 Acc: 0.8025\n",
            "Epoch  32 | Train Loss: 0.8851 Acc: 0.7255 | Val Loss: 0.7126 Acc: 0.8038\n",
            "Epoch  33 | Train Loss: 0.8730 Acc: 0.7291 | Val Loss: 0.7051 Acc: 0.8092\n",
            "Epoch  34 | Train Loss: 0.8664 Acc: 0.7283 | Val Loss: 0.6941 Acc: 0.8093\n",
            "Epoch  35 | Train Loss: 0.8605 Acc: 0.7304 | Val Loss: 0.6853 Acc: 0.8117\n",
            "Epoch  36 | Train Loss: 0.8507 Acc: 0.7340 | Val Loss: 0.6744 Acc: 0.8148\n",
            "Epoch  37 | Train Loss: 0.8468 Acc: 0.7352 | Val Loss: 0.6670 Acc: 0.8148\n",
            "Epoch  38 | Train Loss: 0.8396 Acc: 0.7368 | Val Loss: 0.6582 Acc: 0.8173\n",
            "Epoch  39 | Train Loss: 0.8303 Acc: 0.7397 | Val Loss: 0.6517 Acc: 0.8188\n",
            "Epoch  40 | Train Loss: 0.8255 Acc: 0.7408 | Val Loss: 0.6457 Acc: 0.8203\n",
            "Epoch  41 | Train Loss: 0.8174 Acc: 0.7421 | Val Loss: 0.6390 Acc: 0.8217\n",
            "Epoch  42 | Train Loss: 0.8144 Acc: 0.7436 | Val Loss: 0.6306 Acc: 0.8218\n",
            "Epoch  43 | Train Loss: 0.8095 Acc: 0.7452 | Val Loss: 0.6261 Acc: 0.8240\n",
            "Epoch  44 | Train Loss: 0.7971 Acc: 0.7503 | Val Loss: 0.6183 Acc: 0.8272\n",
            "Epoch  45 | Train Loss: 0.7953 Acc: 0.7509 | Val Loss: 0.6101 Acc: 0.8288\n",
            "Epoch  46 | Train Loss: 0.7884 Acc: 0.7542 | Val Loss: 0.6052 Acc: 0.8305\n",
            "Epoch  47 | Train Loss: 0.7833 Acc: 0.7527 | Val Loss: 0.6008 Acc: 0.8293\n",
            "Epoch  48 | Train Loss: 0.7763 Acc: 0.7559 | Val Loss: 0.5971 Acc: 0.8307\n",
            "Epoch  49 | Train Loss: 0.7696 Acc: 0.7610 | Val Loss: 0.5894 Acc: 0.8327\n",
            "Epoch  50 | Train Loss: 0.7703 Acc: 0.7584 | Val Loss: 0.5868 Acc: 0.8358\n",
            "Epoch  51 | Train Loss: 0.7635 Acc: 0.7611 | Val Loss: 0.5790 Acc: 0.8375\n",
            "Epoch  52 | Train Loss: 0.7544 Acc: 0.7630 | Val Loss: 0.5736 Acc: 0.8355\n",
            "Epoch  53 | Train Loss: 0.7547 Acc: 0.7616 | Val Loss: 0.5698 Acc: 0.8377\n",
            "Epoch  54 | Train Loss: 0.7477 Acc: 0.7643 | Val Loss: 0.5651 Acc: 0.8395\n",
            "Epoch  55 | Train Loss: 0.7455 Acc: 0.7662 | Val Loss: 0.5613 Acc: 0.8395\n",
            "Epoch  56 | Train Loss: 0.7389 Acc: 0.7691 | Val Loss: 0.5573 Acc: 0.8422\n",
            "Epoch  57 | Train Loss: 0.7366 Acc: 0.7685 | Val Loss: 0.5536 Acc: 0.8422\n",
            "Epoch  58 | Train Loss: 0.7341 Acc: 0.7694 | Val Loss: 0.5485 Acc: 0.8448\n",
            "Epoch  59 | Train Loss: 0.7262 Acc: 0.7723 | Val Loss: 0.5440 Acc: 0.8437\n",
            "Epoch  60 | Train Loss: 0.7252 Acc: 0.7745 | Val Loss: 0.5411 Acc: 0.8483\n",
            "Epoch  61 | Train Loss: 0.7181 Acc: 0.7766 | Val Loss: 0.5347 Acc: 0.8487\n",
            "Epoch  62 | Train Loss: 0.7196 Acc: 0.7753 | Val Loss: 0.5355 Acc: 0.8478\n",
            "Epoch  63 | Train Loss: 0.7176 Acc: 0.7754 | Val Loss: 0.5292 Acc: 0.8510\n",
            "Epoch  64 | Train Loss: 0.7093 Acc: 0.7792 | Val Loss: 0.5268 Acc: 0.8515\n",
            "Epoch  65 | Train Loss: 0.7045 Acc: 0.7792 | Val Loss: 0.5239 Acc: 0.8522\n",
            "Epoch  66 | Train Loss: 0.7037 Acc: 0.7789 | Val Loss: 0.5197 Acc: 0.8547\n",
            "Epoch  67 | Train Loss: 0.6976 Acc: 0.7805 | Val Loss: 0.5150 Acc: 0.8528\n",
            "Epoch  68 | Train Loss: 0.6938 Acc: 0.7823 | Val Loss: 0.5134 Acc: 0.8553\n",
            "Epoch  69 | Train Loss: 0.6955 Acc: 0.7822 | Val Loss: 0.5108 Acc: 0.8557\n",
            "Epoch  70 | Train Loss: 0.6890 Acc: 0.7849 | Val Loss: 0.5078 Acc: 0.8580\n",
            "Epoch  71 | Train Loss: 0.6911 Acc: 0.7833 | Val Loss: 0.5040 Acc: 0.8592\n",
            "Epoch  72 | Train Loss: 0.6868 Acc: 0.7848 | Val Loss: 0.5010 Acc: 0.8575\n",
            "Epoch  73 | Train Loss: 0.6801 Acc: 0.7866 | Val Loss: 0.4977 Acc: 0.8585\n",
            "Epoch  74 | Train Loss: 0.6747 Acc: 0.7876 | Val Loss: 0.4937 Acc: 0.8618\n",
            "Epoch  75 | Train Loss: 0.6748 Acc: 0.7897 | Val Loss: 0.4916 Acc: 0.8608\n",
            "Epoch  76 | Train Loss: 0.6728 Acc: 0.7893 | Val Loss: 0.4889 Acc: 0.8612\n",
            "Epoch  77 | Train Loss: 0.6706 Acc: 0.7902 | Val Loss: 0.4883 Acc: 0.8615\n",
            "Epoch  78 | Train Loss: 0.6678 Acc: 0.7913 | Val Loss: 0.4831 Acc: 0.8647\n",
            "Epoch  79 | Train Loss: 0.6674 Acc: 0.7907 | Val Loss: 0.4829 Acc: 0.8645\n",
            "Epoch  80 | Train Loss: 0.6637 Acc: 0.7928 | Val Loss: 0.4774 Acc: 0.8655\n",
            "Epoch  81 | Train Loss: 0.6607 Acc: 0.7938 | Val Loss: 0.4769 Acc: 0.8663\n",
            "Epoch  82 | Train Loss: 0.6583 Acc: 0.7939 | Val Loss: 0.4739 Acc: 0.8665\n",
            "Epoch  83 | Train Loss: 0.6552 Acc: 0.7946 | Val Loss: 0.4723 Acc: 0.8657\n",
            "Epoch  84 | Train Loss: 0.6539 Acc: 0.7932 | Val Loss: 0.4706 Acc: 0.8680\n",
            "Epoch  85 | Train Loss: 0.6501 Acc: 0.7966 | Val Loss: 0.4669 Acc: 0.8697\n",
            "Epoch  86 | Train Loss: 0.6482 Acc: 0.7974 | Val Loss: 0.4642 Acc: 0.8700\n",
            "Epoch  87 | Train Loss: 0.6434 Acc: 0.7983 | Val Loss: 0.4638 Acc: 0.8700\n",
            "Epoch  88 | Train Loss: 0.6493 Acc: 0.7963 | Val Loss: 0.4620 Acc: 0.8697\n",
            "Epoch  89 | Train Loss: 0.6412 Acc: 0.7995 | Val Loss: 0.4584 Acc: 0.8725\n",
            "Epoch  90 | Train Loss: 0.6412 Acc: 0.7986 | Val Loss: 0.4560 Acc: 0.8725\n",
            "Epoch  91 | Train Loss: 0.6387 Acc: 0.7985 | Val Loss: 0.4543 Acc: 0.8722\n",
            "Epoch  92 | Train Loss: 0.6400 Acc: 0.7988 | Val Loss: 0.4518 Acc: 0.8752\n",
            "Epoch  93 | Train Loss: 0.6302 Acc: 0.8019 | Val Loss: 0.4509 Acc: 0.8745\n",
            "Epoch  94 | Train Loss: 0.6368 Acc: 0.8000 | Val Loss: 0.4473 Acc: 0.8747\n",
            "Epoch  95 | Train Loss: 0.6292 Acc: 0.8020 | Val Loss: 0.4462 Acc: 0.8752\n",
            "Epoch  96 | Train Loss: 0.6277 Acc: 0.8020 | Val Loss: 0.4436 Acc: 0.8772\n",
            "Epoch  97 | Train Loss: 0.6214 Acc: 0.8051 | Val Loss: 0.4432 Acc: 0.8742\n",
            "Epoch  98 | Train Loss: 0.6264 Acc: 0.8027 | Val Loss: 0.4409 Acc: 0.8762\n",
            "Epoch  99 | Train Loss: 0.6170 Acc: 0.8063 | Val Loss: 0.4391 Acc: 0.8762\n",
            "Epoch 100 | Train Loss: 0.6248 Acc: 0.8037 | Val Loss: 0.4377 Acc: 0.8763\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_5.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.3043 Acc: 0.1324 | Val Loss: 2.2493 Acc: 0.2108\n",
            "Epoch   2 | Train Loss: 2.2264 Acc: 0.2088 | Val Loss: 2.1717 Acc: 0.2623\n",
            "Epoch   3 | Train Loss: 2.1542 Acc: 0.2478 | Val Loss: 2.0968 Acc: 0.3157\n",
            "Epoch   4 | Train Loss: 2.0863 Acc: 0.2945 | Val Loss: 2.0255 Acc: 0.3618\n",
            "Epoch   5 | Train Loss: 2.0173 Acc: 0.3318 | Val Loss: 1.9486 Acc: 0.3882\n",
            "Epoch   6 | Train Loss: 1.9461 Acc: 0.3578 | Val Loss: 1.8739 Acc: 0.4255\n",
            "Epoch   7 | Train Loss: 1.8718 Acc: 0.3869 | Val Loss: 1.7943 Acc: 0.4590\n",
            "Epoch   8 | Train Loss: 1.8001 Acc: 0.4111 | Val Loss: 1.7138 Acc: 0.4983\n",
            "Epoch   9 | Train Loss: 1.7273 Acc: 0.4541 | Val Loss: 1.6338 Acc: 0.5522\n",
            "Epoch  10 | Train Loss: 1.6543 Acc: 0.4965 | Val Loss: 1.5542 Acc: 0.5952\n",
            "Epoch  11 | Train Loss: 1.5863 Acc: 0.5238 | Val Loss: 1.4819 Acc: 0.6302\n",
            "Epoch  12 | Train Loss: 1.5192 Acc: 0.5448 | Val Loss: 1.4092 Acc: 0.6448\n",
            "Epoch  13 | Train Loss: 1.4584 Acc: 0.5614 | Val Loss: 1.3351 Acc: 0.6642\n",
            "Epoch  14 | Train Loss: 1.4049 Acc: 0.5707 | Val Loss: 1.2737 Acc: 0.6768\n",
            "Epoch  15 | Train Loss: 1.3523 Acc: 0.5821 | Val Loss: 1.2192 Acc: 0.6868\n",
            "Epoch  16 | Train Loss: 1.3117 Acc: 0.5909 | Val Loss: 1.1688 Acc: 0.6995\n",
            "Epoch  17 | Train Loss: 1.2780 Acc: 0.5983 | Val Loss: 1.1257 Acc: 0.7100\n",
            "Epoch  18 | Train Loss: 1.2405 Acc: 0.6090 | Val Loss: 1.0927 Acc: 0.7185\n",
            "Epoch  19 | Train Loss: 1.2148 Acc: 0.6177 | Val Loss: 1.0517 Acc: 0.7263\n",
            "Epoch  20 | Train Loss: 1.1896 Acc: 0.6257 | Val Loss: 1.0223 Acc: 0.7390\n",
            "Epoch  21 | Train Loss: 1.1652 Acc: 0.6327 | Val Loss: 0.9955 Acc: 0.7493\n",
            "Epoch  22 | Train Loss: 1.1415 Acc: 0.6436 | Val Loss: 0.9644 Acc: 0.7563\n",
            "Epoch  23 | Train Loss: 1.1239 Acc: 0.6458 | Val Loss: 0.9420 Acc: 0.7645\n",
            "Epoch  24 | Train Loss: 1.1004 Acc: 0.6539 | Val Loss: 0.9159 Acc: 0.7703\n",
            "Epoch  25 | Train Loss: 1.0826 Acc: 0.6595 | Val Loss: 0.8956 Acc: 0.7708\n",
            "Epoch  26 | Train Loss: 1.0688 Acc: 0.6649 | Val Loss: 0.8760 Acc: 0.7792\n",
            "Epoch  27 | Train Loss: 1.0507 Acc: 0.6689 | Val Loss: 0.8588 Acc: 0.7822\n",
            "Epoch  28 | Train Loss: 1.0375 Acc: 0.6716 | Val Loss: 0.8387 Acc: 0.7862\n",
            "Epoch  29 | Train Loss: 1.0224 Acc: 0.6784 | Val Loss: 0.8243 Acc: 0.7873\n",
            "Epoch  30 | Train Loss: 1.0093 Acc: 0.6809 | Val Loss: 0.8060 Acc: 0.7942\n",
            "Epoch  31 | Train Loss: 0.9981 Acc: 0.6851 | Val Loss: 0.7990 Acc: 0.7955\n",
            "Epoch  32 | Train Loss: 0.9850 Acc: 0.6877 | Val Loss: 0.7807 Acc: 0.7987\n",
            "Epoch  33 | Train Loss: 0.9734 Acc: 0.6925 | Val Loss: 0.7653 Acc: 0.8002\n",
            "Epoch  34 | Train Loss: 0.9660 Acc: 0.6930 | Val Loss: 0.7545 Acc: 0.8050\n",
            "Epoch  35 | Train Loss: 0.9524 Acc: 0.6966 | Val Loss: 0.7449 Acc: 0.8053\n",
            "Epoch  36 | Train Loss: 0.9447 Acc: 0.6994 | Val Loss: 0.7333 Acc: 0.8097\n",
            "Epoch  37 | Train Loss: 0.9387 Acc: 0.6999 | Val Loss: 0.7202 Acc: 0.8097\n",
            "Epoch  38 | Train Loss: 0.9228 Acc: 0.7061 | Val Loss: 0.7098 Acc: 0.8135\n",
            "Epoch  39 | Train Loss: 0.9192 Acc: 0.7078 | Val Loss: 0.7021 Acc: 0.8167\n",
            "Epoch  40 | Train Loss: 0.9055 Acc: 0.7101 | Val Loss: 0.6911 Acc: 0.8187\n",
            "Epoch  41 | Train Loss: 0.9023 Acc: 0.7109 | Val Loss: 0.6829 Acc: 0.8198\n",
            "Epoch  42 | Train Loss: 0.8915 Acc: 0.7162 | Val Loss: 0.6754 Acc: 0.8213\n",
            "Epoch  43 | Train Loss: 0.8846 Acc: 0.7179 | Val Loss: 0.6673 Acc: 0.8248\n",
            "Epoch  44 | Train Loss: 0.8795 Acc: 0.7188 | Val Loss: 0.6579 Acc: 0.8267\n",
            "Epoch  45 | Train Loss: 0.8719 Acc: 0.7201 | Val Loss: 0.6543 Acc: 0.8295\n",
            "Epoch  46 | Train Loss: 0.8663 Acc: 0.7210 | Val Loss: 0.6442 Acc: 0.8297\n",
            "Epoch  47 | Train Loss: 0.8614 Acc: 0.7233 | Val Loss: 0.6388 Acc: 0.8303\n",
            "Epoch  48 | Train Loss: 0.8550 Acc: 0.7268 | Val Loss: 0.6319 Acc: 0.8305\n",
            "Epoch  49 | Train Loss: 0.8543 Acc: 0.7262 | Val Loss: 0.6272 Acc: 0.8347\n",
            "Epoch  50 | Train Loss: 0.8409 Acc: 0.7307 | Val Loss: 0.6226 Acc: 0.8327\n",
            "Epoch  51 | Train Loss: 0.8395 Acc: 0.7316 | Val Loss: 0.6146 Acc: 0.8363\n",
            "Epoch  52 | Train Loss: 0.8332 Acc: 0.7326 | Val Loss: 0.6098 Acc: 0.8378\n",
            "Epoch  53 | Train Loss: 0.8304 Acc: 0.7331 | Val Loss: 0.6029 Acc: 0.8377\n",
            "Epoch  54 | Train Loss: 0.8268 Acc: 0.7352 | Val Loss: 0.5992 Acc: 0.8388\n",
            "Epoch  55 | Train Loss: 0.8188 Acc: 0.7350 | Val Loss: 0.5933 Acc: 0.8398\n",
            "Epoch  56 | Train Loss: 0.8116 Acc: 0.7381 | Val Loss: 0.5870 Acc: 0.8398\n",
            "Epoch  57 | Train Loss: 0.8115 Acc: 0.7379 | Val Loss: 0.5838 Acc: 0.8418\n",
            "Epoch  58 | Train Loss: 0.8063 Acc: 0.7397 | Val Loss: 0.5813 Acc: 0.8430\n",
            "Epoch  59 | Train Loss: 0.8056 Acc: 0.7387 | Val Loss: 0.5747 Acc: 0.8423\n",
            "Epoch  60 | Train Loss: 0.7976 Acc: 0.7442 | Val Loss: 0.5717 Acc: 0.8435\n",
            "Epoch  61 | Train Loss: 0.7975 Acc: 0.7416 | Val Loss: 0.5655 Acc: 0.8450\n",
            "Epoch  62 | Train Loss: 0.7880 Acc: 0.7438 | Val Loss: 0.5633 Acc: 0.8450\n",
            "Epoch  63 | Train Loss: 0.7837 Acc: 0.7464 | Val Loss: 0.5573 Acc: 0.8467\n",
            "Epoch  64 | Train Loss: 0.7788 Acc: 0.7481 | Val Loss: 0.5531 Acc: 0.8477\n",
            "Epoch  65 | Train Loss: 0.7855 Acc: 0.7466 | Val Loss: 0.5498 Acc: 0.8493\n",
            "Epoch  66 | Train Loss: 0.7757 Acc: 0.7483 | Val Loss: 0.5476 Acc: 0.8497\n",
            "Epoch  67 | Train Loss: 0.7687 Acc: 0.7492 | Val Loss: 0.5459 Acc: 0.8488\n",
            "Epoch  68 | Train Loss: 0.7653 Acc: 0.7506 | Val Loss: 0.5403 Acc: 0.8513\n",
            "Epoch  69 | Train Loss: 0.7694 Acc: 0.7495 | Val Loss: 0.5344 Acc: 0.8518\n",
            "Epoch  70 | Train Loss: 0.7687 Acc: 0.7511 | Val Loss: 0.5327 Acc: 0.8520\n",
            "Epoch  71 | Train Loss: 0.7578 Acc: 0.7551 | Val Loss: 0.5293 Acc: 0.8528\n",
            "Epoch  72 | Train Loss: 0.7548 Acc: 0.7552 | Val Loss: 0.5278 Acc: 0.8537\n",
            "Epoch  73 | Train Loss: 0.7558 Acc: 0.7538 | Val Loss: 0.5251 Acc: 0.8535\n",
            "Epoch  74 | Train Loss: 0.7481 Acc: 0.7586 | Val Loss: 0.5202 Acc: 0.8558\n",
            "Epoch  75 | Train Loss: 0.7434 Acc: 0.7585 | Val Loss: 0.5172 Acc: 0.8560\n",
            "Epoch  76 | Train Loss: 0.7485 Acc: 0.7603 | Val Loss: 0.5152 Acc: 0.8563\n",
            "Epoch  77 | Train Loss: 0.7448 Acc: 0.7600 | Val Loss: 0.5130 Acc: 0.8560\n",
            "Epoch  78 | Train Loss: 0.7390 Acc: 0.7606 | Val Loss: 0.5095 Acc: 0.8582\n",
            "Epoch  79 | Train Loss: 0.7350 Acc: 0.7636 | Val Loss: 0.5058 Acc: 0.8585\n",
            "Epoch  80 | Train Loss: 0.7357 Acc: 0.7628 | Val Loss: 0.5009 Acc: 0.8615\n",
            "Epoch  81 | Train Loss: 0.7292 Acc: 0.7631 | Val Loss: 0.4998 Acc: 0.8602\n",
            "Epoch  82 | Train Loss: 0.7253 Acc: 0.7675 | Val Loss: 0.4989 Acc: 0.8612\n",
            "Epoch  83 | Train Loss: 0.7265 Acc: 0.7637 | Val Loss: 0.4963 Acc: 0.8613\n",
            "Epoch  84 | Train Loss: 0.7228 Acc: 0.7674 | Val Loss: 0.4943 Acc: 0.8617\n",
            "Epoch  85 | Train Loss: 0.7221 Acc: 0.7665 | Val Loss: 0.4911 Acc: 0.8625\n",
            "Epoch  86 | Train Loss: 0.7217 Acc: 0.7676 | Val Loss: 0.4886 Acc: 0.8630\n",
            "Epoch  87 | Train Loss: 0.7197 Acc: 0.7679 | Val Loss: 0.4855 Acc: 0.8643\n",
            "Epoch  88 | Train Loss: 0.7183 Acc: 0.7692 | Val Loss: 0.4842 Acc: 0.8653\n",
            "Epoch  89 | Train Loss: 0.7092 Acc: 0.7705 | Val Loss: 0.4831 Acc: 0.8653\n",
            "Epoch  90 | Train Loss: 0.7135 Acc: 0.7710 | Val Loss: 0.4813 Acc: 0.8675\n",
            "Epoch  91 | Train Loss: 0.7070 Acc: 0.7717 | Val Loss: 0.4775 Acc: 0.8688\n",
            "Epoch  92 | Train Loss: 0.7009 Acc: 0.7754 | Val Loss: 0.4755 Acc: 0.8688\n",
            "Epoch  93 | Train Loss: 0.7063 Acc: 0.7726 | Val Loss: 0.4721 Acc: 0.8675\n",
            "Epoch  94 | Train Loss: 0.7019 Acc: 0.7750 | Val Loss: 0.4706 Acc: 0.8683\n",
            "Epoch  95 | Train Loss: 0.6988 Acc: 0.7742 | Val Loss: 0.4715 Acc: 0.8700\n",
            "Epoch  96 | Train Loss: 0.6987 Acc: 0.7754 | Val Loss: 0.4712 Acc: 0.8693\n",
            "Epoch  97 | Train Loss: 0.7003 Acc: 0.7744 | Val Loss: 0.4676 Acc: 0.8703\n",
            "Epoch  98 | Train Loss: 0.6959 Acc: 0.7764 | Val Loss: 0.4635 Acc: 0.8708\n",
            "Epoch  99 | Train Loss: 0.6888 Acc: 0.7786 | Val Loss: 0.4636 Acc: 0.8712\n",
            "Epoch 100 | Train Loss: 0.6934 Acc: 0.7767 | Val Loss: 0.4616 Acc: 0.8707\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_6.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2830 Acc: 0.1049 | Val Loss: 2.2263 Acc: 0.1157\n",
            "Epoch   2 | Train Loss: 2.1953 Acc: 0.1206 | Val Loss: 2.1477 Acc: 0.1422\n",
            "Epoch   3 | Train Loss: 2.1180 Acc: 0.1526 | Val Loss: 2.0690 Acc: 0.1842\n",
            "Epoch   4 | Train Loss: 2.0375 Acc: 0.2207 | Val Loss: 1.9816 Acc: 0.2853\n",
            "Epoch   5 | Train Loss: 1.9509 Acc: 0.3146 | Val Loss: 1.8880 Acc: 0.3810\n",
            "Epoch   6 | Train Loss: 1.8617 Acc: 0.3762 | Val Loss: 1.7894 Acc: 0.4355\n",
            "Epoch   7 | Train Loss: 1.7688 Acc: 0.4149 | Val Loss: 1.6899 Acc: 0.4710\n",
            "Epoch   8 | Train Loss: 1.6775 Acc: 0.4432 | Val Loss: 1.5991 Acc: 0.4940\n",
            "Epoch   9 | Train Loss: 1.5953 Acc: 0.4712 | Val Loss: 1.5111 Acc: 0.5227\n",
            "Epoch  10 | Train Loss: 1.5192 Acc: 0.4951 | Val Loss: 1.4355 Acc: 0.5500\n",
            "Epoch  11 | Train Loss: 1.4487 Acc: 0.5254 | Val Loss: 1.3633 Acc: 0.5795\n",
            "Epoch  12 | Train Loss: 1.3813 Acc: 0.5547 | Val Loss: 1.2890 Acc: 0.6075\n",
            "Epoch  13 | Train Loss: 1.3243 Acc: 0.5785 | Val Loss: 1.2300 Acc: 0.6317\n",
            "Epoch  14 | Train Loss: 1.2663 Acc: 0.6097 | Val Loss: 1.1732 Acc: 0.6705\n",
            "Epoch  15 | Train Loss: 1.2147 Acc: 0.6485 | Val Loss: 1.1164 Acc: 0.7185\n",
            "Epoch  16 | Train Loss: 1.1621 Acc: 0.6792 | Val Loss: 1.0661 Acc: 0.7408\n",
            "Epoch  17 | Train Loss: 1.1142 Acc: 0.6985 | Val Loss: 1.0165 Acc: 0.7625\n",
            "Epoch  18 | Train Loss: 1.0700 Acc: 0.7136 | Val Loss: 0.9734 Acc: 0.7815\n",
            "Epoch  19 | Train Loss: 1.0258 Acc: 0.7317 | Val Loss: 0.9252 Acc: 0.7947\n",
            "Epoch  20 | Train Loss: 0.9864 Acc: 0.7400 | Val Loss: 0.8841 Acc: 0.8058\n",
            "Epoch  21 | Train Loss: 0.9481 Acc: 0.7508 | Val Loss: 0.8434 Acc: 0.8148\n",
            "Epoch  22 | Train Loss: 0.9087 Acc: 0.7617 | Val Loss: 0.8040 Acc: 0.8233\n",
            "Epoch  23 | Train Loss: 0.8728 Acc: 0.7684 | Val Loss: 0.7616 Acc: 0.8308\n",
            "Epoch  24 | Train Loss: 0.8364 Acc: 0.7776 | Val Loss: 0.7204 Acc: 0.8400\n",
            "Epoch  25 | Train Loss: 0.7976 Acc: 0.7842 | Val Loss: 0.6851 Acc: 0.8443\n",
            "Epoch  26 | Train Loss: 0.7660 Acc: 0.7905 | Val Loss: 0.6497 Acc: 0.8502\n",
            "Epoch  27 | Train Loss: 0.7372 Acc: 0.7982 | Val Loss: 0.6233 Acc: 0.8547\n",
            "Epoch  28 | Train Loss: 0.7100 Acc: 0.8019 | Val Loss: 0.5959 Acc: 0.8583\n",
            "Epoch  29 | Train Loss: 0.6844 Acc: 0.8091 | Val Loss: 0.5710 Acc: 0.8620\n",
            "Epoch  30 | Train Loss: 0.6641 Acc: 0.8124 | Val Loss: 0.5541 Acc: 0.8645\n",
            "Epoch  31 | Train Loss: 0.6444 Acc: 0.8162 | Val Loss: 0.5295 Acc: 0.8697\n",
            "Epoch  32 | Train Loss: 0.6264 Acc: 0.8183 | Val Loss: 0.5143 Acc: 0.8713\n",
            "Epoch  33 | Train Loss: 0.6072 Acc: 0.8244 | Val Loss: 0.4996 Acc: 0.8760\n",
            "Epoch  34 | Train Loss: 0.5935 Acc: 0.8274 | Val Loss: 0.4845 Acc: 0.8777\n",
            "Epoch  35 | Train Loss: 0.5787 Acc: 0.8307 | Val Loss: 0.4703 Acc: 0.8793\n",
            "Epoch  36 | Train Loss: 0.5662 Acc: 0.8349 | Val Loss: 0.4560 Acc: 0.8815\n",
            "Epoch  37 | Train Loss: 0.5517 Acc: 0.8380 | Val Loss: 0.4472 Acc: 0.8847\n",
            "Epoch  38 | Train Loss: 0.5398 Acc: 0.8420 | Val Loss: 0.4322 Acc: 0.8870\n",
            "Epoch  39 | Train Loss: 0.5254 Acc: 0.8466 | Val Loss: 0.4225 Acc: 0.8885\n",
            "Epoch  40 | Train Loss: 0.5207 Acc: 0.8469 | Val Loss: 0.4119 Acc: 0.8922\n",
            "Epoch  41 | Train Loss: 0.5093 Acc: 0.8492 | Val Loss: 0.4057 Acc: 0.8940\n",
            "Epoch  42 | Train Loss: 0.4967 Acc: 0.8549 | Val Loss: 0.3941 Acc: 0.8963\n",
            "Epoch  43 | Train Loss: 0.4892 Acc: 0.8565 | Val Loss: 0.3888 Acc: 0.8967\n",
            "Epoch  44 | Train Loss: 0.4792 Acc: 0.8598 | Val Loss: 0.3793 Acc: 0.8978\n",
            "Epoch  45 | Train Loss: 0.4658 Acc: 0.8654 | Val Loss: 0.3741 Acc: 0.8993\n",
            "Epoch  46 | Train Loss: 0.4603 Acc: 0.8657 | Val Loss: 0.3648 Acc: 0.9023\n",
            "Epoch  47 | Train Loss: 0.4534 Acc: 0.8678 | Val Loss: 0.3603 Acc: 0.9022\n",
            "Epoch  48 | Train Loss: 0.4422 Acc: 0.8729 | Val Loss: 0.3528 Acc: 0.9072\n",
            "Epoch  49 | Train Loss: 0.4366 Acc: 0.8736 | Val Loss: 0.3473 Acc: 0.9077\n",
            "Epoch  50 | Train Loss: 0.4337 Acc: 0.8731 | Val Loss: 0.3438 Acc: 0.9090\n",
            "Epoch  51 | Train Loss: 0.4232 Acc: 0.8767 | Val Loss: 0.3368 Acc: 0.9102\n",
            "Epoch  52 | Train Loss: 0.4170 Acc: 0.8790 | Val Loss: 0.3307 Acc: 0.9103\n",
            "Epoch  53 | Train Loss: 0.4094 Acc: 0.8804 | Val Loss: 0.3276 Acc: 0.9125\n",
            "Epoch  54 | Train Loss: 0.4068 Acc: 0.8809 | Val Loss: 0.3230 Acc: 0.9147\n",
            "Epoch  55 | Train Loss: 0.4004 Acc: 0.8845 | Val Loss: 0.3195 Acc: 0.9115\n",
            "Epoch  56 | Train Loss: 0.3922 Acc: 0.8851 | Val Loss: 0.3172 Acc: 0.9127\n",
            "Epoch  57 | Train Loss: 0.3849 Acc: 0.8877 | Val Loss: 0.3133 Acc: 0.9162\n",
            "Epoch  58 | Train Loss: 0.3816 Acc: 0.8880 | Val Loss: 0.3054 Acc: 0.9177\n",
            "Epoch  59 | Train Loss: 0.3718 Acc: 0.8928 | Val Loss: 0.3047 Acc: 0.9178\n",
            "Epoch  60 | Train Loss: 0.3754 Acc: 0.8908 | Val Loss: 0.2998 Acc: 0.9193\n",
            "Epoch  61 | Train Loss: 0.3640 Acc: 0.8955 | Val Loss: 0.2971 Acc: 0.9183\n",
            "Epoch  62 | Train Loss: 0.3611 Acc: 0.8942 | Val Loss: 0.2948 Acc: 0.9205\n",
            "Epoch  63 | Train Loss: 0.3538 Acc: 0.8972 | Val Loss: 0.2912 Acc: 0.9208\n",
            "Epoch  64 | Train Loss: 0.3537 Acc: 0.8961 | Val Loss: 0.2882 Acc: 0.9200\n",
            "Epoch  65 | Train Loss: 0.3438 Acc: 0.9018 | Val Loss: 0.2860 Acc: 0.9210\n",
            "Epoch  66 | Train Loss: 0.3405 Acc: 0.9014 | Val Loss: 0.2817 Acc: 0.9220\n",
            "Epoch  67 | Train Loss: 0.3387 Acc: 0.9014 | Val Loss: 0.2791 Acc: 0.9217\n",
            "Epoch  68 | Train Loss: 0.3349 Acc: 0.9030 | Val Loss: 0.2768 Acc: 0.9213\n",
            "Epoch  69 | Train Loss: 0.3283 Acc: 0.9049 | Val Loss: 0.2735 Acc: 0.9220\n",
            "Epoch  70 | Train Loss: 0.3252 Acc: 0.9049 | Val Loss: 0.2734 Acc: 0.9237\n",
            "Epoch  71 | Train Loss: 0.3232 Acc: 0.9064 | Val Loss: 0.2694 Acc: 0.9250\n",
            "Epoch  72 | Train Loss: 0.3148 Acc: 0.9092 | Val Loss: 0.2682 Acc: 0.9240\n",
            "Epoch  73 | Train Loss: 0.3133 Acc: 0.9095 | Val Loss: 0.2651 Acc: 0.9270\n",
            "Epoch  74 | Train Loss: 0.3099 Acc: 0.9099 | Val Loss: 0.2642 Acc: 0.9258\n",
            "Epoch  75 | Train Loss: 0.3079 Acc: 0.9111 | Val Loss: 0.2606 Acc: 0.9248\n",
            "Epoch  76 | Train Loss: 0.3039 Acc: 0.9120 | Val Loss: 0.2597 Acc: 0.9260\n",
            "Epoch  77 | Train Loss: 0.2991 Acc: 0.9118 | Val Loss: 0.2566 Acc: 0.9287\n",
            "Epoch  78 | Train Loss: 0.2980 Acc: 0.9134 | Val Loss: 0.2574 Acc: 0.9282\n",
            "Epoch  79 | Train Loss: 0.2934 Acc: 0.9138 | Val Loss: 0.2562 Acc: 0.9278\n",
            "Epoch  80 | Train Loss: 0.2915 Acc: 0.9153 | Val Loss: 0.2520 Acc: 0.9300\n",
            "Epoch  81 | Train Loss: 0.2881 Acc: 0.9172 | Val Loss: 0.2514 Acc: 0.9307\n",
            "Epoch  82 | Train Loss: 0.2852 Acc: 0.9177 | Val Loss: 0.2490 Acc: 0.9302\n",
            "Epoch  83 | Train Loss: 0.2798 Acc: 0.9183 | Val Loss: 0.2466 Acc: 0.9325\n",
            "Epoch  84 | Train Loss: 0.2794 Acc: 0.9194 | Val Loss: 0.2486 Acc: 0.9290\n",
            "Epoch  85 | Train Loss: 0.2777 Acc: 0.9196 | Val Loss: 0.2459 Acc: 0.9305\n",
            "Epoch  86 | Train Loss: 0.2739 Acc: 0.9191 | Val Loss: 0.2429 Acc: 0.9303\n",
            "Epoch  87 | Train Loss: 0.2725 Acc: 0.9215 | Val Loss: 0.2417 Acc: 0.9322\n",
            "Epoch  88 | Train Loss: 0.2675 Acc: 0.9223 | Val Loss: 0.2405 Acc: 0.9323\n",
            "Epoch  89 | Train Loss: 0.2649 Acc: 0.9241 | Val Loss: 0.2401 Acc: 0.9325\n",
            "Epoch  90 | Train Loss: 0.2631 Acc: 0.9241 | Val Loss: 0.2369 Acc: 0.9347\n",
            "Epoch  91 | Train Loss: 0.2628 Acc: 0.9232 | Val Loss: 0.2368 Acc: 0.9342\n",
            "Epoch  92 | Train Loss: 0.2561 Acc: 0.9252 | Val Loss: 0.2364 Acc: 0.9340\n",
            "Epoch  93 | Train Loss: 0.2517 Acc: 0.9266 | Val Loss: 0.2356 Acc: 0.9353\n",
            "Epoch  94 | Train Loss: 0.2547 Acc: 0.9257 | Val Loss: 0.2360 Acc: 0.9348\n",
            "Epoch  95 | Train Loss: 0.2518 Acc: 0.9257 | Val Loss: 0.2360 Acc: 0.9340\n",
            "Epoch  96 | Train Loss: 0.2493 Acc: 0.9276 | Val Loss: 0.2321 Acc: 0.9347\n",
            "Epoch  97 | Train Loss: 0.2470 Acc: 0.9280 | Val Loss: 0.2320 Acc: 0.9350\n",
            "Epoch  98 | Train Loss: 0.2424 Acc: 0.9304 | Val Loss: 0.2309 Acc: 0.9358\n",
            "Epoch  99 | Train Loss: 0.2406 Acc: 0.9301 | Val Loss: 0.2293 Acc: 0.9363\n",
            "Epoch 100 | Train Loss: 0.2422 Acc: 0.9297 | Val Loss: 0.2281 Acc: 0.9362\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_7.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.3039 Acc: 0.1209 | Val Loss: 2.2483 Acc: 0.1683\n",
            "Epoch   2 | Train Loss: 2.2262 Acc: 0.1719 | Val Loss: 2.1694 Acc: 0.2407\n",
            "Epoch   3 | Train Loss: 2.1480 Acc: 0.2252 | Val Loss: 2.0764 Acc: 0.2968\n",
            "Epoch   4 | Train Loss: 2.0634 Acc: 0.2943 | Val Loss: 1.9772 Acc: 0.3948\n",
            "Epoch   5 | Train Loss: 1.9696 Acc: 0.3496 | Val Loss: 1.8726 Acc: 0.4320\n",
            "Epoch   6 | Train Loss: 1.8794 Acc: 0.3745 | Val Loss: 1.7763 Acc: 0.4518\n",
            "Epoch   7 | Train Loss: 1.7977 Acc: 0.4002 | Val Loss: 1.6936 Acc: 0.4893\n",
            "Epoch   8 | Train Loss: 1.7221 Acc: 0.4305 | Val Loss: 1.6098 Acc: 0.5282\n",
            "Epoch   9 | Train Loss: 1.6535 Acc: 0.4588 | Val Loss: 1.5371 Acc: 0.5577\n",
            "Epoch  10 | Train Loss: 1.5905 Acc: 0.4837 | Val Loss: 1.4758 Acc: 0.5898\n",
            "Epoch  11 | Train Loss: 1.5302 Acc: 0.5079 | Val Loss: 1.4045 Acc: 0.6138\n",
            "Epoch  12 | Train Loss: 1.4701 Acc: 0.5303 | Val Loss: 1.3408 Acc: 0.6368\n",
            "Epoch  13 | Train Loss: 1.4196 Acc: 0.5474 | Val Loss: 1.2831 Acc: 0.6582\n",
            "Epoch  14 | Train Loss: 1.3690 Acc: 0.5659 | Val Loss: 1.2276 Acc: 0.6733\n",
            "Epoch  15 | Train Loss: 1.3150 Acc: 0.5840 | Val Loss: 1.1731 Acc: 0.6870\n",
            "Epoch  16 | Train Loss: 1.2755 Acc: 0.5957 | Val Loss: 1.1189 Acc: 0.6997\n",
            "Epoch  17 | Train Loss: 1.2306 Acc: 0.6098 | Val Loss: 1.0750 Acc: 0.7178\n",
            "Epoch  18 | Train Loss: 1.1911 Acc: 0.6303 | Val Loss: 1.0262 Acc: 0.7418\n",
            "Epoch  19 | Train Loss: 1.1517 Acc: 0.6466 | Val Loss: 0.9850 Acc: 0.7660\n",
            "Epoch  20 | Train Loss: 1.1180 Acc: 0.6658 | Val Loss: 0.9372 Acc: 0.7837\n",
            "Epoch  21 | Train Loss: 1.0783 Acc: 0.6836 | Val Loss: 0.9019 Acc: 0.7910\n",
            "Epoch  22 | Train Loss: 1.0542 Acc: 0.6896 | Val Loss: 0.8707 Acc: 0.7990\n",
            "Epoch  23 | Train Loss: 1.0198 Acc: 0.6989 | Val Loss: 0.8403 Acc: 0.8037\n",
            "Epoch  24 | Train Loss: 0.9982 Acc: 0.7061 | Val Loss: 0.8119 Acc: 0.8063\n",
            "Epoch  25 | Train Loss: 0.9731 Acc: 0.7100 | Val Loss: 0.7867 Acc: 0.8095\n",
            "Epoch  26 | Train Loss: 0.9493 Acc: 0.7153 | Val Loss: 0.7618 Acc: 0.8150\n",
            "Epoch  27 | Train Loss: 0.9274 Acc: 0.7197 | Val Loss: 0.7321 Acc: 0.8188\n",
            "Epoch  28 | Train Loss: 0.9150 Acc: 0.7201 | Val Loss: 0.7131 Acc: 0.8228\n",
            "Epoch  29 | Train Loss: 0.8895 Acc: 0.7271 | Val Loss: 0.6881 Acc: 0.8293\n",
            "Epoch  30 | Train Loss: 0.8730 Acc: 0.7344 | Val Loss: 0.6782 Acc: 0.8283\n",
            "Epoch  31 | Train Loss: 0.8595 Acc: 0.7368 | Val Loss: 0.6541 Acc: 0.8315\n",
            "Epoch  32 | Train Loss: 0.8396 Acc: 0.7419 | Val Loss: 0.6361 Acc: 0.8372\n",
            "Epoch  33 | Train Loss: 0.8265 Acc: 0.7452 | Val Loss: 0.6217 Acc: 0.8387\n",
            "Epoch  34 | Train Loss: 0.8109 Acc: 0.7494 | Val Loss: 0.6092 Acc: 0.8417\n",
            "Epoch  35 | Train Loss: 0.8012 Acc: 0.7507 | Val Loss: 0.5944 Acc: 0.8437\n",
            "Epoch  36 | Train Loss: 0.7839 Acc: 0.7575 | Val Loss: 0.5830 Acc: 0.8488\n",
            "Epoch  37 | Train Loss: 0.7777 Acc: 0.7583 | Val Loss: 0.5669 Acc: 0.8518\n",
            "Epoch  38 | Train Loss: 0.7641 Acc: 0.7644 | Val Loss: 0.5542 Acc: 0.8557\n",
            "Epoch  39 | Train Loss: 0.7512 Acc: 0.7679 | Val Loss: 0.5438 Acc: 0.8570\n",
            "Epoch  40 | Train Loss: 0.7421 Acc: 0.7695 | Val Loss: 0.5373 Acc: 0.8587\n",
            "Epoch  41 | Train Loss: 0.7298 Acc: 0.7743 | Val Loss: 0.5243 Acc: 0.8618\n",
            "Epoch  42 | Train Loss: 0.7234 Acc: 0.7754 | Val Loss: 0.5147 Acc: 0.8632\n",
            "Epoch  43 | Train Loss: 0.7148 Acc: 0.7787 | Val Loss: 0.5063 Acc: 0.8667\n",
            "Epoch  44 | Train Loss: 0.7051 Acc: 0.7833 | Val Loss: 0.4953 Acc: 0.8700\n",
            "Epoch  45 | Train Loss: 0.6934 Acc: 0.7856 | Val Loss: 0.4861 Acc: 0.8708\n",
            "Epoch  46 | Train Loss: 0.6879 Acc: 0.7866 | Val Loss: 0.4797 Acc: 0.8728\n",
            "Epoch  47 | Train Loss: 0.6815 Acc: 0.7896 | Val Loss: 0.4729 Acc: 0.8725\n",
            "Epoch  48 | Train Loss: 0.6704 Acc: 0.7930 | Val Loss: 0.4640 Acc: 0.8777\n",
            "Epoch  49 | Train Loss: 0.6670 Acc: 0.7931 | Val Loss: 0.4571 Acc: 0.8783\n",
            "Epoch  50 | Train Loss: 0.6541 Acc: 0.7990 | Val Loss: 0.4499 Acc: 0.8797\n",
            "Epoch  51 | Train Loss: 0.6529 Acc: 0.7980 | Val Loss: 0.4431 Acc: 0.8825\n",
            "Epoch  52 | Train Loss: 0.6441 Acc: 0.8006 | Val Loss: 0.4377 Acc: 0.8823\n",
            "Epoch  53 | Train Loss: 0.6416 Acc: 0.8018 | Val Loss: 0.4328 Acc: 0.8833\n",
            "Epoch  54 | Train Loss: 0.6358 Acc: 0.8027 | Val Loss: 0.4280 Acc: 0.8837\n",
            "Epoch  55 | Train Loss: 0.6274 Acc: 0.8076 | Val Loss: 0.4207 Acc: 0.8872\n",
            "Epoch  56 | Train Loss: 0.6237 Acc: 0.8078 | Val Loss: 0.4145 Acc: 0.8888\n",
            "Epoch  57 | Train Loss: 0.6144 Acc: 0.8093 | Val Loss: 0.4093 Acc: 0.8898\n",
            "Epoch  58 | Train Loss: 0.6055 Acc: 0.8115 | Val Loss: 0.4033 Acc: 0.8905\n",
            "Epoch  59 | Train Loss: 0.6063 Acc: 0.8116 | Val Loss: 0.4009 Acc: 0.8898\n",
            "Epoch  60 | Train Loss: 0.6007 Acc: 0.8169 | Val Loss: 0.3954 Acc: 0.8912\n",
            "Epoch  61 | Train Loss: 0.5937 Acc: 0.8163 | Val Loss: 0.3909 Acc: 0.8948\n",
            "Epoch  62 | Train Loss: 0.5891 Acc: 0.8171 | Val Loss: 0.3868 Acc: 0.8920\n",
            "Epoch  63 | Train Loss: 0.5842 Acc: 0.8190 | Val Loss: 0.3852 Acc: 0.8952\n",
            "Epoch  64 | Train Loss: 0.5760 Acc: 0.8220 | Val Loss: 0.3781 Acc: 0.8972\n",
            "Epoch  65 | Train Loss: 0.5728 Acc: 0.8223 | Val Loss: 0.3774 Acc: 0.8970\n",
            "Epoch  66 | Train Loss: 0.5726 Acc: 0.8222 | Val Loss: 0.3715 Acc: 0.8973\n",
            "Epoch  67 | Train Loss: 0.5689 Acc: 0.8237 | Val Loss: 0.3675 Acc: 0.8982\n",
            "Epoch  68 | Train Loss: 0.5633 Acc: 0.8253 | Val Loss: 0.3654 Acc: 0.9010\n",
            "Epoch  69 | Train Loss: 0.5617 Acc: 0.8260 | Val Loss: 0.3624 Acc: 0.9008\n",
            "Epoch  70 | Train Loss: 0.5589 Acc: 0.8266 | Val Loss: 0.3597 Acc: 0.9010\n",
            "Epoch  71 | Train Loss: 0.5465 Acc: 0.8305 | Val Loss: 0.3550 Acc: 0.9042\n",
            "Epoch  72 | Train Loss: 0.5440 Acc: 0.8312 | Val Loss: 0.3534 Acc: 0.9037\n",
            "Epoch  73 | Train Loss: 0.5420 Acc: 0.8344 | Val Loss: 0.3486 Acc: 0.9038\n",
            "Epoch  74 | Train Loss: 0.5392 Acc: 0.8332 | Val Loss: 0.3473 Acc: 0.9045\n",
            "Epoch  75 | Train Loss: 0.5373 Acc: 0.8348 | Val Loss: 0.3414 Acc: 0.9070\n",
            "Epoch  76 | Train Loss: 0.5370 Acc: 0.8339 | Val Loss: 0.3379 Acc: 0.9078\n",
            "Epoch  77 | Train Loss: 0.5307 Acc: 0.8364 | Val Loss: 0.3362 Acc: 0.9068\n",
            "Epoch  78 | Train Loss: 0.5231 Acc: 0.8406 | Val Loss: 0.3344 Acc: 0.9080\n",
            "Epoch  79 | Train Loss: 0.5228 Acc: 0.8383 | Val Loss: 0.3310 Acc: 0.9097\n",
            "Epoch  80 | Train Loss: 0.5202 Acc: 0.8399 | Val Loss: 0.3298 Acc: 0.9085\n",
            "Epoch  81 | Train Loss: 0.5136 Acc: 0.8417 | Val Loss: 0.3266 Acc: 0.9093\n",
            "Epoch  82 | Train Loss: 0.5149 Acc: 0.8416 | Val Loss: 0.3262 Acc: 0.9100\n",
            "Epoch  83 | Train Loss: 0.5100 Acc: 0.8429 | Val Loss: 0.3217 Acc: 0.9125\n",
            "Epoch  84 | Train Loss: 0.5063 Acc: 0.8454 | Val Loss: 0.3200 Acc: 0.9132\n",
            "Epoch  85 | Train Loss: 0.5047 Acc: 0.8460 | Val Loss: 0.3184 Acc: 0.9120\n",
            "Epoch  86 | Train Loss: 0.5028 Acc: 0.8456 | Val Loss: 0.3168 Acc: 0.9108\n",
            "Epoch  87 | Train Loss: 0.5025 Acc: 0.8464 | Val Loss: 0.3129 Acc: 0.9128\n",
            "Epoch  88 | Train Loss: 0.4986 Acc: 0.8462 | Val Loss: 0.3110 Acc: 0.9137\n",
            "Epoch  89 | Train Loss: 0.4907 Acc: 0.8496 | Val Loss: 0.3086 Acc: 0.9145\n",
            "Epoch  90 | Train Loss: 0.4944 Acc: 0.8482 | Val Loss: 0.3088 Acc: 0.9128\n",
            "Epoch  91 | Train Loss: 0.4912 Acc: 0.8504 | Val Loss: 0.3060 Acc: 0.9137\n",
            "Epoch  92 | Train Loss: 0.4883 Acc: 0.8524 | Val Loss: 0.3021 Acc: 0.9160\n",
            "Epoch  93 | Train Loss: 0.4852 Acc: 0.8499 | Val Loss: 0.3016 Acc: 0.9170\n",
            "Epoch  94 | Train Loss: 0.4796 Acc: 0.8536 | Val Loss: 0.3004 Acc: 0.9155\n",
            "Epoch  95 | Train Loss: 0.4785 Acc: 0.8541 | Val Loss: 0.2996 Acc: 0.9162\n",
            "Epoch  96 | Train Loss: 0.4731 Acc: 0.8562 | Val Loss: 0.2973 Acc: 0.9180\n",
            "Epoch  97 | Train Loss: 0.4704 Acc: 0.8551 | Val Loss: 0.2954 Acc: 0.9170\n",
            "Epoch  98 | Train Loss: 0.4751 Acc: 0.8551 | Val Loss: 0.2947 Acc: 0.9182\n",
            "Epoch  99 | Train Loss: 0.4677 Acc: 0.8578 | Val Loss: 0.2918 Acc: 0.9187\n",
            "Epoch 100 | Train Loss: 0.4690 Acc: 0.8573 | Val Loss: 0.2909 Acc: 0.9193\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_8.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.3216 Acc: 0.0828 | Val Loss: 2.3136 Acc: 0.0863\n",
            "Epoch   2 | Train Loss: 2.3078 Acc: 0.0990 | Val Loss: 2.2999 Acc: 0.1113\n",
            "Epoch   3 | Train Loss: 2.2919 Acc: 0.1376 | Val Loss: 2.2812 Acc: 0.1563\n",
            "Epoch   4 | Train Loss: 2.2705 Acc: 0.1647 | Val Loss: 2.2571 Acc: 0.1663\n",
            "Epoch   5 | Train Loss: 2.2451 Acc: 0.1878 | Val Loss: 2.2274 Acc: 0.2002\n",
            "Epoch   6 | Train Loss: 2.2122 Acc: 0.2309 | Val Loss: 2.1898 Acc: 0.2578\n",
            "Epoch   7 | Train Loss: 2.1713 Acc: 0.2600 | Val Loss: 2.1423 Acc: 0.2862\n",
            "Epoch   8 | Train Loss: 2.1209 Acc: 0.2654 | Val Loss: 2.0833 Acc: 0.2933\n",
            "Epoch   9 | Train Loss: 2.0563 Acc: 0.2751 | Val Loss: 2.0099 Acc: 0.3172\n",
            "Epoch  10 | Train Loss: 1.9811 Acc: 0.3238 | Val Loss: 1.9219 Acc: 0.3812\n",
            "Epoch  11 | Train Loss: 1.8912 Acc: 0.3576 | Val Loss: 1.8221 Acc: 0.4120\n",
            "Epoch  12 | Train Loss: 1.7970 Acc: 0.3764 | Val Loss: 1.7206 Acc: 0.4323\n",
            "Epoch  13 | Train Loss: 1.7098 Acc: 0.3963 | Val Loss: 1.6267 Acc: 0.4610\n",
            "Epoch  14 | Train Loss: 1.6284 Acc: 0.4265 | Val Loss: 1.5419 Acc: 0.4990\n",
            "Epoch  15 | Train Loss: 1.5592 Acc: 0.4594 | Val Loss: 1.4654 Acc: 0.5287\n",
            "Epoch  16 | Train Loss: 1.4907 Acc: 0.4875 | Val Loss: 1.3956 Acc: 0.5582\n",
            "Epoch  17 | Train Loss: 1.4326 Acc: 0.5121 | Val Loss: 1.3314 Acc: 0.5792\n",
            "Epoch  18 | Train Loss: 1.3778 Acc: 0.5348 | Val Loss: 1.2721 Acc: 0.6050\n",
            "Epoch  19 | Train Loss: 1.3254 Acc: 0.5586 | Val Loss: 1.2169 Acc: 0.6255\n",
            "Epoch  20 | Train Loss: 1.2764 Acc: 0.5772 | Val Loss: 1.1661 Acc: 0.6473\n",
            "Epoch  21 | Train Loss: 1.2362 Acc: 0.5941 | Val Loss: 1.1202 Acc: 0.6695\n",
            "Epoch  22 | Train Loss: 1.1960 Acc: 0.6086 | Val Loss: 1.0779 Acc: 0.6835\n",
            "Epoch  23 | Train Loss: 1.1603 Acc: 0.6229 | Val Loss: 1.0385 Acc: 0.6933\n",
            "Epoch  24 | Train Loss: 1.1282 Acc: 0.6328 | Val Loss: 1.0030 Acc: 0.7065\n",
            "Epoch  25 | Train Loss: 1.0967 Acc: 0.6437 | Val Loss: 0.9703 Acc: 0.7145\n",
            "Epoch  26 | Train Loss: 1.0610 Acc: 0.6550 | Val Loss: 0.9395 Acc: 0.7213\n",
            "Epoch  27 | Train Loss: 1.0374 Acc: 0.6628 | Val Loss: 0.9106 Acc: 0.7313\n",
            "Epoch  28 | Train Loss: 1.0154 Acc: 0.6702 | Val Loss: 0.8850 Acc: 0.7377\n",
            "Epoch  29 | Train Loss: 0.9873 Acc: 0.6800 | Val Loss: 0.8608 Acc: 0.7438\n",
            "Epoch  30 | Train Loss: 0.9683 Acc: 0.6858 | Val Loss: 0.8384 Acc: 0.7520\n",
            "Epoch  31 | Train Loss: 0.9426 Acc: 0.6941 | Val Loss: 0.8174 Acc: 0.7593\n",
            "Epoch  32 | Train Loss: 0.9297 Acc: 0.6968 | Val Loss: 0.7984 Acc: 0.7622\n",
            "Epoch  33 | Train Loss: 0.9066 Acc: 0.7053 | Val Loss: 0.7801 Acc: 0.7675\n",
            "Epoch  34 | Train Loss: 0.8916 Acc: 0.7111 | Val Loss: 0.7637 Acc: 0.7720\n",
            "Epoch  35 | Train Loss: 0.8733 Acc: 0.7174 | Val Loss: 0.7477 Acc: 0.7780\n",
            "Epoch  36 | Train Loss: 0.8581 Acc: 0.7210 | Val Loss: 0.7329 Acc: 0.7818\n",
            "Epoch  37 | Train Loss: 0.8492 Acc: 0.7259 | Val Loss: 0.7194 Acc: 0.7867\n",
            "Epoch  38 | Train Loss: 0.8356 Acc: 0.7311 | Val Loss: 0.7066 Acc: 0.7915\n",
            "Epoch  39 | Train Loss: 0.8195 Acc: 0.7350 | Val Loss: 0.6948 Acc: 0.7957\n",
            "Epoch  40 | Train Loss: 0.8044 Acc: 0.7408 | Val Loss: 0.6824 Acc: 0.7997\n",
            "Epoch  41 | Train Loss: 0.7970 Acc: 0.7433 | Val Loss: 0.6714 Acc: 0.8032\n",
            "Epoch  42 | Train Loss: 0.7866 Acc: 0.7453 | Val Loss: 0.6614 Acc: 0.8045\n",
            "Epoch  43 | Train Loss: 0.7713 Acc: 0.7523 | Val Loss: 0.6502 Acc: 0.8082\n",
            "Epoch  44 | Train Loss: 0.7581 Acc: 0.7567 | Val Loss: 0.6400 Acc: 0.8107\n",
            "Epoch  45 | Train Loss: 0.7530 Acc: 0.7586 | Val Loss: 0.6307 Acc: 0.8115\n",
            "Epoch  46 | Train Loss: 0.7430 Acc: 0.7620 | Val Loss: 0.6216 Acc: 0.8150\n",
            "Epoch  47 | Train Loss: 0.7304 Acc: 0.7666 | Val Loss: 0.6131 Acc: 0.8180\n",
            "Epoch  48 | Train Loss: 0.7237 Acc: 0.7680 | Val Loss: 0.6048 Acc: 0.8192\n",
            "Epoch  49 | Train Loss: 0.7131 Acc: 0.7725 | Val Loss: 0.5965 Acc: 0.8235\n",
            "Epoch  50 | Train Loss: 0.7021 Acc: 0.7773 | Val Loss: 0.5883 Acc: 0.8260\n",
            "Epoch  51 | Train Loss: 0.6951 Acc: 0.7784 | Val Loss: 0.5807 Acc: 0.8272\n",
            "Epoch  52 | Train Loss: 0.6846 Acc: 0.7819 | Val Loss: 0.5726 Acc: 0.8298\n",
            "Epoch  53 | Train Loss: 0.6777 Acc: 0.7852 | Val Loss: 0.5654 Acc: 0.8327\n",
            "Epoch  54 | Train Loss: 0.6704 Acc: 0.7872 | Val Loss: 0.5582 Acc: 0.8352\n",
            "Epoch  55 | Train Loss: 0.6578 Acc: 0.7923 | Val Loss: 0.5515 Acc: 0.8372\n",
            "Epoch  56 | Train Loss: 0.6527 Acc: 0.7935 | Val Loss: 0.5447 Acc: 0.8393\n",
            "Epoch  57 | Train Loss: 0.6459 Acc: 0.7966 | Val Loss: 0.5383 Acc: 0.8405\n",
            "Epoch  58 | Train Loss: 0.6372 Acc: 0.8004 | Val Loss: 0.5320 Acc: 0.8430\n",
            "Epoch  59 | Train Loss: 0.6323 Acc: 0.8016 | Val Loss: 0.5258 Acc: 0.8440\n",
            "Epoch  60 | Train Loss: 0.6286 Acc: 0.8027 | Val Loss: 0.5208 Acc: 0.8470\n",
            "Epoch  61 | Train Loss: 0.6216 Acc: 0.8038 | Val Loss: 0.5147 Acc: 0.8487\n",
            "Epoch  62 | Train Loss: 0.6143 Acc: 0.8099 | Val Loss: 0.5091 Acc: 0.8503\n",
            "Epoch  63 | Train Loss: 0.6086 Acc: 0.8091 | Val Loss: 0.5037 Acc: 0.8523\n",
            "Epoch  64 | Train Loss: 0.5978 Acc: 0.8112 | Val Loss: 0.4984 Acc: 0.8555\n",
            "Epoch  65 | Train Loss: 0.5950 Acc: 0.8144 | Val Loss: 0.4934 Acc: 0.8567\n",
            "Epoch  66 | Train Loss: 0.5876 Acc: 0.8166 | Val Loss: 0.4876 Acc: 0.8587\n",
            "Epoch  67 | Train Loss: 0.5813 Acc: 0.8187 | Val Loss: 0.4827 Acc: 0.8607\n",
            "Epoch  68 | Train Loss: 0.5719 Acc: 0.8204 | Val Loss: 0.4780 Acc: 0.8617\n",
            "Epoch  69 | Train Loss: 0.5744 Acc: 0.8181 | Val Loss: 0.4736 Acc: 0.8650\n",
            "Epoch  70 | Train Loss: 0.5633 Acc: 0.8236 | Val Loss: 0.4692 Acc: 0.8665\n",
            "Epoch  71 | Train Loss: 0.5626 Acc: 0.8228 | Val Loss: 0.4650 Acc: 0.8675\n",
            "Epoch  72 | Train Loss: 0.5548 Acc: 0.8262 | Val Loss: 0.4608 Acc: 0.8700\n",
            "Epoch  73 | Train Loss: 0.5516 Acc: 0.8270 | Val Loss: 0.4562 Acc: 0.8702\n",
            "Epoch  74 | Train Loss: 0.5457 Acc: 0.8284 | Val Loss: 0.4522 Acc: 0.8705\n",
            "Epoch  75 | Train Loss: 0.5400 Acc: 0.8322 | Val Loss: 0.4481 Acc: 0.8717\n",
            "Epoch  76 | Train Loss: 0.5324 Acc: 0.8337 | Val Loss: 0.4443 Acc: 0.8725\n",
            "Epoch  77 | Train Loss: 0.5335 Acc: 0.8320 | Val Loss: 0.4406 Acc: 0.8737\n",
            "Epoch  78 | Train Loss: 0.5241 Acc: 0.8356 | Val Loss: 0.4365 Acc: 0.8733\n",
            "Epoch  79 | Train Loss: 0.5219 Acc: 0.8371 | Val Loss: 0.4327 Acc: 0.8752\n",
            "Epoch  80 | Train Loss: 0.5121 Acc: 0.8406 | Val Loss: 0.4298 Acc: 0.8760\n",
            "Epoch  81 | Train Loss: 0.5112 Acc: 0.8400 | Val Loss: 0.4258 Acc: 0.8768\n",
            "Epoch  82 | Train Loss: 0.5067 Acc: 0.8408 | Val Loss: 0.4224 Acc: 0.8782\n",
            "Epoch  83 | Train Loss: 0.5032 Acc: 0.8427 | Val Loss: 0.4190 Acc: 0.8788\n",
            "Epoch  84 | Train Loss: 0.5011 Acc: 0.8435 | Val Loss: 0.4161 Acc: 0.8792\n",
            "Epoch  85 | Train Loss: 0.4932 Acc: 0.8472 | Val Loss: 0.4127 Acc: 0.8793\n",
            "Epoch  86 | Train Loss: 0.4930 Acc: 0.8466 | Val Loss: 0.4098 Acc: 0.8800\n",
            "Epoch  87 | Train Loss: 0.4898 Acc: 0.8466 | Val Loss: 0.4065 Acc: 0.8817\n",
            "Epoch  88 | Train Loss: 0.4832 Acc: 0.8487 | Val Loss: 0.4037 Acc: 0.8823\n",
            "Epoch  89 | Train Loss: 0.4822 Acc: 0.8486 | Val Loss: 0.4011 Acc: 0.8827\n",
            "Epoch  90 | Train Loss: 0.4742 Acc: 0.8511 | Val Loss: 0.3979 Acc: 0.8835\n",
            "Epoch  91 | Train Loss: 0.4744 Acc: 0.8515 | Val Loss: 0.3953 Acc: 0.8837\n",
            "Epoch  92 | Train Loss: 0.4710 Acc: 0.8519 | Val Loss: 0.3922 Acc: 0.8845\n",
            "Epoch  93 | Train Loss: 0.4665 Acc: 0.8540 | Val Loss: 0.3895 Acc: 0.8862\n",
            "Epoch  94 | Train Loss: 0.4604 Acc: 0.8564 | Val Loss: 0.3870 Acc: 0.8868\n",
            "Epoch  95 | Train Loss: 0.4596 Acc: 0.8554 | Val Loss: 0.3854 Acc: 0.8887\n",
            "Epoch  96 | Train Loss: 0.4585 Acc: 0.8570 | Val Loss: 0.3820 Acc: 0.8905\n",
            "Epoch  97 | Train Loss: 0.4512 Acc: 0.8596 | Val Loss: 0.3797 Acc: 0.8903\n",
            "Epoch  98 | Train Loss: 0.4507 Acc: 0.8590 | Val Loss: 0.3771 Acc: 0.8920\n",
            "Epoch  99 | Train Loss: 0.4502 Acc: 0.8584 | Val Loss: 0.3753 Acc: 0.8923\n",
            "Epoch 100 | Train Loss: 0.4443 Acc: 0.8608 | Val Loss: 0.3727 Acc: 0.8928\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_9.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.3023 Acc: 0.1276 | Val Loss: 2.2505 Acc: 0.1920\n",
            "Epoch   2 | Train Loss: 2.2279 Acc: 0.1699 | Val Loss: 2.1710 Acc: 0.2412\n",
            "Epoch   3 | Train Loss: 2.1485 Acc: 0.2181 | Val Loss: 2.0813 Acc: 0.3078\n",
            "Epoch   4 | Train Loss: 2.0678 Acc: 0.2751 | Val Loss: 1.9929 Acc: 0.3800\n",
            "Epoch   5 | Train Loss: 1.9856 Acc: 0.3212 | Val Loss: 1.9116 Acc: 0.4245\n",
            "Epoch   6 | Train Loss: 1.9085 Acc: 0.3644 | Val Loss: 1.8198 Acc: 0.4827\n",
            "Epoch   7 | Train Loss: 1.8281 Acc: 0.4071 | Val Loss: 1.7391 Acc: 0.5173\n",
            "Epoch   8 | Train Loss: 1.7534 Acc: 0.4315 | Val Loss: 1.6577 Acc: 0.5383\n",
            "Epoch   9 | Train Loss: 1.6840 Acc: 0.4549 | Val Loss: 1.5745 Acc: 0.5493\n",
            "Epoch  10 | Train Loss: 1.6174 Acc: 0.4738 | Val Loss: 1.4974 Acc: 0.5702\n",
            "Epoch  11 | Train Loss: 1.5548 Acc: 0.4978 | Val Loss: 1.4351 Acc: 0.6175\n",
            "Epoch  12 | Train Loss: 1.4971 Acc: 0.5251 | Val Loss: 1.3708 Acc: 0.6420\n",
            "Epoch  13 | Train Loss: 1.4406 Acc: 0.5474 | Val Loss: 1.3096 Acc: 0.6582\n",
            "Epoch  14 | Train Loss: 1.3909 Acc: 0.5652 | Val Loss: 1.2593 Acc: 0.6697\n",
            "Epoch  15 | Train Loss: 1.3436 Acc: 0.5822 | Val Loss: 1.1988 Acc: 0.6825\n",
            "Epoch  16 | Train Loss: 1.2924 Acc: 0.6001 | Val Loss: 1.1451 Acc: 0.6953\n",
            "Epoch  17 | Train Loss: 1.2504 Acc: 0.6160 | Val Loss: 1.0890 Acc: 0.7097\n",
            "Epoch  18 | Train Loss: 1.2039 Acc: 0.6329 | Val Loss: 1.0360 Acc: 0.7315\n",
            "Epoch  19 | Train Loss: 1.1658 Acc: 0.6437 | Val Loss: 0.9952 Acc: 0.7438\n",
            "Epoch  20 | Train Loss: 1.1239 Acc: 0.6601 | Val Loss: 0.9541 Acc: 0.7538\n",
            "Epoch  21 | Train Loss: 1.0815 Acc: 0.6735 | Val Loss: 0.9082 Acc: 0.7708\n",
            "Epoch  22 | Train Loss: 1.0491 Acc: 0.6829 | Val Loss: 0.8664 Acc: 0.7803\n",
            "Epoch  23 | Train Loss: 1.0147 Acc: 0.6944 | Val Loss: 0.8234 Acc: 0.7932\n",
            "Epoch  24 | Train Loss: 0.9825 Acc: 0.7019 | Val Loss: 0.7896 Acc: 0.8000\n",
            "Epoch  25 | Train Loss: 0.9510 Acc: 0.7162 | Val Loss: 0.7611 Acc: 0.8090\n",
            "Epoch  26 | Train Loss: 0.9281 Acc: 0.7208 | Val Loss: 0.7304 Acc: 0.8153\n",
            "Epoch  27 | Train Loss: 0.9030 Acc: 0.7269 | Val Loss: 0.7071 Acc: 0.8213\n",
            "Epoch  28 | Train Loss: 0.8781 Acc: 0.7325 | Val Loss: 0.6813 Acc: 0.8270\n",
            "Epoch  29 | Train Loss: 0.8557 Acc: 0.7415 | Val Loss: 0.6494 Acc: 0.8325\n",
            "Epoch  30 | Train Loss: 0.8435 Acc: 0.7460 | Val Loss: 0.6419 Acc: 0.8373\n",
            "Epoch  31 | Train Loss: 0.8212 Acc: 0.7504 | Val Loss: 0.6193 Acc: 0.8427\n",
            "Epoch  32 | Train Loss: 0.8109 Acc: 0.7539 | Val Loss: 0.6053 Acc: 0.8443\n",
            "Epoch  33 | Train Loss: 0.7882 Acc: 0.7606 | Val Loss: 0.5892 Acc: 0.8512\n",
            "Epoch  34 | Train Loss: 0.7747 Acc: 0.7680 | Val Loss: 0.5709 Acc: 0.8528\n",
            "Epoch  35 | Train Loss: 0.7639 Acc: 0.7690 | Val Loss: 0.5586 Acc: 0.8550\n",
            "Epoch  36 | Train Loss: 0.7448 Acc: 0.7743 | Val Loss: 0.5415 Acc: 0.8603\n",
            "Epoch  37 | Train Loss: 0.7303 Acc: 0.7788 | Val Loss: 0.5319 Acc: 0.8622\n",
            "Epoch  38 | Train Loss: 0.7237 Acc: 0.7809 | Val Loss: 0.5169 Acc: 0.8660\n",
            "Epoch  39 | Train Loss: 0.7119 Acc: 0.7829 | Val Loss: 0.5052 Acc: 0.8698\n",
            "Epoch  40 | Train Loss: 0.7052 Acc: 0.7848 | Val Loss: 0.4971 Acc: 0.8683\n",
            "Epoch  41 | Train Loss: 0.6894 Acc: 0.7882 | Val Loss: 0.4889 Acc: 0.8735\n",
            "Epoch  42 | Train Loss: 0.6827 Acc: 0.7921 | Val Loss: 0.4738 Acc: 0.8757\n",
            "Epoch  43 | Train Loss: 0.6694 Acc: 0.7971 | Val Loss: 0.4707 Acc: 0.8768\n",
            "Epoch  44 | Train Loss: 0.6613 Acc: 0.7977 | Val Loss: 0.4561 Acc: 0.8805\n",
            "Epoch  45 | Train Loss: 0.6485 Acc: 0.8042 | Val Loss: 0.4505 Acc: 0.8807\n",
            "Epoch  46 | Train Loss: 0.6435 Acc: 0.8053 | Val Loss: 0.4401 Acc: 0.8830\n",
            "Epoch  47 | Train Loss: 0.6319 Acc: 0.8090 | Val Loss: 0.4298 Acc: 0.8862\n",
            "Epoch  48 | Train Loss: 0.6296 Acc: 0.8097 | Val Loss: 0.4225 Acc: 0.8870\n",
            "Epoch  49 | Train Loss: 0.6168 Acc: 0.8132 | Val Loss: 0.4137 Acc: 0.8902\n",
            "Epoch  50 | Train Loss: 0.6098 Acc: 0.8161 | Val Loss: 0.4097 Acc: 0.8915\n",
            "Epoch  51 | Train Loss: 0.6054 Acc: 0.8157 | Val Loss: 0.4036 Acc: 0.8938\n",
            "Epoch  52 | Train Loss: 0.5964 Acc: 0.8199 | Val Loss: 0.3948 Acc: 0.8962\n",
            "Epoch  53 | Train Loss: 0.5914 Acc: 0.8206 | Val Loss: 0.3874 Acc: 0.8980\n",
            "Epoch  54 | Train Loss: 0.5834 Acc: 0.8229 | Val Loss: 0.3832 Acc: 0.8987\n",
            "Epoch  55 | Train Loss: 0.5757 Acc: 0.8253 | Val Loss: 0.3759 Acc: 0.9012\n",
            "Epoch  56 | Train Loss: 0.5725 Acc: 0.8249 | Val Loss: 0.3732 Acc: 0.8987\n",
            "Epoch  57 | Train Loss: 0.5687 Acc: 0.8255 | Val Loss: 0.3695 Acc: 0.9010\n",
            "Epoch  58 | Train Loss: 0.5629 Acc: 0.8267 | Val Loss: 0.3622 Acc: 0.9040\n",
            "Epoch  59 | Train Loss: 0.5550 Acc: 0.8328 | Val Loss: 0.3595 Acc: 0.9042\n",
            "Epoch  60 | Train Loss: 0.5464 Acc: 0.8345 | Val Loss: 0.3528 Acc: 0.9067\n",
            "Epoch  61 | Train Loss: 0.5435 Acc: 0.8355 | Val Loss: 0.3480 Acc: 0.9102\n",
            "Epoch  62 | Train Loss: 0.5335 Acc: 0.8392 | Val Loss: 0.3466 Acc: 0.9072\n",
            "Epoch  63 | Train Loss: 0.5310 Acc: 0.8389 | Val Loss: 0.3390 Acc: 0.9103\n",
            "Epoch  64 | Train Loss: 0.5223 Acc: 0.8438 | Val Loss: 0.3383 Acc: 0.9112\n",
            "Epoch  65 | Train Loss: 0.5206 Acc: 0.8416 | Val Loss: 0.3325 Acc: 0.9122\n",
            "Epoch  66 | Train Loss: 0.5150 Acc: 0.8440 | Val Loss: 0.3281 Acc: 0.9130\n",
            "Epoch  67 | Train Loss: 0.5145 Acc: 0.8447 | Val Loss: 0.3254 Acc: 0.9140\n",
            "Epoch  68 | Train Loss: 0.5056 Acc: 0.8484 | Val Loss: 0.3229 Acc: 0.9142\n",
            "Epoch  69 | Train Loss: 0.5043 Acc: 0.8476 | Val Loss: 0.3211 Acc: 0.9153\n",
            "Epoch  70 | Train Loss: 0.4999 Acc: 0.8492 | Val Loss: 0.3152 Acc: 0.9170\n",
            "Epoch  71 | Train Loss: 0.4931 Acc: 0.8495 | Val Loss: 0.3125 Acc: 0.9157\n",
            "Epoch  72 | Train Loss: 0.4820 Acc: 0.8544 | Val Loss: 0.3143 Acc: 0.9175\n",
            "Epoch  73 | Train Loss: 0.4888 Acc: 0.8542 | Val Loss: 0.3063 Acc: 0.9173\n",
            "Epoch  74 | Train Loss: 0.4845 Acc: 0.8529 | Val Loss: 0.3050 Acc: 0.9172\n",
            "Epoch  75 | Train Loss: 0.4812 Acc: 0.8537 | Val Loss: 0.2993 Acc: 0.9197\n",
            "Epoch  76 | Train Loss: 0.4746 Acc: 0.8573 | Val Loss: 0.3001 Acc: 0.9190\n",
            "Epoch  77 | Train Loss: 0.4711 Acc: 0.8584 | Val Loss: 0.2939 Acc: 0.9210\n",
            "Epoch  78 | Train Loss: 0.4695 Acc: 0.8571 | Val Loss: 0.2925 Acc: 0.9210\n",
            "Epoch  79 | Train Loss: 0.4626 Acc: 0.8597 | Val Loss: 0.2928 Acc: 0.9202\n",
            "Epoch  80 | Train Loss: 0.4617 Acc: 0.8592 | Val Loss: 0.2909 Acc: 0.9220\n",
            "Epoch  81 | Train Loss: 0.4600 Acc: 0.8608 | Val Loss: 0.2863 Acc: 0.9238\n",
            "Epoch  82 | Train Loss: 0.4572 Acc: 0.8607 | Val Loss: 0.2840 Acc: 0.9243\n",
            "Epoch  83 | Train Loss: 0.4472 Acc: 0.8641 | Val Loss: 0.2801 Acc: 0.9250\n",
            "Epoch  84 | Train Loss: 0.4524 Acc: 0.8637 | Val Loss: 0.2799 Acc: 0.9240\n",
            "Epoch  85 | Train Loss: 0.4453 Acc: 0.8646 | Val Loss: 0.2752 Acc: 0.9260\n",
            "Epoch  86 | Train Loss: 0.4406 Acc: 0.8686 | Val Loss: 0.2774 Acc: 0.9248\n",
            "Epoch  87 | Train Loss: 0.4388 Acc: 0.8660 | Val Loss: 0.2720 Acc: 0.9272\n",
            "Epoch  88 | Train Loss: 0.4349 Acc: 0.8684 | Val Loss: 0.2700 Acc: 0.9273\n",
            "Epoch  89 | Train Loss: 0.4305 Acc: 0.8694 | Val Loss: 0.2702 Acc: 0.9288\n",
            "Epoch  90 | Train Loss: 0.4300 Acc: 0.8696 | Val Loss: 0.2685 Acc: 0.9283\n",
            "Epoch  91 | Train Loss: 0.4251 Acc: 0.8716 | Val Loss: 0.2666 Acc: 0.9278\n",
            "Epoch  92 | Train Loss: 0.4237 Acc: 0.8719 | Val Loss: 0.2631 Acc: 0.9290\n",
            "Epoch  93 | Train Loss: 0.4241 Acc: 0.8718 | Val Loss: 0.2628 Acc: 0.9287\n",
            "Epoch  94 | Train Loss: 0.4199 Acc: 0.8716 | Val Loss: 0.2587 Acc: 0.9302\n",
            "Epoch  95 | Train Loss: 0.4145 Acc: 0.8735 | Val Loss: 0.2596 Acc: 0.9298\n",
            "Epoch  96 | Train Loss: 0.4154 Acc: 0.8739 | Val Loss: 0.2618 Acc: 0.9277\n",
            "Epoch  97 | Train Loss: 0.4076 Acc: 0.8761 | Val Loss: 0.2572 Acc: 0.9303\n",
            "Epoch  98 | Train Loss: 0.4108 Acc: 0.8749 | Val Loss: 0.2528 Acc: 0.9340\n",
            "Epoch  99 | Train Loss: 0.4050 Acc: 0.8767 | Val Loss: 0.2529 Acc: 0.9333\n",
            "Epoch 100 | Train Loss: 0.4013 Acc: 0.8786 | Val Loss: 0.2492 Acc: 0.9338\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_10.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 | Train Loss: 2.2856 Acc: 0.1171 | Val Loss: 2.2466 Acc: 0.1543\n",
            "Epoch   2 | Train Loss: 2.2322 Acc: 0.1692 | Val Loss: 2.1854 Acc: 0.2483\n",
            "Epoch   3 | Train Loss: 2.1716 Acc: 0.2322 | Val Loss: 2.1107 Acc: 0.3465\n",
            "Epoch   4 | Train Loss: 2.0994 Acc: 0.2945 | Val Loss: 2.0253 Acc: 0.4185\n",
            "Epoch   5 | Train Loss: 2.0198 Acc: 0.3445 | Val Loss: 1.9288 Acc: 0.4712\n",
            "Epoch   6 | Train Loss: 1.9341 Acc: 0.3854 | Val Loss: 1.8340 Acc: 0.5085\n",
            "Epoch   7 | Train Loss: 1.8487 Acc: 0.4195 | Val Loss: 1.7428 Acc: 0.5487\n",
            "Epoch   8 | Train Loss: 1.7678 Acc: 0.4488 | Val Loss: 1.6457 Acc: 0.5788\n",
            "Epoch   9 | Train Loss: 1.6902 Acc: 0.4757 | Val Loss: 1.5686 Acc: 0.6102\n",
            "Epoch  10 | Train Loss: 1.6226 Acc: 0.5001 | Val Loss: 1.4985 Acc: 0.6368\n",
            "Epoch  11 | Train Loss: 1.5586 Acc: 0.5193 | Val Loss: 1.4242 Acc: 0.6638\n",
            "Epoch  12 | Train Loss: 1.4956 Acc: 0.5427 | Val Loss: 1.3455 Acc: 0.6848\n",
            "Epoch  13 | Train Loss: 1.4341 Acc: 0.5682 | Val Loss: 1.2831 Acc: 0.7038\n",
            "Epoch  14 | Train Loss: 1.3774 Acc: 0.5889 | Val Loss: 1.2288 Acc: 0.7190\n",
            "Epoch  15 | Train Loss: 1.3268 Acc: 0.6041 | Val Loss: 1.1684 Acc: 0.7280\n",
            "Epoch  16 | Train Loss: 1.2775 Acc: 0.6163 | Val Loss: 1.1120 Acc: 0.7418\n",
            "Epoch  17 | Train Loss: 1.2314 Acc: 0.6296 | Val Loss: 1.0531 Acc: 0.7542\n",
            "Epoch  18 | Train Loss: 1.1900 Acc: 0.6446 | Val Loss: 1.0045 Acc: 0.7658\n",
            "Epoch  19 | Train Loss: 1.1530 Acc: 0.6519 | Val Loss: 0.9581 Acc: 0.7758\n",
            "Epoch  20 | Train Loss: 1.1145 Acc: 0.6639 | Val Loss: 0.9178 Acc: 0.7890\n",
            "Epoch  21 | Train Loss: 1.0789 Acc: 0.6777 | Val Loss: 0.8802 Acc: 0.7977\n",
            "Epoch  22 | Train Loss: 1.0458 Acc: 0.6864 | Val Loss: 0.8389 Acc: 0.8042\n",
            "Epoch  23 | Train Loss: 1.0196 Acc: 0.6934 | Val Loss: 0.8026 Acc: 0.8102\n",
            "Epoch  24 | Train Loss: 0.9934 Acc: 0.7011 | Val Loss: 0.7744 Acc: 0.8143\n",
            "Epoch  25 | Train Loss: 0.9653 Acc: 0.7100 | Val Loss: 0.7406 Acc: 0.8187\n",
            "Epoch  26 | Train Loss: 0.9433 Acc: 0.7148 | Val Loss: 0.7141 Acc: 0.8265\n",
            "Epoch  27 | Train Loss: 0.9202 Acc: 0.7220 | Val Loss: 0.6865 Acc: 0.8323\n",
            "Epoch  28 | Train Loss: 0.8966 Acc: 0.7307 | Val Loss: 0.6687 Acc: 0.8375\n",
            "Epoch  29 | Train Loss: 0.8851 Acc: 0.7306 | Val Loss: 0.6477 Acc: 0.8420\n",
            "Epoch  30 | Train Loss: 0.8625 Acc: 0.7369 | Val Loss: 0.6208 Acc: 0.8442\n",
            "Epoch  31 | Train Loss: 0.8459 Acc: 0.7400 | Val Loss: 0.6088 Acc: 0.8480\n",
            "Epoch  32 | Train Loss: 0.8285 Acc: 0.7445 | Val Loss: 0.5882 Acc: 0.8513\n",
            "Epoch  33 | Train Loss: 0.8180 Acc: 0.7490 | Val Loss: 0.5718 Acc: 0.8570\n",
            "Epoch  34 | Train Loss: 0.8033 Acc: 0.7539 | Val Loss: 0.5570 Acc: 0.8577\n",
            "Epoch  35 | Train Loss: 0.7940 Acc: 0.7555 | Val Loss: 0.5417 Acc: 0.8605\n",
            "Epoch  36 | Train Loss: 0.7761 Acc: 0.7614 | Val Loss: 0.5305 Acc: 0.8643\n",
            "Epoch  37 | Train Loss: 0.7613 Acc: 0.7667 | Val Loss: 0.5193 Acc: 0.8652\n",
            "Epoch  38 | Train Loss: 0.7497 Acc: 0.7699 | Val Loss: 0.5050 Acc: 0.8680\n",
            "Epoch  39 | Train Loss: 0.7376 Acc: 0.7736 | Val Loss: 0.4966 Acc: 0.8688\n",
            "Epoch  40 | Train Loss: 0.7327 Acc: 0.7741 | Val Loss: 0.4842 Acc: 0.8725\n",
            "Epoch  41 | Train Loss: 0.7207 Acc: 0.7774 | Val Loss: 0.4768 Acc: 0.8723\n",
            "Epoch  42 | Train Loss: 0.7106 Acc: 0.7813 | Val Loss: 0.4667 Acc: 0.8758\n",
            "Epoch  43 | Train Loss: 0.6982 Acc: 0.7834 | Val Loss: 0.4568 Acc: 0.8790\n",
            "Epoch  44 | Train Loss: 0.6927 Acc: 0.7885 | Val Loss: 0.4499 Acc: 0.8802\n",
            "Epoch  45 | Train Loss: 0.6855 Acc: 0.7876 | Val Loss: 0.4415 Acc: 0.8808\n",
            "Epoch  46 | Train Loss: 0.6769 Acc: 0.7928 | Val Loss: 0.4326 Acc: 0.8807\n",
            "Epoch  47 | Train Loss: 0.6655 Acc: 0.7964 | Val Loss: 0.4280 Acc: 0.8852\n",
            "Epoch  48 | Train Loss: 0.6591 Acc: 0.7944 | Val Loss: 0.4191 Acc: 0.8837\n",
            "Epoch  49 | Train Loss: 0.6481 Acc: 0.8016 | Val Loss: 0.4153 Acc: 0.8860\n",
            "Epoch  50 | Train Loss: 0.6471 Acc: 0.8026 | Val Loss: 0.4101 Acc: 0.8885\n",
            "Epoch  51 | Train Loss: 0.6397 Acc: 0.8043 | Val Loss: 0.4045 Acc: 0.8887\n",
            "Epoch  52 | Train Loss: 0.6344 Acc: 0.8067 | Val Loss: 0.3995 Acc: 0.8893\n",
            "Epoch  53 | Train Loss: 0.6252 Acc: 0.8094 | Val Loss: 0.3939 Acc: 0.8917\n",
            "Epoch  54 | Train Loss: 0.6222 Acc: 0.8086 | Val Loss: 0.3887 Acc: 0.8930\n",
            "Epoch  55 | Train Loss: 0.6165 Acc: 0.8107 | Val Loss: 0.3852 Acc: 0.8910\n",
            "Epoch  56 | Train Loss: 0.6095 Acc: 0.8132 | Val Loss: 0.3772 Acc: 0.8957\n",
            "Epoch  57 | Train Loss: 0.6050 Acc: 0.8148 | Val Loss: 0.3729 Acc: 0.8953\n",
            "Epoch  58 | Train Loss: 0.5963 Acc: 0.8164 | Val Loss: 0.3727 Acc: 0.8935\n",
            "Epoch  59 | Train Loss: 0.5894 Acc: 0.8199 | Val Loss: 0.3678 Acc: 0.8960\n",
            "Epoch  60 | Train Loss: 0.5840 Acc: 0.8218 | Val Loss: 0.3625 Acc: 0.8962\n",
            "Epoch  61 | Train Loss: 0.5851 Acc: 0.8218 | Val Loss: 0.3615 Acc: 0.8968\n",
            "Epoch  62 | Train Loss: 0.5732 Acc: 0.8255 | Val Loss: 0.3561 Acc: 0.8970\n",
            "Epoch  63 | Train Loss: 0.5753 Acc: 0.8243 | Val Loss: 0.3523 Acc: 0.8983\n",
            "Epoch  64 | Train Loss: 0.5670 Acc: 0.8278 | Val Loss: 0.3499 Acc: 0.8992\n",
            "Epoch  65 | Train Loss: 0.5693 Acc: 0.8292 | Val Loss: 0.3468 Acc: 0.8993\n",
            "Epoch  66 | Train Loss: 0.5562 Acc: 0.8305 | Val Loss: 0.3426 Acc: 0.9012\n",
            "Epoch  67 | Train Loss: 0.5528 Acc: 0.8318 | Val Loss: 0.3397 Acc: 0.9015\n",
            "Epoch  68 | Train Loss: 0.5505 Acc: 0.8330 | Val Loss: 0.3399 Acc: 0.9037\n",
            "Epoch  69 | Train Loss: 0.5508 Acc: 0.8315 | Val Loss: 0.3369 Acc: 0.9038\n",
            "Epoch  70 | Train Loss: 0.5330 Acc: 0.8398 | Val Loss: 0.3351 Acc: 0.9043\n",
            "Epoch  71 | Train Loss: 0.5368 Acc: 0.8396 | Val Loss: 0.3293 Acc: 0.9057\n",
            "Epoch  72 | Train Loss: 0.5311 Acc: 0.8380 | Val Loss: 0.3278 Acc: 0.9062\n",
            "Epoch  73 | Train Loss: 0.5339 Acc: 0.8398 | Val Loss: 0.3247 Acc: 0.9073\n",
            "Epoch  74 | Train Loss: 0.5248 Acc: 0.8408 | Val Loss: 0.3228 Acc: 0.9075\n",
            "Epoch  75 | Train Loss: 0.5289 Acc: 0.8410 | Val Loss: 0.3191 Acc: 0.9077\n",
            "Epoch  76 | Train Loss: 0.5180 Acc: 0.8451 | Val Loss: 0.3162 Acc: 0.9092\n",
            "Epoch  77 | Train Loss: 0.5166 Acc: 0.8431 | Val Loss: 0.3155 Acc: 0.9082\n",
            "Epoch  78 | Train Loss: 0.5097 Acc: 0.8464 | Val Loss: 0.3146 Acc: 0.9083\n",
            "Epoch  79 | Train Loss: 0.5109 Acc: 0.8469 | Val Loss: 0.3120 Acc: 0.9105\n",
            "Epoch  80 | Train Loss: 0.5047 Acc: 0.8485 | Val Loss: 0.3111 Acc: 0.9100\n",
            "Epoch  81 | Train Loss: 0.5001 Acc: 0.8502 | Val Loss: 0.3063 Acc: 0.9108\n",
            "Epoch  82 | Train Loss: 0.4994 Acc: 0.8503 | Val Loss: 0.3079 Acc: 0.9092\n",
            "Epoch  83 | Train Loss: 0.4957 Acc: 0.8500 | Val Loss: 0.3056 Acc: 0.9113\n",
            "Epoch  84 | Train Loss: 0.4893 Acc: 0.8530 | Val Loss: 0.3026 Acc: 0.9108\n",
            "Epoch  85 | Train Loss: 0.4868 Acc: 0.8560 | Val Loss: 0.2980 Acc: 0.9137\n",
            "Epoch  86 | Train Loss: 0.4880 Acc: 0.8543 | Val Loss: 0.3003 Acc: 0.9138\n",
            "Epoch  87 | Train Loss: 0.4850 Acc: 0.8536 | Val Loss: 0.2933 Acc: 0.9145\n",
            "Epoch  88 | Train Loss: 0.4796 Acc: 0.8564 | Val Loss: 0.2905 Acc: 0.9163\n",
            "Epoch  89 | Train Loss: 0.4810 Acc: 0.8557 | Val Loss: 0.2934 Acc: 0.9135\n",
            "Epoch  90 | Train Loss: 0.4784 Acc: 0.8568 | Val Loss: 0.2902 Acc: 0.9147\n",
            "Epoch  91 | Train Loss: 0.4699 Acc: 0.8586 | Val Loss: 0.2887 Acc: 0.9153\n",
            "Epoch  92 | Train Loss: 0.4750 Acc: 0.8567 | Val Loss: 0.2881 Acc: 0.9148\n",
            "Epoch  93 | Train Loss: 0.4703 Acc: 0.8599 | Val Loss: 0.2855 Acc: 0.9172\n",
            "Epoch  94 | Train Loss: 0.4675 Acc: 0.8589 | Val Loss: 0.2839 Acc: 0.9188\n",
            "Epoch  95 | Train Loss: 0.4644 Acc: 0.8625 | Val Loss: 0.2837 Acc: 0.9173\n",
            "Epoch  96 | Train Loss: 0.4617 Acc: 0.8627 | Val Loss: 0.2806 Acc: 0.9172\n",
            "Epoch  97 | Train Loss: 0.4639 Acc: 0.8626 | Val Loss: 0.2812 Acc: 0.9177\n",
            "Epoch  98 | Train Loss: 0.4610 Acc: 0.8634 | Val Loss: 0.2788 Acc: 0.9190\n",
            "Epoch  99 | Train Loss: 0.4562 Acc: 0.8644 | Val Loss: 0.2794 Acc: 0.9173\n",
            "Epoch 100 | Train Loss: 0.4494 Acc: 0.8654 | Val Loss: 0.2744 Acc: 0.9203\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_11.pt\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>layers</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>macro_precision</th>\n",
              "      <th>macro_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>exp_0</td>\n",
              "      <td>[32, 10]</td>\n",
              "      <td>0.7413</td>\n",
              "      <td>0.742177</td>\n",
              "      <td>0.753625</td>\n",
              "      <td>0.7413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>exp_1</td>\n",
              "      <td>[64, 10]</td>\n",
              "      <td>0.7866</td>\n",
              "      <td>0.787157</td>\n",
              "      <td>0.793749</td>\n",
              "      <td>0.7866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>exp_2</td>\n",
              "      <td>[128, 10]</td>\n",
              "      <td>0.8082</td>\n",
              "      <td>0.809166</td>\n",
              "      <td>0.816217</td>\n",
              "      <td>0.8082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>exp_3</td>\n",
              "      <td>[256, 10]</td>\n",
              "      <td>0.8257</td>\n",
              "      <td>0.826034</td>\n",
              "      <td>0.829225</td>\n",
              "      <td>0.8257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>exp_4</td>\n",
              "      <td>[64, 10]</td>\n",
              "      <td>0.7772</td>\n",
              "      <td>0.778357</td>\n",
              "      <td>0.785799</td>\n",
              "      <td>0.7772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>exp_5</td>\n",
              "      <td>[64, 10]</td>\n",
              "      <td>0.7621</td>\n",
              "      <td>0.762790</td>\n",
              "      <td>0.770971</td>\n",
              "      <td>0.7621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>exp_6</td>\n",
              "      <td>[64, 10]</td>\n",
              "      <td>0.7401</td>\n",
              "      <td>0.741018</td>\n",
              "      <td>0.754488</td>\n",
              "      <td>0.7401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>exp_7</td>\n",
              "      <td>[128, 64, 10]</td>\n",
              "      <td>0.8553</td>\n",
              "      <td>0.855499</td>\n",
              "      <td>0.857222</td>\n",
              "      <td>0.8553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>exp_8</td>\n",
              "      <td>[128, 64, 10]</td>\n",
              "      <td>0.8266</td>\n",
              "      <td>0.827357</td>\n",
              "      <td>0.832544</td>\n",
              "      <td>0.8266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>exp_9</td>\n",
              "      <td>[128, 64, 10]</td>\n",
              "      <td>0.7852</td>\n",
              "      <td>0.786066</td>\n",
              "      <td>0.791708</td>\n",
              "      <td>0.7852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>exp_10</td>\n",
              "      <td>[256, 128, 64, 10]</td>\n",
              "      <td>0.8494</td>\n",
              "      <td>0.850282</td>\n",
              "      <td>0.854866</td>\n",
              "      <td>0.8494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>exp_11</td>\n",
              "      <td>[128, 128, 64, 10]</td>\n",
              "      <td>0.8275</td>\n",
              "      <td>0.828138</td>\n",
              "      <td>0.832562</td>\n",
              "      <td>0.8275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   experiment              layers  test_accuracy  macro_f1  macro_precision  \\\n",
              "0       exp_0            [32, 10]         0.7413  0.742177         0.753625   \n",
              "1       exp_1            [64, 10]         0.7866  0.787157         0.793749   \n",
              "2       exp_2           [128, 10]         0.8082  0.809166         0.816217   \n",
              "3       exp_3           [256, 10]         0.8257  0.826034         0.829225   \n",
              "4       exp_4            [64, 10]         0.7772  0.778357         0.785799   \n",
              "5       exp_5            [64, 10]         0.7621  0.762790         0.770971   \n",
              "6       exp_6            [64, 10]         0.7401  0.741018         0.754488   \n",
              "7       exp_7       [128, 64, 10]         0.8553  0.855499         0.857222   \n",
              "8       exp_8       [128, 64, 10]         0.8266  0.827357         0.832544   \n",
              "9       exp_9       [128, 64, 10]         0.7852  0.786066         0.791708   \n",
              "10     exp_10  [256, 128, 64, 10]         0.8494  0.850282         0.854866   \n",
              "11     exp_11  [128, 128, 64, 10]         0.8275  0.828138         0.832562   \n",
              "\n",
              "    macro_recall  \n",
              "0         0.7413  \n",
              "1         0.7866  \n",
              "2         0.8082  \n",
              "3         0.8257  \n",
              "4         0.7772  \n",
              "5         0.7621  \n",
              "6         0.7401  \n",
              "7         0.8553  \n",
              "8         0.8266  \n",
              "9         0.7852  \n",
              "10        0.8494  \n",
              "11        0.8275  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train_cfg = TrainConfig(\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    lr=1e-4,\n",
        "    patience=5,\n",
        "    val_fraction=0.1\n",
        ")\n",
        "\n",
        "data_mgr = DataManager(\n",
        "    dataset_class=datasets.KMNIST,\n",
        "    val_fraction=train_cfg.val_fraction,\n",
        "    batch_size=train_cfg.batch_size,\n",
        "    seed=train_cfg.seed\n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = data_mgr.get_loaders()\n",
        "\n",
        "experiments = [\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=32, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=256, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.1, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.3, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.0, batch_norm=False, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.0, batch_norm=False, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=256, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "mlflow.set_experiment(\"KMNIST_Assigment\")\n",
        "excel_results = []\n",
        "best_macro_f1 = -1\n",
        "best_trainer = None\n",
        "best_experiment_idx = None\n",
        "\n",
        "\n",
        "for i, exp in enumerate(experiments):\n",
        "    model_cfg = ModelConfig(layers=exp[\"layers\"])\n",
        "    model = MLPFromConfig(model_cfg)\n",
        "    trainer = Trainer(model, train_cfg)\n",
        "\n",
        "    mlflow.start_run(run_name=f\"exp_{i}\")\n",
        "\n",
        "    mlflow.log_param(\"layers\", [spec.out_dim for spec in model_cfg.layers])\n",
        "    mlflow.log_param(\"dropout\", [spec.dropout for spec in model_cfg.layers])\n",
        "    mlflow.log_param(\"batch_norm\", [spec.batch_norm for spec in model_cfg.layers])\n",
        "\n",
        "    trainer.fit(train_loader, val_loader)\n",
        "\n",
        "    preds, targets = trainer.predict_all(test_loader)\n",
        "    report = classification_report(targets, preds, digits=4, output_dict=True)\n",
        "\n",
        "    mlflow.log_metric(\"test_accuracy\", report[\"accuracy\"])\n",
        "    mlflow.log_metric(\"macro_f1\", report[\"macro avg\"][\"f1-score\"])\n",
        "    mlflow.log_metric(\"macro_precision\", report[\"macro avg\"][\"precision\"])\n",
        "    mlflow.log_metric(\"macro_recall\", report[\"macro avg\"][\"recall\"])\n",
        "\n",
        "    trainer.save(f\"model_exp_{i}.pt\")\n",
        "\n",
        "\n",
        "    excel_results.append({\n",
        "    \"experiment\": f\"exp_{i}\",\n",
        "    \"layers\": [spec.out_dim for spec in model_cfg.layers],\n",
        "    \"test_accuracy\": report[\"accuracy\"],\n",
        "    \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "    \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
        "    \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
        "})\n",
        "    current_f1 = report[\"macro avg\"][\"f1-score\"]\n",
        "\n",
        "    if current_f1 > best_macro_f1:\n",
        "        best_macro_f1 = current_f1\n",
        "        best_trainer = trainer\n",
        "        best_experiment_idx = i\n",
        "    mlflow.end_run()\n",
        "df = pd.DataFrame(excel_results)\n",
        "df.to_excel(\"results.xlsx\", index=False)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0c6f58",
      "metadata": {
        "id": "3e0c6f58"
      },
      "source": [
        "Visuazize the train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "740b2e48",
      "metadata": {
        "id": "740b2e48"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "history = best_trainer.history\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"],   label=\"Val Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
        "plt.plot(history[\"val_acc\"],   label=\"Val Acc\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5kjoDILMt2Zw",
      "metadata": {
        "id": "5kjoDILMt2Zw"
      },
      "source": [
        "Visualized the test (do change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "A-FR-NYgt6Hv",
      "metadata": {
        "id": "A-FR-NYgt6Hv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/var/folders/11/82r2_0991njg6n8j1xjz5kmw0000gn/T/ipykernel_35210/2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ† Test Loss: 0.5114 | Test Accuracy: 0.8553\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9055    0.8720    0.8884      1000\n",
            "           1     0.8150    0.8370    0.8259      1000\n",
            "           2     0.7979    0.8170    0.8073      1000\n",
            "           3     0.8810    0.9180    0.8991      1000\n",
            "           4     0.8311    0.8020    0.8163      1000\n",
            "           5     0.9159    0.8490    0.8812      1000\n",
            "           6     0.8352    0.8920    0.8627      1000\n",
            "           7     0.8941    0.8360    0.8641      1000\n",
            "           8     0.8011    0.8900    0.8432      1000\n",
            "           9     0.8955    0.8400    0.8669      1000\n",
            "\n",
            "    accuracy                         0.8553     10000\n",
            "   macro avg     0.8572    0.8553    0.8555     10000\n",
            "weighted avg     0.8572    0.8553    0.8555     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = best_trainer.evaluate(test_loader)\n",
        "print(f\"ðŸ† Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "preds, targets = best_trainer.predict_all(test_loader)\n",
        "\n",
        "cm = confusion_matrix(targets, preds)\n",
        "plt.figure()\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix (Test)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print(\"Classification report (Test):\")\n",
        "print(classification_report(targets, preds, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
