{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Libraries (don't change)"
      ],
      "metadata": {
        "id": "Q3S5loqBCqgM"
      },
      "id": "Q3S5loqBCqgM"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5f1ba5ec",
      "metadata": {
        "id": "5f1ba5ec"
      },
      "outputs": [],
      "source": [
        "!pip -q install torchinfo\n",
        "!pip -q install mlflow\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Callable, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchinfo import summary\n",
        "import mlflow\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Device (don't change)"
      ],
      "metadata": {
        "id": "ykrfYnc6g1Bs"
      },
      "id": "ykrfYnc6g1Bs"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Mixed precision (AMP): {use_amp}\")"
      ],
      "metadata": {
        "id": "XJIYaWIggSCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ca0bcd-5037-4d74-8334-7bd89e22ba29"
      },
      "id": "XJIYaWIggSCp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Mixed precision (AMP): False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71ffe61",
      "metadata": {
        "id": "d71ffe61"
      },
      "source": [
        "Data (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2cb0bf0c",
      "metadata": {
        "id": "2cb0bf0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataManager:\n",
        "    def __init__(self, dataset_class, root: str = \"./data\", val_fraction: float = 0.1,\n",
        "                 batch_size: int = 32, seed: int = 42):\n",
        "        self.dataset_class = dataset_class\n",
        "        self.root = root\n",
        "        self.val_fraction = val_fraction\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1918,), (0.3483,))\n",
        "        ])\n",
        "\n",
        "    def get_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "        full_train = self.dataset_class(root=self.root, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "        test_ds = self.dataset_class(root=self.root, train=False,\n",
        "                                     download=True, transform=self.transform)\n",
        "\n",
        "        val_size = int(len(full_train) * self.val_fraction)\n",
        "        train_size = len(full_train) - val_size\n",
        "\n",
        "        generator = torch.Generator().manual_seed(self.seed)\n",
        "        train_ds, val_ds = random_split(full_train, [train_size, val_size], generator=generator)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=self.batch_size,\n",
        "                                  shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader   = DataLoader(val_ds,   batch_size=self.batch_size,\n",
        "                                  shuffle=False, num_workers=2, pin_memory=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=self.batch_size,\n",
        "                                  shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
        "        return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurations (don't change)"
      ],
      "metadata": {
        "id": "O0SlvxE36HG0"
      },
      "id": "O0SlvxE36HG0"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class LayerSpec:\n",
        "    out_dim: int\n",
        "    activation: Callable[[torch.Tensor], torch.Tensor] = F.relu\n",
        "    dropout: float = 0.0\n",
        "    batch_norm: bool = True\n",
        "    weight_decay: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    input_dim: Tuple[int, int, int] = (1, 28, 28)\n",
        "    num_classes: int = 10\n",
        "    layers: List[LayerSpec] = None\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 100\n",
        "    lr: float = 1e-4\n",
        "    patience: int = 15\n",
        "    min_delta: float = 1e-4\n",
        "    val_fraction: float = 0.1\n",
        "    seed: int = 42\n"
      ],
      "metadata": {
        "id": "FwDGf9J66bXL"
      },
      "id": "FwDGf9J66bXL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "53221445",
      "metadata": {
        "id": "53221445"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b5843387",
      "metadata": {
        "id": "b5843387"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPFromConfig(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        flat_dim = config.input_dim[0] * config.input_dim[1] * config.input_dim[2]\n",
        "        self.layers_specs = config.layers\n",
        "        layers = []\n",
        "        prev_dim = flat_dim\n",
        "\n",
        "        for i, spec in enumerate(config.layers):\n",
        "            linear = nn.Linear(prev_dim, spec.out_dim)\n",
        "\n",
        "            layers.append(linear)\n",
        "            if spec.batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(spec.out_dim))\n",
        "            if spec.dropout > 0:\n",
        "                layers.append(nn.Dropout(spec.dropout))\n",
        "            layers.append(spec.activation())\n",
        "            prev_dim = spec.out_dim\n",
        "\n",
        "        # Final classifier layer\n",
        "        self.final_linear = nn.Linear(prev_dim, config.num_classes)\n",
        "        layers.append(self.final_linear)\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "    def get_layer_params(self):\n",
        "        param_groups = []\n",
        "        for i, spec in enumerate(self.layers_specs):\n",
        "            linear_layer = self.net[i * (4 if spec.batch_norm or spec.dropout > 0 else 3)]\n",
        "            pass\n",
        "        return self.layers_specs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping (don't change)"
      ],
      "metadata": {
        "id": "eND74vML5XYh"
      },
      "id": "eND74vML5XYh"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 10, min_delta: float = 1e-4):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        return self.should_stop"
      ],
      "metadata": {
        "id": "KgImYRwI5hAr"
      },
      "id": "KgImYRwI5hAr",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9b11a04f",
      "metadata": {
        "id": "9b11a04f"
      },
      "source": [
        "Trainer (don't change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e474713f",
      "metadata": {
        "id": "e474713f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module, config: TrainConfig):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.config = config\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = self._build_optimizer()\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "        self.early_stopping = EarlyStopping(patience=config.patience,\n",
        "                                            min_delta=config.min_delta)\n",
        "\n",
        "        self.history = {\"train_loss\": [], \"train_acc\": [],\n",
        "                        \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    def _build_optimizer(self):\n",
        "\n",
        "        # Collect all Linear layers in the order they appear\n",
        "        linear_layers = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                linear_layers.append((name, module))\n",
        "\n",
        "        param_groups = []\n",
        "\n",
        "        for i, spec in enumerate(self.model.layers_specs):\n",
        "            name, layer = linear_layers[i]\n",
        "            param_groups.append({\n",
        "                'params': layer.parameters(),\n",
        "                'weight_decay': spec.weight_decay\n",
        "            })\n",
        "\n",
        "        final_name, final_layer = linear_layers[-1]\n",
        "        param_groups.append({\n",
        "            'params': final_layer.parameters(),\n",
        "            'weight_decay': 0.0\n",
        "        })\n",
        "\n",
        "        return torch.optim.SGD(param_groups, momentum=0.9, nesterov=True, lr=self.config.lr)\n",
        "\n",
        "    def _train_epoch(self, loader: DataLoader):\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            correct += (output.argmax(1) == target).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "        return total_loss / total, correct / total\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _eval_epoch(self, loader: DataLoader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            correct += (output.argmax(1) == target).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "        return total_loss / total, correct / total\n",
        "\n",
        "    def fit(self, train_loader: DataLoader, val_loader: DataLoader):\n",
        "        print(\"ðŸš€ Starting training...\\n\")\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "            train_loss, train_acc = self._train_epoch(train_loader)\n",
        "            val_loss, val_acc     = self._eval_epoch(val_loader)\n",
        "\n",
        "            self.history[\"train_loss\"].append(train_loss)\n",
        "            self.history[\"train_acc\"].append(train_acc)\n",
        "            self.history[\"val_loss\"].append(val_loss)\n",
        "            self.history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "            print(f\"Epoch {epoch:3d} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "            if self.early_stopping(val_loss):\n",
        "                print(f\"\\nðŸ›‘ Early stopping triggered at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(\"\\nâœ… Training complete!\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, loader: DataLoader):\n",
        "        return self._eval_epoch(loader)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_all(self, loader: DataLoader):\n",
        "        self.model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            logits = self.model(x)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(y.numpy())\n",
        "        return np.concatenate(all_preds), np.concatenate(all_targets)\n",
        "\n",
        "\n",
        "    def save(self, path: str = \"mlp_best.pt\"):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"ðŸ’¾ Model saved to {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c7d02d",
      "metadata": {
        "id": "55c7d02d"
      },
      "source": [
        "\n",
        "Run (do change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d133bb1a",
      "metadata": {
        "id": "d133bb1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c40b6f-58a9-4269-ccad-6ef818d21fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 54000 | Val: 6000 | Test: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/01/14 09:50:29 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2026/01/14 09:50:29 INFO mlflow.store.db.utils: Updating database tables\n",
            "2026/01/14 09:50:29 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/14 09:50:29 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2026/01/14 09:50:29 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/14 09:50:29 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "/tmp/ipython-input-2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train Loss: 2.2145 Acc: 0.2194 | Val Loss: 2.1455 Acc: 0.2862\n",
            "Epoch   2 | Train Loss: 2.1030 Acc: 0.2919 | Val Loss: 2.0569 Acc: 0.3257\n",
            "Epoch   3 | Train Loss: 2.0282 Acc: 0.3173 | Val Loss: 1.9830 Acc: 0.3470\n",
            "Epoch   4 | Train Loss: 1.9612 Acc: 0.3369 | Val Loss: 1.9130 Acc: 0.3663\n",
            "Epoch   5 | Train Loss: 1.8995 Acc: 0.3545 | Val Loss: 1.8487 Acc: 0.3833\n",
            "Epoch   6 | Train Loss: 1.8411 Acc: 0.3736 | Val Loss: 1.7890 Acc: 0.4112\n",
            "Epoch   7 | Train Loss: 1.7846 Acc: 0.3953 | Val Loss: 1.7311 Acc: 0.4405\n",
            "Epoch   8 | Train Loss: 1.7269 Acc: 0.4252 | Val Loss: 1.6644 Acc: 0.4805\n",
            "Epoch   9 | Train Loss: 1.6736 Acc: 0.4522 | Val Loss: 1.6092 Acc: 0.5075\n",
            "Epoch  10 | Train Loss: 1.6231 Acc: 0.4789 | Val Loss: 1.5531 Acc: 0.5332\n",
            "Epoch  11 | Train Loss: 1.5710 Acc: 0.5025 | Val Loss: 1.4983 Acc: 0.5575\n",
            "Epoch  12 | Train Loss: 1.5227 Acc: 0.5212 | Val Loss: 1.4493 Acc: 0.5728\n",
            "Epoch  13 | Train Loss: 1.4806 Acc: 0.5342 | Val Loss: 1.3978 Acc: 0.5957\n",
            "Epoch  14 | Train Loss: 1.4369 Acc: 0.5508 | Val Loss: 1.3543 Acc: 0.6075\n",
            "Epoch  15 | Train Loss: 1.3976 Acc: 0.5644 | Val Loss: 1.3102 Acc: 0.6250\n",
            "Epoch  16 | Train Loss: 1.3596 Acc: 0.5786 | Val Loss: 1.2737 Acc: 0.6407\n",
            "Epoch  17 | Train Loss: 1.3269 Acc: 0.5898 | Val Loss: 1.2374 Acc: 0.6578\n",
            "Epoch  18 | Train Loss: 1.2915 Acc: 0.6039 | Val Loss: 1.1981 Acc: 0.6725\n",
            "Epoch  19 | Train Loss: 1.2611 Acc: 0.6163 | Val Loss: 1.1662 Acc: 0.6880\n",
            "Epoch  20 | Train Loss: 1.2303 Acc: 0.6293 | Val Loss: 1.1373 Acc: 0.7002\n",
            "Epoch  21 | Train Loss: 1.2033 Acc: 0.6411 | Val Loss: 1.1061 Acc: 0.7155\n",
            "Epoch  22 | Train Loss: 1.1779 Acc: 0.6487 | Val Loss: 1.0780 Acc: 0.7212\n",
            "Epoch  23 | Train Loss: 1.1514 Acc: 0.6595 | Val Loss: 1.0509 Acc: 0.7305\n",
            "Epoch  24 | Train Loss: 1.1294 Acc: 0.6662 | Val Loss: 1.0247 Acc: 0.7408\n",
            "Epoch  25 | Train Loss: 1.1029 Acc: 0.6766 | Val Loss: 1.0036 Acc: 0.7487\n",
            "Epoch  26 | Train Loss: 1.0809 Acc: 0.6843 | Val Loss: 0.9812 Acc: 0.7512\n",
            "Epoch  27 | Train Loss: 1.0620 Acc: 0.6879 | Val Loss: 0.9531 Acc: 0.7597\n",
            "Epoch  28 | Train Loss: 1.0415 Acc: 0.6931 | Val Loss: 0.9381 Acc: 0.7650\n",
            "Epoch  29 | Train Loss: 1.0241 Acc: 0.6988 | Val Loss: 0.9167 Acc: 0.7693\n",
            "Epoch  30 | Train Loss: 1.0085 Acc: 0.7015 | Val Loss: 0.8945 Acc: 0.7717\n",
            "Epoch  31 | Train Loss: 0.9923 Acc: 0.7068 | Val Loss: 0.8793 Acc: 0.7780\n",
            "Epoch  32 | Train Loss: 0.9749 Acc: 0.7120 | Val Loss: 0.8654 Acc: 0.7805\n",
            "Epoch  33 | Train Loss: 0.9614 Acc: 0.7137 | Val Loss: 0.8491 Acc: 0.7835\n",
            "Epoch  34 | Train Loss: 0.9475 Acc: 0.7179 | Val Loss: 0.8343 Acc: 0.7885\n",
            "Epoch  35 | Train Loss: 0.9332 Acc: 0.7233 | Val Loss: 0.8212 Acc: 0.7893\n",
            "Epoch  36 | Train Loss: 0.9176 Acc: 0.7281 | Val Loss: 0.8042 Acc: 0.7955\n",
            "Epoch  37 | Train Loss: 0.9078 Acc: 0.7305 | Val Loss: 0.7907 Acc: 0.7972\n",
            "Epoch  38 | Train Loss: 0.8958 Acc: 0.7325 | Val Loss: 0.7795 Acc: 0.8022\n",
            "Epoch  39 | Train Loss: 0.8813 Acc: 0.7390 | Val Loss: 0.7685 Acc: 0.8070\n",
            "Epoch  40 | Train Loss: 0.8768 Acc: 0.7387 | Val Loss: 0.7567 Acc: 0.8068\n",
            "Epoch  41 | Train Loss: 0.8611 Acc: 0.7432 | Val Loss: 0.7437 Acc: 0.8093\n",
            "Epoch  42 | Train Loss: 0.8553 Acc: 0.7437 | Val Loss: 0.7340 Acc: 0.8097\n",
            "Epoch  43 | Train Loss: 0.8443 Acc: 0.7469 | Val Loss: 0.7244 Acc: 0.8128\n",
            "Epoch  44 | Train Loss: 0.8350 Acc: 0.7503 | Val Loss: 0.7144 Acc: 0.8143\n",
            "Epoch  45 | Train Loss: 0.8284 Acc: 0.7535 | Val Loss: 0.7064 Acc: 0.8157\n",
            "Epoch  46 | Train Loss: 0.8153 Acc: 0.7554 | Val Loss: 0.6977 Acc: 0.8157\n",
            "Epoch  47 | Train Loss: 0.8092 Acc: 0.7581 | Val Loss: 0.6884 Acc: 0.8188\n",
            "Epoch  48 | Train Loss: 0.8006 Acc: 0.7596 | Val Loss: 0.6827 Acc: 0.8220\n",
            "Epoch  49 | Train Loss: 0.7916 Acc: 0.7619 | Val Loss: 0.6730 Acc: 0.8230\n",
            "Epoch  50 | Train Loss: 0.7857 Acc: 0.7634 | Val Loss: 0.6635 Acc: 0.8265\n",
            "Epoch  51 | Train Loss: 0.7731 Acc: 0.7678 | Val Loss: 0.6561 Acc: 0.8263\n",
            "Epoch  52 | Train Loss: 0.7664 Acc: 0.7694 | Val Loss: 0.6506 Acc: 0.8287\n",
            "Epoch  53 | Train Loss: 0.7628 Acc: 0.7709 | Val Loss: 0.6422 Acc: 0.8308\n",
            "Epoch  54 | Train Loss: 0.7549 Acc: 0.7726 | Val Loss: 0.6373 Acc: 0.8318\n",
            "Epoch  55 | Train Loss: 0.7464 Acc: 0.7751 | Val Loss: 0.6322 Acc: 0.8333\n",
            "Epoch  56 | Train Loss: 0.7410 Acc: 0.7781 | Val Loss: 0.6246 Acc: 0.8342\n",
            "Epoch  57 | Train Loss: 0.7359 Acc: 0.7772 | Val Loss: 0.6194 Acc: 0.8358\n",
            "Epoch  58 | Train Loss: 0.7302 Acc: 0.7810 | Val Loss: 0.6144 Acc: 0.8360\n",
            "Epoch  59 | Train Loss: 0.7221 Acc: 0.7830 | Val Loss: 0.6107 Acc: 0.8357\n",
            "Epoch  60 | Train Loss: 0.7188 Acc: 0.7839 | Val Loss: 0.6017 Acc: 0.8393\n",
            "Epoch  61 | Train Loss: 0.7164 Acc: 0.7826 | Val Loss: 0.5970 Acc: 0.8375\n",
            "Epoch  62 | Train Loss: 0.7082 Acc: 0.7861 | Val Loss: 0.5930 Acc: 0.8378\n",
            "Epoch  63 | Train Loss: 0.7033 Acc: 0.7879 | Val Loss: 0.5892 Acc: 0.8413\n",
            "Epoch  64 | Train Loss: 0.6999 Acc: 0.7877 | Val Loss: 0.5818 Acc: 0.8435\n",
            "Epoch  65 | Train Loss: 0.6926 Acc: 0.7915 | Val Loss: 0.5794 Acc: 0.8427\n",
            "Epoch  66 | Train Loss: 0.6882 Acc: 0.7921 | Val Loss: 0.5736 Acc: 0.8442\n",
            "Epoch  67 | Train Loss: 0.6865 Acc: 0.7914 | Val Loss: 0.5696 Acc: 0.8460\n",
            "Epoch  68 | Train Loss: 0.6824 Acc: 0.7926 | Val Loss: 0.5668 Acc: 0.8473\n",
            "Epoch  69 | Train Loss: 0.6720 Acc: 0.7960 | Val Loss: 0.5617 Acc: 0.8468\n",
            "Epoch  70 | Train Loss: 0.6685 Acc: 0.7981 | Val Loss: 0.5587 Acc: 0.8482\n",
            "Epoch  71 | Train Loss: 0.6658 Acc: 0.7986 | Val Loss: 0.5535 Acc: 0.8490\n",
            "Epoch  72 | Train Loss: 0.6617 Acc: 0.7989 | Val Loss: 0.5501 Acc: 0.8498\n",
            "Epoch  73 | Train Loss: 0.6584 Acc: 0.8015 | Val Loss: 0.5440 Acc: 0.8503\n",
            "Epoch  74 | Train Loss: 0.6519 Acc: 0.8023 | Val Loss: 0.5403 Acc: 0.8525\n",
            "Epoch  75 | Train Loss: 0.6470 Acc: 0.8048 | Val Loss: 0.5410 Acc: 0.8523\n",
            "Epoch  76 | Train Loss: 0.6455 Acc: 0.8033 | Val Loss: 0.5349 Acc: 0.8525\n",
            "Epoch  77 | Train Loss: 0.6454 Acc: 0.8043 | Val Loss: 0.5326 Acc: 0.8532\n",
            "Epoch  78 | Train Loss: 0.6383 Acc: 0.8059 | Val Loss: 0.5316 Acc: 0.8557\n",
            "Epoch  79 | Train Loss: 0.6376 Acc: 0.8071 | Val Loss: 0.5267 Acc: 0.8563\n",
            "Epoch  80 | Train Loss: 0.6303 Acc: 0.8096 | Val Loss: 0.5216 Acc: 0.8563\n",
            "Epoch  81 | Train Loss: 0.6274 Acc: 0.8103 | Val Loss: 0.5169 Acc: 0.8585\n",
            "Epoch  82 | Train Loss: 0.6223 Acc: 0.8130 | Val Loss: 0.5166 Acc: 0.8583\n",
            "Epoch  83 | Train Loss: 0.6203 Acc: 0.8115 | Val Loss: 0.5124 Acc: 0.8588\n",
            "Epoch  84 | Train Loss: 0.6165 Acc: 0.8139 | Val Loss: 0.5122 Acc: 0.8583\n",
            "Epoch  85 | Train Loss: 0.6115 Acc: 0.8141 | Val Loss: 0.5076 Acc: 0.8605\n",
            "Epoch  86 | Train Loss: 0.6087 Acc: 0.8154 | Val Loss: 0.5054 Acc: 0.8613\n",
            "Epoch  87 | Train Loss: 0.6067 Acc: 0.8157 | Val Loss: 0.5013 Acc: 0.8623\n",
            "Epoch  88 | Train Loss: 0.6058 Acc: 0.8169 | Val Loss: 0.4999 Acc: 0.8622\n",
            "Epoch  89 | Train Loss: 0.6016 Acc: 0.8185 | Val Loss: 0.4953 Acc: 0.8617\n",
            "Epoch  90 | Train Loss: 0.5977 Acc: 0.8182 | Val Loss: 0.4942 Acc: 0.8623\n",
            "Epoch  91 | Train Loss: 0.5936 Acc: 0.8195 | Val Loss: 0.4929 Acc: 0.8633\n",
            "Epoch  92 | Train Loss: 0.5937 Acc: 0.8194 | Val Loss: 0.4914 Acc: 0.8638\n",
            "Epoch  93 | Train Loss: 0.5918 Acc: 0.8199 | Val Loss: 0.4866 Acc: 0.8627\n",
            "Epoch  94 | Train Loss: 0.5828 Acc: 0.8231 | Val Loss: 0.4854 Acc: 0.8653\n",
            "Epoch  95 | Train Loss: 0.5813 Acc: 0.8226 | Val Loss: 0.4823 Acc: 0.8653\n",
            "Epoch  96 | Train Loss: 0.5814 Acc: 0.8240 | Val Loss: 0.4797 Acc: 0.8670\n",
            "Epoch  97 | Train Loss: 0.5760 Acc: 0.8244 | Val Loss: 0.4787 Acc: 0.8660\n",
            "Epoch  98 | Train Loss: 0.5783 Acc: 0.8236 | Val Loss: 0.4769 Acc: 0.8665\n",
            "Epoch  99 | Train Loss: 0.5723 Acc: 0.8274 | Val Loss: 0.4744 Acc: 0.8685\n",
            "Epoch 100 | Train Loss: 0.5719 Acc: 0.8270 | Val Loss: 0.4732 Acc: 0.8670\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_0.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train Loss: 2.2833 Acc: 0.1298 | Val Loss: 2.2320 Acc: 0.1948\n",
            "Epoch   2 | Train Loss: 2.1871 Acc: 0.2460 | Val Loss: 2.1342 Acc: 0.3167\n",
            "Epoch   3 | Train Loss: 2.0922 Acc: 0.3361 | Val Loss: 2.0365 Acc: 0.4012\n",
            "Epoch   4 | Train Loss: 1.9985 Acc: 0.4210 | Val Loss: 1.9423 Acc: 0.4932\n",
            "Epoch   5 | Train Loss: 1.9089 Acc: 0.4619 | Val Loss: 1.8495 Acc: 0.5262\n",
            "Epoch   6 | Train Loss: 1.8204 Acc: 0.4841 | Val Loss: 1.7542 Acc: 0.5563\n",
            "Epoch   7 | Train Loss: 1.7342 Acc: 0.5110 | Val Loss: 1.6621 Acc: 0.5953\n",
            "Epoch   8 | Train Loss: 1.6474 Acc: 0.5434 | Val Loss: 1.5697 Acc: 0.6253\n",
            "Epoch   9 | Train Loss: 1.5660 Acc: 0.5706 | Val Loss: 1.4835 Acc: 0.6517\n",
            "Epoch  10 | Train Loss: 1.4931 Acc: 0.5894 | Val Loss: 1.3969 Acc: 0.6738\n",
            "Epoch  11 | Train Loss: 1.4224 Acc: 0.6070 | Val Loss: 1.3242 Acc: 0.6928\n",
            "Epoch  12 | Train Loss: 1.3590 Acc: 0.6198 | Val Loss: 1.2556 Acc: 0.7023\n",
            "Epoch  13 | Train Loss: 1.2999 Acc: 0.6338 | Val Loss: 1.1952 Acc: 0.7138\n",
            "Epoch  14 | Train Loss: 1.2516 Acc: 0.6444 | Val Loss: 1.1392 Acc: 0.7263\n",
            "Epoch  15 | Train Loss: 1.2069 Acc: 0.6526 | Val Loss: 1.0938 Acc: 0.7300\n",
            "Epoch  16 | Train Loss: 1.1692 Acc: 0.6582 | Val Loss: 1.0548 Acc: 0.7368\n",
            "Epoch  17 | Train Loss: 1.1327 Acc: 0.6622 | Val Loss: 1.0153 Acc: 0.7458\n",
            "Epoch  18 | Train Loss: 1.0983 Acc: 0.6720 | Val Loss: 0.9813 Acc: 0.7502\n",
            "Epoch  19 | Train Loss: 1.0736 Acc: 0.6771 | Val Loss: 0.9501 Acc: 0.7565\n",
            "Epoch  20 | Train Loss: 1.0418 Acc: 0.6860 | Val Loss: 0.9165 Acc: 0.7613\n",
            "Epoch  21 | Train Loss: 1.0170 Acc: 0.6916 | Val Loss: 0.8943 Acc: 0.7667\n",
            "Epoch  22 | Train Loss: 0.9933 Acc: 0.6970 | Val Loss: 0.8679 Acc: 0.7730\n",
            "Epoch  23 | Train Loss: 0.9682 Acc: 0.7052 | Val Loss: 0.8424 Acc: 0.7780\n",
            "Epoch  24 | Train Loss: 0.9506 Acc: 0.7095 | Val Loss: 0.8182 Acc: 0.7837\n",
            "Epoch  25 | Train Loss: 0.9322 Acc: 0.7144 | Val Loss: 0.8008 Acc: 0.7887\n",
            "Epoch  26 | Train Loss: 0.9148 Acc: 0.7183 | Val Loss: 0.7846 Acc: 0.7923\n",
            "Epoch  27 | Train Loss: 0.8939 Acc: 0.7271 | Val Loss: 0.7647 Acc: 0.7975\n",
            "Epoch  28 | Train Loss: 0.8779 Acc: 0.7321 | Val Loss: 0.7491 Acc: 0.8020\n",
            "Epoch  29 | Train Loss: 0.8577 Acc: 0.7380 | Val Loss: 0.7293 Acc: 0.8058\n",
            "Epoch  30 | Train Loss: 0.8503 Acc: 0.7386 | Val Loss: 0.7173 Acc: 0.8077\n",
            "Epoch  31 | Train Loss: 0.8350 Acc: 0.7431 | Val Loss: 0.7023 Acc: 0.8128\n",
            "Epoch  32 | Train Loss: 0.8192 Acc: 0.7488 | Val Loss: 0.6900 Acc: 0.8138\n",
            "Epoch  33 | Train Loss: 0.8074 Acc: 0.7521 | Val Loss: 0.6734 Acc: 0.8182\n",
            "Epoch  34 | Train Loss: 0.8024 Acc: 0.7527 | Val Loss: 0.6635 Acc: 0.8227\n",
            "Epoch  35 | Train Loss: 0.7859 Acc: 0.7608 | Val Loss: 0.6577 Acc: 0.8253\n",
            "Epoch  36 | Train Loss: 0.7753 Acc: 0.7634 | Val Loss: 0.6436 Acc: 0.8295\n",
            "Epoch  37 | Train Loss: 0.7636 Acc: 0.7657 | Val Loss: 0.6375 Acc: 0.8297\n",
            "Epoch  38 | Train Loss: 0.7572 Acc: 0.7670 | Val Loss: 0.6224 Acc: 0.8355\n",
            "Epoch  39 | Train Loss: 0.7490 Acc: 0.7697 | Val Loss: 0.6139 Acc: 0.8362\n",
            "Epoch  40 | Train Loss: 0.7372 Acc: 0.7737 | Val Loss: 0.6090 Acc: 0.8408\n",
            "Epoch  41 | Train Loss: 0.7287 Acc: 0.7753 | Val Loss: 0.5974 Acc: 0.8425\n",
            "Epoch  42 | Train Loss: 0.7208 Acc: 0.7772 | Val Loss: 0.5897 Acc: 0.8447\n",
            "Epoch  43 | Train Loss: 0.7152 Acc: 0.7807 | Val Loss: 0.5822 Acc: 0.8482\n",
            "Epoch  44 | Train Loss: 0.7007 Acc: 0.7842 | Val Loss: 0.5752 Acc: 0.8497\n",
            "Epoch  45 | Train Loss: 0.6998 Acc: 0.7823 | Val Loss: 0.5673 Acc: 0.8547\n",
            "Epoch  46 | Train Loss: 0.6870 Acc: 0.7877 | Val Loss: 0.5622 Acc: 0.8548\n",
            "Epoch  47 | Train Loss: 0.6849 Acc: 0.7891 | Val Loss: 0.5531 Acc: 0.8563\n",
            "Epoch  48 | Train Loss: 0.6798 Acc: 0.7904 | Val Loss: 0.5486 Acc: 0.8575\n",
            "Epoch  49 | Train Loss: 0.6670 Acc: 0.7937 | Val Loss: 0.5395 Acc: 0.8602\n",
            "Epoch  50 | Train Loss: 0.6644 Acc: 0.7934 | Val Loss: 0.5359 Acc: 0.8615\n",
            "Epoch  51 | Train Loss: 0.6593 Acc: 0.7957 | Val Loss: 0.5327 Acc: 0.8617\n",
            "Epoch  52 | Train Loss: 0.6512 Acc: 0.7984 | Val Loss: 0.5237 Acc: 0.8630\n",
            "Epoch  53 | Train Loss: 0.6471 Acc: 0.7993 | Val Loss: 0.5197 Acc: 0.8617\n",
            "Epoch  54 | Train Loss: 0.6380 Acc: 0.8039 | Val Loss: 0.5153 Acc: 0.8647\n",
            "Epoch  55 | Train Loss: 0.6366 Acc: 0.8024 | Val Loss: 0.5113 Acc: 0.8643\n",
            "Epoch  56 | Train Loss: 0.6269 Acc: 0.8067 | Val Loss: 0.5059 Acc: 0.8650\n",
            "Epoch  57 | Train Loss: 0.6219 Acc: 0.8073 | Val Loss: 0.5015 Acc: 0.8667\n",
            "Epoch  58 | Train Loss: 0.6174 Acc: 0.8079 | Val Loss: 0.4950 Acc: 0.8692\n",
            "Epoch  59 | Train Loss: 0.6130 Acc: 0.8093 | Val Loss: 0.4897 Acc: 0.8688\n",
            "Epoch  60 | Train Loss: 0.6108 Acc: 0.8109 | Val Loss: 0.4875 Acc: 0.8700\n",
            "Epoch  61 | Train Loss: 0.6065 Acc: 0.8097 | Val Loss: 0.4829 Acc: 0.8697\n",
            "Epoch  62 | Train Loss: 0.5957 Acc: 0.8141 | Val Loss: 0.4788 Acc: 0.8707\n",
            "Epoch  63 | Train Loss: 0.5964 Acc: 0.8153 | Val Loss: 0.4746 Acc: 0.8718\n",
            "Epoch  64 | Train Loss: 0.5904 Acc: 0.8166 | Val Loss: 0.4715 Acc: 0.8735\n",
            "Epoch  65 | Train Loss: 0.5882 Acc: 0.8169 | Val Loss: 0.4661 Acc: 0.8753\n",
            "Epoch  66 | Train Loss: 0.5832 Acc: 0.8201 | Val Loss: 0.4636 Acc: 0.8742\n",
            "Epoch  67 | Train Loss: 0.5759 Acc: 0.8233 | Val Loss: 0.4603 Acc: 0.8767\n",
            "Epoch  68 | Train Loss: 0.5700 Acc: 0.8251 | Val Loss: 0.4573 Acc: 0.8760\n",
            "Epoch  69 | Train Loss: 0.5681 Acc: 0.8259 | Val Loss: 0.4522 Acc: 0.8757\n",
            "Epoch  70 | Train Loss: 0.5651 Acc: 0.8265 | Val Loss: 0.4515 Acc: 0.8768\n",
            "Epoch  71 | Train Loss: 0.5611 Acc: 0.8269 | Val Loss: 0.4473 Acc: 0.8778\n",
            "Epoch  72 | Train Loss: 0.5574 Acc: 0.8288 | Val Loss: 0.4449 Acc: 0.8778\n",
            "Epoch  73 | Train Loss: 0.5518 Acc: 0.8308 | Val Loss: 0.4407 Acc: 0.8782\n",
            "Epoch  74 | Train Loss: 0.5492 Acc: 0.8314 | Val Loss: 0.4370 Acc: 0.8795\n",
            "Epoch  75 | Train Loss: 0.5458 Acc: 0.8333 | Val Loss: 0.4356 Acc: 0.8805\n",
            "Epoch  76 | Train Loss: 0.5470 Acc: 0.8316 | Val Loss: 0.4344 Acc: 0.8817\n",
            "Epoch  77 | Train Loss: 0.5360 Acc: 0.8374 | Val Loss: 0.4288 Acc: 0.8822\n",
            "Epoch  78 | Train Loss: 0.5369 Acc: 0.8368 | Val Loss: 0.4274 Acc: 0.8813\n",
            "Epoch  79 | Train Loss: 0.5323 Acc: 0.8371 | Val Loss: 0.4241 Acc: 0.8835\n",
            "Epoch  80 | Train Loss: 0.5285 Acc: 0.8388 | Val Loss: 0.4232 Acc: 0.8837\n",
            "Epoch  81 | Train Loss: 0.5255 Acc: 0.8387 | Val Loss: 0.4183 Acc: 0.8850\n",
            "Epoch  82 | Train Loss: 0.5236 Acc: 0.8403 | Val Loss: 0.4180 Acc: 0.8835\n",
            "Epoch  83 | Train Loss: 0.5163 Acc: 0.8425 | Val Loss: 0.4140 Acc: 0.8860\n",
            "Epoch  84 | Train Loss: 0.5159 Acc: 0.8438 | Val Loss: 0.4113 Acc: 0.8860\n",
            "Epoch  85 | Train Loss: 0.5154 Acc: 0.8431 | Val Loss: 0.4103 Acc: 0.8867\n",
            "Epoch  86 | Train Loss: 0.5087 Acc: 0.8450 | Val Loss: 0.4082 Acc: 0.8867\n",
            "Epoch  87 | Train Loss: 0.5081 Acc: 0.8444 | Val Loss: 0.4047 Acc: 0.8865\n",
            "Epoch  88 | Train Loss: 0.5029 Acc: 0.8476 | Val Loss: 0.4026 Acc: 0.8888\n",
            "Epoch  89 | Train Loss: 0.5048 Acc: 0.8463 | Val Loss: 0.4010 Acc: 0.8873\n",
            "Epoch  90 | Train Loss: 0.4977 Acc: 0.8486 | Val Loss: 0.3979 Acc: 0.8880\n",
            "Epoch  91 | Train Loss: 0.4980 Acc: 0.8488 | Val Loss: 0.3969 Acc: 0.8905\n",
            "Epoch  92 | Train Loss: 0.4915 Acc: 0.8514 | Val Loss: 0.3950 Acc: 0.8907\n",
            "Epoch  93 | Train Loss: 0.4894 Acc: 0.8502 | Val Loss: 0.3934 Acc: 0.8903\n",
            "Epoch  94 | Train Loss: 0.4891 Acc: 0.8518 | Val Loss: 0.3918 Acc: 0.8905\n",
            "Epoch  95 | Train Loss: 0.4855 Acc: 0.8527 | Val Loss: 0.3876 Acc: 0.8923\n",
            "Epoch  96 | Train Loss: 0.4815 Acc: 0.8541 | Val Loss: 0.3863 Acc: 0.8918\n",
            "Epoch  97 | Train Loss: 0.4800 Acc: 0.8554 | Val Loss: 0.3847 Acc: 0.8923\n",
            "Epoch  98 | Train Loss: 0.4749 Acc: 0.8563 | Val Loss: 0.3831 Acc: 0.8918\n",
            "Epoch  99 | Train Loss: 0.4730 Acc: 0.8590 | Val Loss: 0.3821 Acc: 0.8940\n",
            "Epoch 100 | Train Loss: 0.4733 Acc: 0.8568 | Val Loss: 0.3792 Acc: 0.8942\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_1.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train Loss: 2.2834 Acc: 0.1354 | Val Loss: 2.2058 Acc: 0.2098\n",
            "Epoch   2 | Train Loss: 2.1442 Acc: 0.2564 | Val Loss: 2.0681 Acc: 0.3355\n",
            "Epoch   3 | Train Loss: 2.0252 Acc: 0.3379 | Val Loss: 1.9499 Acc: 0.3893\n",
            "Epoch   4 | Train Loss: 1.9225 Acc: 0.3811 | Val Loss: 1.8469 Acc: 0.4278\n",
            "Epoch   5 | Train Loss: 1.8270 Acc: 0.4174 | Val Loss: 1.7532 Acc: 0.4717\n",
            "Epoch   6 | Train Loss: 1.7415 Acc: 0.4536 | Val Loss: 1.6630 Acc: 0.5078\n",
            "Epoch   7 | Train Loss: 1.6578 Acc: 0.4897 | Val Loss: 1.5759 Acc: 0.5567\n",
            "Epoch   8 | Train Loss: 1.5769 Acc: 0.5293 | Val Loss: 1.4911 Acc: 0.5933\n",
            "Epoch   9 | Train Loss: 1.4977 Acc: 0.5590 | Val Loss: 1.4073 Acc: 0.6255\n",
            "Epoch  10 | Train Loss: 1.4234 Acc: 0.5830 | Val Loss: 1.3323 Acc: 0.6488\n",
            "Epoch  11 | Train Loss: 1.3524 Acc: 0.6039 | Val Loss: 1.2577 Acc: 0.6692\n",
            "Epoch  12 | Train Loss: 1.2924 Acc: 0.6188 | Val Loss: 1.1938 Acc: 0.6850\n",
            "Epoch  13 | Train Loss: 1.2365 Acc: 0.6311 | Val Loss: 1.1345 Acc: 0.6955\n",
            "Epoch  14 | Train Loss: 1.1864 Acc: 0.6446 | Val Loss: 1.0799 Acc: 0.7073\n",
            "Epoch  15 | Train Loss: 1.1399 Acc: 0.6562 | Val Loss: 1.0341 Acc: 0.7187\n",
            "Epoch  16 | Train Loss: 1.1002 Acc: 0.6662 | Val Loss: 0.9953 Acc: 0.7353\n",
            "Epoch  17 | Train Loss: 1.0664 Acc: 0.6750 | Val Loss: 0.9563 Acc: 0.7432\n",
            "Epoch  18 | Train Loss: 1.0318 Acc: 0.6864 | Val Loss: 0.9209 Acc: 0.7543\n",
            "Epoch  19 | Train Loss: 1.0025 Acc: 0.6977 | Val Loss: 0.8905 Acc: 0.7653\n",
            "Epoch  20 | Train Loss: 0.9805 Acc: 0.7048 | Val Loss: 0.8639 Acc: 0.7740\n",
            "Epoch  21 | Train Loss: 0.9533 Acc: 0.7143 | Val Loss: 0.8371 Acc: 0.7808\n",
            "Epoch  22 | Train Loss: 0.9305 Acc: 0.7230 | Val Loss: 0.8147 Acc: 0.7893\n",
            "Epoch  23 | Train Loss: 0.9135 Acc: 0.7248 | Val Loss: 0.7927 Acc: 0.7943\n",
            "Epoch  24 | Train Loss: 0.8921 Acc: 0.7341 | Val Loss: 0.7717 Acc: 0.8018\n",
            "Epoch  25 | Train Loss: 0.8732 Acc: 0.7362 | Val Loss: 0.7518 Acc: 0.8043\n",
            "Epoch  26 | Train Loss: 0.8613 Acc: 0.7400 | Val Loss: 0.7356 Acc: 0.8108\n",
            "Epoch  27 | Train Loss: 0.8429 Acc: 0.7444 | Val Loss: 0.7199 Acc: 0.8150\n",
            "Epoch  28 | Train Loss: 0.8272 Acc: 0.7493 | Val Loss: 0.7057 Acc: 0.8157\n",
            "Epoch  29 | Train Loss: 0.8159 Acc: 0.7529 | Val Loss: 0.6895 Acc: 0.8223\n",
            "Epoch  30 | Train Loss: 0.8017 Acc: 0.7576 | Val Loss: 0.6772 Acc: 0.8222\n",
            "Epoch  31 | Train Loss: 0.7881 Acc: 0.7599 | Val Loss: 0.6614 Acc: 0.8270\n",
            "Epoch  32 | Train Loss: 0.7803 Acc: 0.7613 | Val Loss: 0.6516 Acc: 0.8292\n",
            "Epoch  33 | Train Loss: 0.7658 Acc: 0.7670 | Val Loss: 0.6393 Acc: 0.8332\n",
            "Epoch  34 | Train Loss: 0.7550 Acc: 0.7689 | Val Loss: 0.6274 Acc: 0.8337\n",
            "Epoch  35 | Train Loss: 0.7466 Acc: 0.7714 | Val Loss: 0.6166 Acc: 0.8348\n",
            "Epoch  36 | Train Loss: 0.7356 Acc: 0.7742 | Val Loss: 0.6056 Acc: 0.8395\n",
            "Epoch  37 | Train Loss: 0.7235 Acc: 0.7779 | Val Loss: 0.5982 Acc: 0.8427\n",
            "Epoch  38 | Train Loss: 0.7145 Acc: 0.7805 | Val Loss: 0.5889 Acc: 0.8432\n",
            "Epoch  39 | Train Loss: 0.7079 Acc: 0.7815 | Val Loss: 0.5795 Acc: 0.8465\n",
            "Epoch  40 | Train Loss: 0.6964 Acc: 0.7854 | Val Loss: 0.5702 Acc: 0.8477\n",
            "Epoch  41 | Train Loss: 0.6913 Acc: 0.7867 | Val Loss: 0.5625 Acc: 0.8515\n",
            "Epoch  42 | Train Loss: 0.6792 Acc: 0.7907 | Val Loss: 0.5533 Acc: 0.8518\n",
            "Epoch  43 | Train Loss: 0.6738 Acc: 0.7914 | Val Loss: 0.5474 Acc: 0.8540\n",
            "Epoch  44 | Train Loss: 0.6656 Acc: 0.7943 | Val Loss: 0.5416 Acc: 0.8570\n",
            "Epoch  45 | Train Loss: 0.6588 Acc: 0.7958 | Val Loss: 0.5342 Acc: 0.8598\n",
            "Epoch  46 | Train Loss: 0.6534 Acc: 0.7969 | Val Loss: 0.5263 Acc: 0.8608\n",
            "Epoch  47 | Train Loss: 0.6452 Acc: 0.7995 | Val Loss: 0.5185 Acc: 0.8627\n",
            "Epoch  48 | Train Loss: 0.6386 Acc: 0.8030 | Val Loss: 0.5141 Acc: 0.8630\n",
            "Epoch  49 | Train Loss: 0.6303 Acc: 0.8048 | Val Loss: 0.5078 Acc: 0.8650\n",
            "Epoch  50 | Train Loss: 0.6239 Acc: 0.8071 | Val Loss: 0.5020 Acc: 0.8660\n",
            "Epoch  51 | Train Loss: 0.6199 Acc: 0.8099 | Val Loss: 0.4955 Acc: 0.8678\n",
            "Epoch  52 | Train Loss: 0.6110 Acc: 0.8109 | Val Loss: 0.4923 Acc: 0.8675\n",
            "Epoch  53 | Train Loss: 0.6079 Acc: 0.8101 | Val Loss: 0.4848 Acc: 0.8707\n",
            "Epoch  54 | Train Loss: 0.6013 Acc: 0.8135 | Val Loss: 0.4809 Acc: 0.8718\n",
            "Epoch  55 | Train Loss: 0.5962 Acc: 0.8148 | Val Loss: 0.4736 Acc: 0.8747\n",
            "Epoch  56 | Train Loss: 0.5911 Acc: 0.8170 | Val Loss: 0.4715 Acc: 0.8753\n",
            "Epoch  57 | Train Loss: 0.5859 Acc: 0.8192 | Val Loss: 0.4663 Acc: 0.8773\n",
            "Epoch  58 | Train Loss: 0.5802 Acc: 0.8208 | Val Loss: 0.4623 Acc: 0.8782\n",
            "Epoch  59 | Train Loss: 0.5771 Acc: 0.8203 | Val Loss: 0.4580 Acc: 0.8797\n",
            "Epoch  60 | Train Loss: 0.5702 Acc: 0.8244 | Val Loss: 0.4517 Acc: 0.8797\n",
            "Epoch  61 | Train Loss: 0.5637 Acc: 0.8271 | Val Loss: 0.4487 Acc: 0.8803\n",
            "Epoch  62 | Train Loss: 0.5637 Acc: 0.8248 | Val Loss: 0.4451 Acc: 0.8818\n",
            "Epoch  63 | Train Loss: 0.5518 Acc: 0.8307 | Val Loss: 0.4400 Acc: 0.8847\n",
            "Epoch  64 | Train Loss: 0.5518 Acc: 0.8312 | Val Loss: 0.4348 Acc: 0.8848\n",
            "Epoch  65 | Train Loss: 0.5427 Acc: 0.8350 | Val Loss: 0.4304 Acc: 0.8853\n",
            "Epoch  66 | Train Loss: 0.5395 Acc: 0.8351 | Val Loss: 0.4275 Acc: 0.8878\n",
            "Epoch  67 | Train Loss: 0.5389 Acc: 0.8334 | Val Loss: 0.4247 Acc: 0.8862\n",
            "Epoch  68 | Train Loss: 0.5355 Acc: 0.8355 | Val Loss: 0.4200 Acc: 0.8892\n",
            "Epoch  69 | Train Loss: 0.5299 Acc: 0.8376 | Val Loss: 0.4153 Acc: 0.8900\n",
            "Epoch  70 | Train Loss: 0.5220 Acc: 0.8393 | Val Loss: 0.4145 Acc: 0.8902\n",
            "Epoch  71 | Train Loss: 0.5220 Acc: 0.8398 | Val Loss: 0.4091 Acc: 0.8905\n",
            "Epoch  72 | Train Loss: 0.5138 Acc: 0.8432 | Val Loss: 0.4050 Acc: 0.8923\n",
            "Epoch  73 | Train Loss: 0.5099 Acc: 0.8436 | Val Loss: 0.4026 Acc: 0.8935\n",
            "Epoch  74 | Train Loss: 0.5069 Acc: 0.8437 | Val Loss: 0.3996 Acc: 0.8948\n",
            "Epoch  75 | Train Loss: 0.5041 Acc: 0.8458 | Val Loss: 0.3959 Acc: 0.8947\n",
            "Epoch  76 | Train Loss: 0.4998 Acc: 0.8482 | Val Loss: 0.3942 Acc: 0.8940\n",
            "Epoch  77 | Train Loss: 0.4966 Acc: 0.8477 | Val Loss: 0.3911 Acc: 0.8955\n",
            "Epoch  78 | Train Loss: 0.4936 Acc: 0.8492 | Val Loss: 0.3889 Acc: 0.8972\n",
            "Epoch  79 | Train Loss: 0.4883 Acc: 0.8510 | Val Loss: 0.3867 Acc: 0.8978\n",
            "Epoch  80 | Train Loss: 0.4875 Acc: 0.8507 | Val Loss: 0.3832 Acc: 0.8992\n",
            "Epoch  81 | Train Loss: 0.4823 Acc: 0.8530 | Val Loss: 0.3799 Acc: 0.8983\n",
            "Epoch  82 | Train Loss: 0.4818 Acc: 0.8528 | Val Loss: 0.3760 Acc: 0.9008\n",
            "Epoch  83 | Train Loss: 0.4755 Acc: 0.8554 | Val Loss: 0.3741 Acc: 0.9013\n",
            "Epoch  84 | Train Loss: 0.4772 Acc: 0.8542 | Val Loss: 0.3712 Acc: 0.9020\n",
            "Epoch  85 | Train Loss: 0.4663 Acc: 0.8587 | Val Loss: 0.3696 Acc: 0.9025\n",
            "Epoch  86 | Train Loss: 0.4662 Acc: 0.8578 | Val Loss: 0.3666 Acc: 0.9027\n",
            "Epoch  87 | Train Loss: 0.4619 Acc: 0.8594 | Val Loss: 0.3646 Acc: 0.9022\n",
            "Epoch  88 | Train Loss: 0.4626 Acc: 0.8608 | Val Loss: 0.3615 Acc: 0.9035\n",
            "Epoch  89 | Train Loss: 0.4545 Acc: 0.8623 | Val Loss: 0.3594 Acc: 0.9048\n",
            "Epoch  90 | Train Loss: 0.4505 Acc: 0.8653 | Val Loss: 0.3584 Acc: 0.9045\n",
            "Epoch  91 | Train Loss: 0.4541 Acc: 0.8611 | Val Loss: 0.3561 Acc: 0.9038\n",
            "Epoch  92 | Train Loss: 0.4484 Acc: 0.8650 | Val Loss: 0.3532 Acc: 0.9048\n",
            "Epoch  93 | Train Loss: 0.4458 Acc: 0.8639 | Val Loss: 0.3503 Acc: 0.9053\n",
            "Epoch  94 | Train Loss: 0.4400 Acc: 0.8666 | Val Loss: 0.3482 Acc: 0.9052\n",
            "Epoch  95 | Train Loss: 0.4389 Acc: 0.8663 | Val Loss: 0.3471 Acc: 0.9065\n",
            "Epoch  96 | Train Loss: 0.4368 Acc: 0.8681 | Val Loss: 0.3445 Acc: 0.9068\n",
            "Epoch  97 | Train Loss: 0.4314 Acc: 0.8701 | Val Loss: 0.3430 Acc: 0.9063\n",
            "Epoch  98 | Train Loss: 0.4326 Acc: 0.8689 | Val Loss: 0.3412 Acc: 0.9073\n",
            "Epoch  99 | Train Loss: 0.4215 Acc: 0.8729 | Val Loss: 0.3394 Acc: 0.9077\n",
            "Epoch 100 | Train Loss: 0.4221 Acc: 0.8736 | Val Loss: 0.3365 Acc: 0.9088\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_2.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train Loss: 2.2395 Acc: 0.1773 | Val Loss: 2.1190 Acc: 0.2810\n",
            "Epoch   2 | Train Loss: 2.0220 Acc: 0.3325 | Val Loss: 1.9065 Acc: 0.4102\n",
            "Epoch   3 | Train Loss: 1.8390 Acc: 0.4356 | Val Loss: 1.7297 Acc: 0.5303\n",
            "Epoch   4 | Train Loss: 1.6828 Acc: 0.5119 | Val Loss: 1.5659 Acc: 0.6012\n",
            "Epoch   5 | Train Loss: 1.5477 Acc: 0.5526 | Val Loss: 1.4263 Acc: 0.6418\n",
            "Epoch   6 | Train Loss: 1.4326 Acc: 0.5809 | Val Loss: 1.3118 Acc: 0.6695\n",
            "Epoch   7 | Train Loss: 1.3384 Acc: 0.6063 | Val Loss: 1.2135 Acc: 0.6923\n",
            "Epoch   8 | Train Loss: 1.2596 Acc: 0.6284 | Val Loss: 1.1266 Acc: 0.7207\n",
            "Epoch   9 | Train Loss: 1.1887 Acc: 0.6446 | Val Loss: 1.0597 Acc: 0.7303\n",
            "Epoch  10 | Train Loss: 1.1309 Acc: 0.6583 | Val Loss: 0.9978 Acc: 0.7422\n",
            "Epoch  11 | Train Loss: 1.0802 Acc: 0.6683 | Val Loss: 0.9422 Acc: 0.7523\n",
            "Epoch  12 | Train Loss: 1.0314 Acc: 0.6848 | Val Loss: 0.8979 Acc: 0.7660\n",
            "Epoch  13 | Train Loss: 0.9985 Acc: 0.6907 | Val Loss: 0.8590 Acc: 0.7752\n",
            "Epoch  14 | Train Loss: 0.9618 Acc: 0.7035 | Val Loss: 0.8247 Acc: 0.7817\n",
            "Epoch  15 | Train Loss: 0.9343 Acc: 0.7085 | Val Loss: 0.7942 Acc: 0.7885\n",
            "Epoch  16 | Train Loss: 0.9072 Acc: 0.7160 | Val Loss: 0.7647 Acc: 0.7957\n",
            "Epoch  17 | Train Loss: 0.8796 Acc: 0.7244 | Val Loss: 0.7409 Acc: 0.8013\n",
            "Epoch  18 | Train Loss: 0.8599 Acc: 0.7310 | Val Loss: 0.7204 Acc: 0.8060\n",
            "Epoch  19 | Train Loss: 0.8342 Acc: 0.7382 | Val Loss: 0.6983 Acc: 0.8098\n",
            "Epoch  20 | Train Loss: 0.8179 Acc: 0.7440 | Val Loss: 0.6795 Acc: 0.8148\n",
            "Epoch  21 | Train Loss: 0.7994 Acc: 0.7502 | Val Loss: 0.6662 Acc: 0.8170\n",
            "Epoch  22 | Train Loss: 0.7832 Acc: 0.7549 | Val Loss: 0.6449 Acc: 0.8215\n",
            "Epoch  23 | Train Loss: 0.7721 Acc: 0.7568 | Val Loss: 0.6328 Acc: 0.8248\n",
            "Epoch  24 | Train Loss: 0.7574 Acc: 0.7604 | Val Loss: 0.6195 Acc: 0.8280\n",
            "Epoch  25 | Train Loss: 0.7452 Acc: 0.7653 | Val Loss: 0.6061 Acc: 0.8315\n",
            "Epoch  26 | Train Loss: 0.7300 Acc: 0.7714 | Val Loss: 0.5965 Acc: 0.8335\n",
            "Epoch  27 | Train Loss: 0.7162 Acc: 0.7735 | Val Loss: 0.5843 Acc: 0.8370\n",
            "Epoch  28 | Train Loss: 0.7067 Acc: 0.7776 | Val Loss: 0.5742 Acc: 0.8380\n",
            "Epoch  29 | Train Loss: 0.6939 Acc: 0.7813 | Val Loss: 0.5636 Acc: 0.8418\n",
            "Epoch  30 | Train Loss: 0.6849 Acc: 0.7837 | Val Loss: 0.5539 Acc: 0.8445\n",
            "Epoch  31 | Train Loss: 0.6776 Acc: 0.7859 | Val Loss: 0.5444 Acc: 0.8492\n",
            "Epoch  32 | Train Loss: 0.6672 Acc: 0.7901 | Val Loss: 0.5351 Acc: 0.8512\n",
            "Epoch  33 | Train Loss: 0.6584 Acc: 0.7929 | Val Loss: 0.5295 Acc: 0.8528\n",
            "Epoch  34 | Train Loss: 0.6471 Acc: 0.7965 | Val Loss: 0.5220 Acc: 0.8560\n",
            "Epoch  35 | Train Loss: 0.6382 Acc: 0.7994 | Val Loss: 0.5153 Acc: 0.8577\n",
            "Epoch  36 | Train Loss: 0.6292 Acc: 0.8025 | Val Loss: 0.5051 Acc: 0.8593\n",
            "Epoch  37 | Train Loss: 0.6253 Acc: 0.8027 | Val Loss: 0.4987 Acc: 0.8637\n",
            "Epoch  38 | Train Loss: 0.6156 Acc: 0.8077 | Val Loss: 0.4911 Acc: 0.8653\n",
            "Epoch  39 | Train Loss: 0.6060 Acc: 0.8106 | Val Loss: 0.4869 Acc: 0.8682\n",
            "Epoch  40 | Train Loss: 0.6061 Acc: 0.8091 | Val Loss: 0.4807 Acc: 0.8710\n",
            "Epoch  41 | Train Loss: 0.5940 Acc: 0.8142 | Val Loss: 0.4737 Acc: 0.8707\n",
            "Epoch  42 | Train Loss: 0.5834 Acc: 0.8191 | Val Loss: 0.4678 Acc: 0.8738\n",
            "Epoch  43 | Train Loss: 0.5806 Acc: 0.8199 | Val Loss: 0.4619 Acc: 0.8730\n",
            "Epoch  44 | Train Loss: 0.5731 Acc: 0.8209 | Val Loss: 0.4557 Acc: 0.8765\n",
            "Epoch  45 | Train Loss: 0.5687 Acc: 0.8216 | Val Loss: 0.4498 Acc: 0.8792\n",
            "Epoch  46 | Train Loss: 0.5660 Acc: 0.8237 | Val Loss: 0.4444 Acc: 0.8792\n",
            "Epoch  47 | Train Loss: 0.5541 Acc: 0.8287 | Val Loss: 0.4407 Acc: 0.8828\n",
            "Epoch  48 | Train Loss: 0.5495 Acc: 0.8291 | Val Loss: 0.4344 Acc: 0.8838\n",
            "Epoch  49 | Train Loss: 0.5442 Acc: 0.8316 | Val Loss: 0.4314 Acc: 0.8860\n",
            "Epoch  50 | Train Loss: 0.5398 Acc: 0.8323 | Val Loss: 0.4266 Acc: 0.8857\n",
            "Epoch  51 | Train Loss: 0.5318 Acc: 0.8350 | Val Loss: 0.4229 Acc: 0.8865\n",
            "Epoch  52 | Train Loss: 0.5276 Acc: 0.8361 | Val Loss: 0.4180 Acc: 0.8882\n",
            "Epoch  53 | Train Loss: 0.5234 Acc: 0.8376 | Val Loss: 0.4130 Acc: 0.8888\n",
            "Epoch  54 | Train Loss: 0.5192 Acc: 0.8386 | Val Loss: 0.4096 Acc: 0.8908\n",
            "Epoch  55 | Train Loss: 0.5188 Acc: 0.8376 | Val Loss: 0.4069 Acc: 0.8902\n",
            "Epoch  56 | Train Loss: 0.5115 Acc: 0.8418 | Val Loss: 0.4000 Acc: 0.8933\n",
            "Epoch  57 | Train Loss: 0.5049 Acc: 0.8423 | Val Loss: 0.3975 Acc: 0.8935\n",
            "Epoch  58 | Train Loss: 0.4973 Acc: 0.8466 | Val Loss: 0.3920 Acc: 0.8950\n",
            "Epoch  59 | Train Loss: 0.4924 Acc: 0.8484 | Val Loss: 0.3895 Acc: 0.8963\n",
            "Epoch  60 | Train Loss: 0.4909 Acc: 0.8481 | Val Loss: 0.3868 Acc: 0.8953\n",
            "Epoch  61 | Train Loss: 0.4883 Acc: 0.8486 | Val Loss: 0.3834 Acc: 0.8972\n",
            "Epoch  62 | Train Loss: 0.4825 Acc: 0.8502 | Val Loss: 0.3786 Acc: 0.8978\n",
            "Epoch  63 | Train Loss: 0.4786 Acc: 0.8512 | Val Loss: 0.3740 Acc: 0.9000\n",
            "Epoch  64 | Train Loss: 0.4714 Acc: 0.8535 | Val Loss: 0.3735 Acc: 0.9003\n",
            "Epoch  65 | Train Loss: 0.4744 Acc: 0.8533 | Val Loss: 0.3692 Acc: 0.8998\n",
            "Epoch  66 | Train Loss: 0.4647 Acc: 0.8546 | Val Loss: 0.3648 Acc: 0.9017\n",
            "Epoch  67 | Train Loss: 0.4626 Acc: 0.8578 | Val Loss: 0.3648 Acc: 0.9008\n",
            "Epoch  68 | Train Loss: 0.4603 Acc: 0.8575 | Val Loss: 0.3619 Acc: 0.9020\n",
            "Epoch  69 | Train Loss: 0.4526 Acc: 0.8601 | Val Loss: 0.3574 Acc: 0.9050\n",
            "Epoch  70 | Train Loss: 0.4525 Acc: 0.8597 | Val Loss: 0.3541 Acc: 0.9047\n",
            "Epoch  71 | Train Loss: 0.4495 Acc: 0.8614 | Val Loss: 0.3520 Acc: 0.9052\n",
            "Epoch  72 | Train Loss: 0.4441 Acc: 0.8626 | Val Loss: 0.3493 Acc: 0.9042\n",
            "Epoch  73 | Train Loss: 0.4382 Acc: 0.8636 | Val Loss: 0.3462 Acc: 0.9058\n",
            "Epoch  74 | Train Loss: 0.4346 Acc: 0.8663 | Val Loss: 0.3431 Acc: 0.9077\n",
            "Epoch  75 | Train Loss: 0.4316 Acc: 0.8664 | Val Loss: 0.3401 Acc: 0.9078\n",
            "Epoch  76 | Train Loss: 0.4319 Acc: 0.8673 | Val Loss: 0.3382 Acc: 0.9083\n",
            "Epoch  77 | Train Loss: 0.4293 Acc: 0.8661 | Val Loss: 0.3373 Acc: 0.9082\n",
            "Epoch  78 | Train Loss: 0.4246 Acc: 0.8704 | Val Loss: 0.3328 Acc: 0.9090\n",
            "Epoch  79 | Train Loss: 0.4216 Acc: 0.8701 | Val Loss: 0.3303 Acc: 0.9095\n",
            "Epoch  80 | Train Loss: 0.4201 Acc: 0.8713 | Val Loss: 0.3275 Acc: 0.9087\n",
            "Epoch  81 | Train Loss: 0.4154 Acc: 0.8707 | Val Loss: 0.3261 Acc: 0.9120\n",
            "Epoch  82 | Train Loss: 0.4122 Acc: 0.8731 | Val Loss: 0.3235 Acc: 0.9117\n",
            "Epoch  83 | Train Loss: 0.4114 Acc: 0.8735 | Val Loss: 0.3234 Acc: 0.9107\n",
            "Epoch  84 | Train Loss: 0.4077 Acc: 0.8740 | Val Loss: 0.3193 Acc: 0.9127\n",
            "Epoch  85 | Train Loss: 0.4043 Acc: 0.8754 | Val Loss: 0.3177 Acc: 0.9133\n",
            "Epoch  86 | Train Loss: 0.4024 Acc: 0.8762 | Val Loss: 0.3160 Acc: 0.9148\n",
            "Epoch  87 | Train Loss: 0.3982 Acc: 0.8775 | Val Loss: 0.3157 Acc: 0.9135\n",
            "Epoch  88 | Train Loss: 0.3971 Acc: 0.8769 | Val Loss: 0.3114 Acc: 0.9142\n",
            "Epoch  89 | Train Loss: 0.3929 Acc: 0.8801 | Val Loss: 0.3108 Acc: 0.9158\n",
            "Epoch  90 | Train Loss: 0.3893 Acc: 0.8801 | Val Loss: 0.3071 Acc: 0.9160\n",
            "Epoch  91 | Train Loss: 0.3892 Acc: 0.8804 | Val Loss: 0.3051 Acc: 0.9163\n",
            "Epoch  92 | Train Loss: 0.3848 Acc: 0.8813 | Val Loss: 0.3042 Acc: 0.9168\n",
            "Epoch  93 | Train Loss: 0.3822 Acc: 0.8816 | Val Loss: 0.3005 Acc: 0.9180\n",
            "Epoch  94 | Train Loss: 0.3797 Acc: 0.8835 | Val Loss: 0.2998 Acc: 0.9163\n",
            "Epoch  95 | Train Loss: 0.3753 Acc: 0.8860 | Val Loss: 0.2980 Acc: 0.9170\n",
            "Epoch  96 | Train Loss: 0.3742 Acc: 0.8851 | Val Loss: 0.2967 Acc: 0.9173\n",
            "Epoch  97 | Train Loss: 0.3752 Acc: 0.8843 | Val Loss: 0.2937 Acc: 0.9198\n",
            "Epoch  98 | Train Loss: 0.3702 Acc: 0.8868 | Val Loss: 0.2936 Acc: 0.9192\n",
            "Epoch  99 | Train Loss: 0.3650 Acc: 0.8879 | Val Loss: 0.2912 Acc: 0.9195\n",
            "Epoch 100 | Train Loss: 0.3671 Acc: 0.8871 | Val Loss: 0.2895 Acc: 0.9197\n",
            "\n",
            "âœ… Training complete!\n",
            "ðŸ’¾ Model saved to model_exp_3.pt\n",
            "ðŸš€ Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705984033.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train Loss: 2.2769 Acc: 0.1205 | Val Loss: 2.2227 Acc: 0.1488\n",
            "Epoch   2 | Train Loss: 2.1841 Acc: 0.1960 | Val Loss: 2.1233 Acc: 0.2547\n",
            "Epoch   3 | Train Loss: 2.0880 Acc: 0.2594 | Val Loss: 2.0240 Acc: 0.2995\n",
            "Epoch   4 | Train Loss: 1.9952 Acc: 0.3051 | Val Loss: 1.9279 Acc: 0.3480\n",
            "Epoch   5 | Train Loss: 1.9090 Acc: 0.3441 | Val Loss: 1.8412 Acc: 0.3903\n",
            "Epoch   6 | Train Loss: 1.8302 Acc: 0.3831 | Val Loss: 1.7630 Acc: 0.4318\n",
            "Epoch   7 | Train Loss: 1.7579 Acc: 0.4227 | Val Loss: 1.6845 Acc: 0.4713\n",
            "Epoch   8 | Train Loss: 1.6893 Acc: 0.4640 | Val Loss: 1.6161 Acc: 0.5157\n",
            "Epoch   9 | Train Loss: 1.6269 Acc: 0.4992 | Val Loss: 1.5535 Acc: 0.5538\n",
            "Epoch  10 | Train Loss: 1.5672 Acc: 0.5307 | Val Loss: 1.4847 Acc: 0.5787\n",
            "Epoch  11 | Train Loss: 1.5079 Acc: 0.5530 | Val Loss: 1.4293 Acc: 0.6097\n",
            "Epoch  12 | Train Loss: 1.4534 Acc: 0.5730 | Val Loss: 1.3648 Acc: 0.6288\n",
            "Epoch  13 | Train Loss: 1.4001 Acc: 0.5869 | Val Loss: 1.3071 Acc: 0.6447\n",
            "Epoch  14 | Train Loss: 1.3524 Acc: 0.5994 | Val Loss: 1.2570 Acc: 0.6588\n",
            "Epoch  15 | Train Loss: 1.3049 Acc: 0.6101 | Val Loss: 1.2021 Acc: 0.6680\n",
            "Epoch  16 | Train Loss: 1.2635 Acc: 0.6203 | Val Loss: 1.1662 Acc: 0.6790\n",
            "Epoch  17 | Train Loss: 1.2193 Acc: 0.6337 | Val Loss: 1.1229 Acc: 0.7057\n",
            "Epoch  18 | Train Loss: 1.1876 Acc: 0.6529 | Val Loss: 1.0855 Acc: 0.7223\n",
            "Epoch  19 | Train Loss: 1.1521 Acc: 0.6650 | Val Loss: 1.0457 Acc: 0.7345\n",
            "Epoch  20 | Train Loss: 1.1227 Acc: 0.6735 | Val Loss: 1.0032 Acc: 0.7447\n",
            "Epoch  21 | Train Loss: 1.0941 Acc: 0.6808 | Val Loss: 0.9832 Acc: 0.7585\n",
            "Epoch  22 | Train Loss: 1.0672 Acc: 0.6911 | Val Loss: 0.9532 Acc: 0.7638\n",
            "Epoch  23 | Train Loss: 1.0431 Acc: 0.6971 | Val Loss: 0.9189 Acc: 0.7730\n",
            "Epoch  24 | Train Loss: 1.0181 Acc: 0.7036 | Val Loss: 0.9005 Acc: 0.7783\n",
            "Epoch  25 | Train Loss: 0.9962 Acc: 0.7084 | Val Loss: 0.8747 Acc: 0.7820\n",
            "Epoch  26 | Train Loss: 0.9827 Acc: 0.7109 | Val Loss: 0.8537 Acc: 0.7880\n",
            "Epoch  27 | Train Loss: 0.9651 Acc: 0.7177 | Val Loss: 0.8357 Acc: 0.7913\n",
            "Epoch  28 | Train Loss: 0.9412 Acc: 0.7215 | Val Loss: 0.8137 Acc: 0.7963\n",
            "Epoch  29 | Train Loss: 0.9306 Acc: 0.7228 | Val Loss: 0.7943 Acc: 0.8002\n",
            "Epoch  30 | Train Loss: 0.9168 Acc: 0.7248 | Val Loss: 0.7765 Acc: 0.8015\n",
            "Epoch  31 | Train Loss: 0.8962 Acc: 0.7312 | Val Loss: 0.7631 Acc: 0.8047\n",
            "Epoch  32 | Train Loss: 0.8909 Acc: 0.7296 | Val Loss: 0.7479 Acc: 0.8065\n",
            "Epoch  33 | Train Loss: 0.8779 Acc: 0.7354 | Val Loss: 0.7334 Acc: 0.8078\n",
            "Epoch  34 | Train Loss: 0.8665 Acc: 0.7364 | Val Loss: 0.7232 Acc: 0.8117\n",
            "Epoch  35 | Train Loss: 0.8531 Acc: 0.7393 | Val Loss: 0.7069 Acc: 0.8127\n",
            "Epoch  36 | Train Loss: 0.8445 Acc: 0.7407 | Val Loss: 0.6942 Acc: 0.8128\n",
            "Epoch  37 | Train Loss: 0.8379 Acc: 0.7422 | Val Loss: 0.6849 Acc: 0.8170\n",
            "Epoch  38 | Train Loss: 0.8190 Acc: 0.7479 | Val Loss: 0.6719 Acc: 0.8203\n",
            "Epoch  39 | Train Loss: 0.8130 Acc: 0.7491 | Val Loss: 0.6644 Acc: 0.8235\n",
            "Epoch  40 | Train Loss: 0.8059 Acc: 0.7496 | Val Loss: 0.6499 Acc: 0.8252\n",
            "Epoch  41 | Train Loss: 0.7907 Acc: 0.7541 | Val Loss: 0.6429 Acc: 0.8278\n",
            "Epoch  42 | Train Loss: 0.7876 Acc: 0.7546 | Val Loss: 0.6324 Acc: 0.8302\n",
            "Epoch  43 | Train Loss: 0.7804 Acc: 0.7573 | Val Loss: 0.6251 Acc: 0.8327\n",
            "Epoch  44 | Train Loss: 0.7740 Acc: 0.7572 | Val Loss: 0.6181 Acc: 0.8332\n",
            "Epoch  45 | Train Loss: 0.7670 Acc: 0.7597 | Val Loss: 0.6109 Acc: 0.8358\n",
            "Epoch  46 | Train Loss: 0.7548 Acc: 0.7637 | Val Loss: 0.6011 Acc: 0.8365\n",
            "Epoch  47 | Train Loss: 0.7528 Acc: 0.7629 | Val Loss: 0.5954 Acc: 0.8390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-2705984033.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-2705984033.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  48 | Train Loss: 0.7512 Acc: 0.7630 | Val Loss: 0.5876 Acc: 0.8415\n",
            "Epoch  49 | Train Loss: 0.7407 Acc: 0.7664 | Val Loss: 0.5812 Acc: 0.8437\n",
            "Epoch  50 | Train Loss: 0.7376 Acc: 0.7659 | Val Loss: 0.5732 Acc: 0.8450\n",
            "Epoch  51 | Train Loss: 0.7348 Acc: 0.7684 | Val Loss: 0.5702 Acc: 0.8455\n",
            "Epoch  52 | Train Loss: 0.7298 Acc: 0.7688 | Val Loss: 0.5651 Acc: 0.8483\n",
            "Epoch  53 | Train Loss: 0.7168 Acc: 0.7724 | Val Loss: 0.5584 Acc: 0.8477\n",
            "Epoch  54 | Train Loss: 0.7147 Acc: 0.7741 | Val Loss: 0.5516 Acc: 0.8500\n",
            "Epoch  55 | Train Loss: 0.7102 Acc: 0.7744 | Val Loss: 0.5481 Acc: 0.8498\n",
            "Epoch  56 | Train Loss: 0.7032 Acc: 0.7759 | Val Loss: 0.5452 Acc: 0.8532\n",
            "Epoch  57 | Train Loss: 0.6999 Acc: 0.7777 | Val Loss: 0.5372 Acc: 0.8542\n",
            "Epoch  58 | Train Loss: 0.6950 Acc: 0.7774 | Val Loss: 0.5361 Acc: 0.8535\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train_cfg = TrainConfig(\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    lr=1e-4,\n",
        "    patience=5,\n",
        "    val_fraction=0.1\n",
        ")\n",
        "\n",
        "data_mgr = DataManager(\n",
        "    dataset_class=datasets.KMNIST,\n",
        "    val_fraction=train_cfg.val_fraction,\n",
        "    batch_size=train_cfg.batch_size,\n",
        "    seed=train_cfg.seed\n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = data_mgr.get_loaders()\n",
        "\n",
        "experiments = [\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=32, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=256, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.1, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=64, dropout=0.3, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.0, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.0, batch_norm=False, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.0, batch_norm=False, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=256, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=128, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=64, dropout=0.2, batch_norm=True, activation=nn.ReLU),\n",
        "            LayerSpec(out_dim=10, dropout=0.1, batch_norm=False, activation=nn.ReLU),\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "mlflow.set_experiment(\"KMNIST_Assigment\")\n",
        "excel_results = []\n",
        "best_macro_f1 = -1\n",
        "best_trainer = None\n",
        "best_experiment_idx = None\n",
        "\n",
        "\n",
        "for i, exp in enumerate(experiments):\n",
        "    model_cfg = ModelConfig(layers=exp[\"layers\"])\n",
        "    model = MLPFromConfig(model_cfg)\n",
        "    trainer = Trainer(model, train_cfg)\n",
        "\n",
        "    mlflow.start_run(run_name=f\"exp_{i}\")\n",
        "\n",
        "    mlflow.log_param(\"layers\", [spec.out_dim for spec in model_cfg.layers])\n",
        "    mlflow.log_param(\"dropout\", [spec.dropout for spec in model_cfg.layers])\n",
        "    mlflow.log_param(\"batch_norm\", [spec.batch_norm for spec in model_cfg.layers])\n",
        "\n",
        "    trainer.fit(train_loader, val_loader)\n",
        "\n",
        "    preds, targets = trainer.predict_all(test_loader)\n",
        "    report = classification_report(targets, preds, digits=4, output_dict=True)\n",
        "\n",
        "    mlflow.log_metric(\"test_accuracy\", report[\"accuracy\"])\n",
        "    mlflow.log_metric(\"macro_f1\", report[\"macro avg\"][\"f1-score\"])\n",
        "    mlflow.log_metric(\"macro_precision\", report[\"macro avg\"][\"precision\"])\n",
        "    mlflow.log_metric(\"macro_recall\", report[\"macro avg\"][\"recall\"])\n",
        "\n",
        "    trainer.save(f\"model_exp_{i}.pt\")\n",
        "\n",
        "\n",
        "    excel_results.append({\n",
        "    \"experiment\": f\"exp_{i}\",\n",
        "    \"layers\": [spec.out_dim for spec in model_cfg.layers],\n",
        "    \"test_accuracy\": report[\"accuracy\"],\n",
        "    \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "    \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
        "    \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
        "})\n",
        "    current_f1 = report[\"macro avg\"][\"f1-score\"]\n",
        "\n",
        "    if current_f1 > best_macro_f1:\n",
        "        best_macro_f1 = current_f1\n",
        "        best_trainer = trainer\n",
        "        best_experiment_idx = i\n",
        "    mlflow.end_run()\n",
        "df = pd.DataFrame(excel_results)\n",
        "df.to_excel(\"results.xlsx\", index=False)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0c6f58",
      "metadata": {
        "id": "3e0c6f58"
      },
      "source": [
        "Visuazize the train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740b2e48",
      "metadata": {
        "id": "740b2e48"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = best_trainer.history\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"],   label=\"Val Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
        "plt.plot(history[\"val_acc\"],   label=\"Val Acc\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualized the test (do change)"
      ],
      "metadata": {
        "id": "5kjoDILMt2Zw"
      },
      "id": "5kjoDILMt2Zw"
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = best_trainer.evaluate(test_loader)\n",
        "print(f\"ðŸ† Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "preds, targets = best_trainer.predict_all(test_loader)\n",
        "\n",
        "cm = confusion_matrix(targets, preds)\n",
        "plt.figure()\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix (Test)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print(\"Classification report (Test):\")\n",
        "print(classification_report(targets, preds, digits=4))"
      ],
      "metadata": {
        "id": "A-FR-NYgt6Hv"
      },
      "id": "A-FR-NYgt6Hv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}